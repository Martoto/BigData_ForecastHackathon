{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecast Model V3 - Daily Training Approach\n",
        "\n",
        "This notebook implements a daily-based forecasting model that:\n",
        "- Trains on daily transaction data (not weekly aggregations)\n",
        "- Properly handles duplicates and data quality issues\n",
        "- Captures day-of-week patterns and daily seasonality\n",
        "- Aggregates daily predictions to weekly output\n",
        "- Uses proper time-series validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n",
            "System: Darwin arm64\n",
            "CPU cores: 14\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import os\n",
        "import platform\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"System: {platform.system()} {platform.machine()}\")\n",
        "print(f\"CPU cores: {os.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 parquet files\n",
            "part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet: Shape (6560698, 11)\n",
            "-> Identified as TRANSACTIONS data\n",
            "part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet: Shape (7092, 8)\n",
            "-> Identified as PRODUCTS data\n",
            "part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet: Shape (14419, 4)\n",
            "-> Identified as STORES data\n",
            "\n",
            "Data loaded:\n",
            "- Transactions: 6,560,698 rows\n",
            "- Products: 7,092 rows\n",
            "- Stores: 14,419 rows\n"
          ]
        }
      ],
      "source": [
        "# Load all parquet files from data directory\n",
        "data_path = \"data/\"\n",
        "parquet_files = [f for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
        "print(f\"Found {len(parquet_files)} parquet files\")\n",
        "\n",
        "transactions = None\n",
        "products = None\n",
        "stores = None\n",
        "\n",
        "for file in parquet_files:\n",
        "    df = pd.read_parquet(os.path.join(data_path, file))\n",
        "    print(f\"{file}: Shape {df.shape}\")\n",
        "    \n",
        "    if 'internal_store_id' in df.columns and 'quantity' in df.columns:\n",
        "        transactions = df\n",
        "        print(\"-> Identified as TRANSACTIONS data\")\n",
        "    elif 'produto' in df.columns and 'categoria' in df.columns:\n",
        "        products = df\n",
        "        print(\"-> Identified as PRODUCTS data\")\n",
        "    elif 'pdv' in df.columns and 'premise' in df.columns:\n",
        "        stores = df\n",
        "        print(\"-> Identified as STORES data\")\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"- Transactions: {transactions.shape[0]:,} rows\")\n",
        "print(f\"- Products: {products.shape[0]:,} rows\") \n",
        "print(f\"- Stores: {stores.shape[0]:,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning transactions data...\n",
            "Initial rows: 6,560,698\n",
            "After removing nulls: 6,560,698 rows\n",
            "Removed 101 duplicate rows\n",
            "After removing zero/negative quantities: 6,430,060 rows\n",
            "Removed 87,994 outlier rows\n",
            "Final 2022 data: 6,342,066 rows\n",
            "Total reduction: 3.3%\n"
          ]
        }
      ],
      "source": [
        "# Clean transactions data: remove nulls, duplicates, outliers, and filter to 2022\n",
        "print(\"Cleaning transactions data...\")\n",
        "\n",
        "initial_rows = len(transactions)\n",
        "print(f\"Initial rows: {initial_rows:,}\")\n",
        "\n",
        "# Remove null values\n",
        "transactions = transactions.dropna(subset=['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date'])\n",
        "print(f\"After removing nulls: {len(transactions):,} rows\")\n",
        "\n",
        "# Remove duplicates based on key columns\n",
        "before_dedup = len(transactions)\n",
        "transactions = transactions.drop_duplicates(subset=['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity', 'gross_value'])\n",
        "print(f\"Removed {before_dedup - len(transactions):,} duplicate rows\")\n",
        "\n",
        "# Keep only positive quantities\n",
        "transactions = transactions[transactions['quantity'] > 0]\n",
        "print(f\"After removing zero/negative quantities: {len(transactions):,} rows\")\n",
        "\n",
        "# Remove extreme outliers\n",
        "value_per_unit = transactions['gross_value'] / transactions['quantity']\n",
        "q01 = value_per_unit.quantile(0.005)\n",
        "q99 = value_per_unit.quantile(0.995)\n",
        "valid_value_mask = (value_per_unit >= q01) & (value_per_unit <= q99)\n",
        "\n",
        "quantity_q99 = transactions['quantity'].quantile(0.995)\n",
        "valid_qty_mask = transactions['quantity'] <= quantity_q99\n",
        "\n",
        "before_outliers = len(transactions)\n",
        "transactions = transactions[valid_value_mask & valid_qty_mask]\n",
        "print(f\"Removed {before_outliers - len(transactions):,} outlier rows\")\n",
        "\n",
        "# Convert dates and filter to 2022\n",
        "transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])\n",
        "transactions = transactions[transactions['transaction_date'].dt.year == 2022]\n",
        "print(f\"Final 2022 data: {len(transactions):,} rows\")\n",
        "\n",
        "print(f\"Total reduction: {((initial_rows - len(transactions))/initial_rows)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning products and stores data...\n",
            "Products cleaned: 7,092 rows\n",
            "Stores cleaned: 14,419 rows\n"
          ]
        }
      ],
      "source": [
        "# Clean and prepare products and stores data\n",
        "print(\"Cleaning products and stores data...\")\n",
        "\n",
        "# Clean products\n",
        "products['descricao'] = products['descricao'].fillna('Unknown')\n",
        "products['categoria'] = products['categoria'].fillna('Other') \n",
        "products['marca'] = products['marca'].fillna('Unknown')\n",
        "\n",
        "# Clean stores\n",
        "stores['categoria_pdv'] = stores['categoria_pdv'].fillna('Other')\n",
        "stores['premise'] = stores['premise'].fillna('Unknown')\n",
        "\n",
        "print(f\"Products cleaned: {len(products):,} rows\")\n",
        "print(f\"Stores cleaned: {len(stores):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging data...\n",
            "After product merge: 6,342,066 rows\n",
            "After store merge: 6,342,066 rows\n",
            "Data merge completed successfully\n"
          ]
        }
      ],
      "source": [
        "# Merge transactions with products and stores\n",
        "print(\"Merging data...\")\n",
        "\n",
        "# Merge with products\n",
        "merged_data = transactions.merge(\n",
        "    products, \n",
        "    left_on='internal_product_id', \n",
        "    right_on='produto', \n",
        "    how='left'\n",
        ")\n",
        "print(f\"After product merge: {len(merged_data):,} rows\")\n",
        "\n",
        "# Merge with stores\n",
        "merged_data = merged_data.merge(\n",
        "    stores,\n",
        "    left_on='internal_store_id', \n",
        "    right_on='pdv',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"After store merge: {len(merged_data):,} rows\")\n",
        "\n",
        "print(\"Data merge completed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating daily aggregations...\n",
            "Created daily aggregations: 6,336,125 rows\n",
            "Date range: 2022-01-01 to 2022-12-31\n",
            "Unique store-product pairs: internal_store_id      14356\n",
            "internal_product_id     6312\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create daily aggregations with temporal features\n",
        "print(\"Creating daily aggregations...\")\n",
        "\n",
        "# Extract date components\n",
        "merged_data['date'] = merged_data['transaction_date'].dt.date\n",
        "merged_data['year'] = merged_data['transaction_date'].dt.year\n",
        "merged_data['month'] = merged_data['transaction_date'].dt.month\n",
        "merged_data['day'] = merged_data['transaction_date'].dt.day\n",
        "merged_data['dayofweek'] = merged_data['transaction_date'].dt.dayofweek  # 0=Monday\n",
        "merged_data['dayofyear'] = merged_data['transaction_date'].dt.dayofyear\n",
        "merged_data['week'] = merged_data['transaction_date'].dt.isocalendar().week\n",
        "merged_data['quarter'] = merged_data['transaction_date'].dt.quarter\n",
        "\n",
        "# Aggregate by day-store-product\n",
        "daily_data = merged_data.groupby([\n",
        "    'date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'week', 'quarter',\n",
        "    'internal_store_id', 'internal_product_id',\n",
        "    'categoria', 'marca', 'premise', 'categoria_pdv'\n",
        "]).agg({\n",
        "    'quantity': ['sum', 'mean', 'count'],\n",
        "    'gross_value': ['sum', 'mean'],\n",
        "    'net_value': ['sum', 'mean'],\n",
        "    'gross_profit': ['sum', 'mean']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "daily_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in daily_data.columns.values]\n",
        "\n",
        "# Rename for clarity\n",
        "daily_data.rename(columns={\n",
        "    'quantity_sum': 'total_quantity',\n",
        "    'quantity_mean': 'avg_quantity_per_transaction', \n",
        "    'quantity_count': 'num_transactions',\n",
        "    'gross_value_sum': 'total_gross_value',\n",
        "    'gross_value_mean': 'avg_gross_value',\n",
        "    'net_value_sum': 'total_net_value',\n",
        "    'net_value_mean': 'avg_net_value',\n",
        "    'gross_profit_sum': 'total_gross_profit',\n",
        "    'gross_profit_mean': 'avg_gross_profit'\n",
        "}, inplace=True)\n",
        "\n",
        "print(f\"Created daily aggregations: {len(daily_data):,} rows\")\n",
        "print(f\"Date range: {daily_data['date'].min()} to {daily_data['date'].max()}\")\n",
        "print(f\"Unique store-product pairs: {daily_data[['internal_store_id', 'internal_product_id']].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating daily temporal features...\n",
            "Daily temporal features created:\n",
            "- Day-of-week sine/cosine encoding\n",
            "- Day-of-year sine/cosine encoding\n",
            "- Month sine/cosine encoding\n",
            "- Week sine/cosine encoding\n",
            "- Weekend flag\n",
            "- Month start/end flags\n"
          ]
        }
      ],
      "source": [
        "# Create daily temporal features including day-of-week effects\n",
        "print(\"Creating daily temporal features...\")\n",
        "\n",
        "# Convert date to datetime for sorting\n",
        "daily_data['date'] = pd.to_datetime(daily_data['date'])\n",
        "daily_data = daily_data.sort_values(['internal_store_id', 'internal_product_id', 'date'])\n",
        "\n",
        "# Day-of-week cyclical features (0=Monday, 6=Sunday)\n",
        "daily_data['dayofweek_sin'] = np.sin(2 * np.pi * daily_data['dayofweek'] / 7)\n",
        "daily_data['dayofweek_cos'] = np.cos(2 * np.pi * daily_data['dayofweek'] / 7)\n",
        "\n",
        "# Day-of-year cyclical features (seasonality)\n",
        "daily_data['dayofyear_sin'] = np.sin(2 * np.pi * daily_data['dayofyear'] / 365)\n",
        "daily_data['dayofyear_cos'] = np.cos(2 * np.pi * daily_data['dayofyear'] / 365)\n",
        "\n",
        "# Month cyclical features\n",
        "daily_data['month_sin'] = np.sin(2 * np.pi * daily_data['month'] / 12)\n",
        "daily_data['month_cos'] = np.cos(2 * np.pi * daily_data['month'] / 12)\n",
        "\n",
        "# Week cyclical features\n",
        "daily_data['week_sin'] = np.sin(2 * np.pi * daily_data['week'] / 52)\n",
        "daily_data['week_cos'] = np.cos(2 * np.pi * daily_data['week'] / 52)\n",
        "\n",
        "# Is weekend flag\n",
        "daily_data['is_weekend'] = (daily_data['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Is month start/end\n",
        "daily_data['is_month_start'] = (daily_data['day'] <= 3).astype(int)\n",
        "daily_data['is_month_end'] = (daily_data['day'] >= 28).astype(int)\n",
        "\n",
        "print(\"Daily temporal features created:\")\n",
        "print(\"- Day-of-week sine/cosine encoding\")\n",
        "print(\"- Day-of-year sine/cosine encoding\") \n",
        "print(\"- Month sine/cosine encoding\")\n",
        "print(\"- Week sine/cosine encoding\")\n",
        "print(\"- Weekend flag\")\n",
        "print(\"- Month start/end flags\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating lag features...\n",
            "Created 10 lag/trend features:\n",
            "- quantity_lag_1\n",
            "- quantity_lag_2\n",
            "- quantity_lag_3\n",
            "- quantity_lag_7\n",
            "- quantity_rolling_3d\n",
            "- quantity_rolling_7d\n",
            "- quantity_rolling_30d\n",
            "- quantity_growth_1d\n",
            "- quantity_growth_7d\n",
            "- days_since_last_sale\n"
          ]
        }
      ],
      "source": [
        "# Create lag features without data leakage (using only past information)\n",
        "print(\"Creating lag features...\")\n",
        "\n",
        "# Sort to ensure proper lag calculation\n",
        "daily_data = daily_data.sort_values(['internal_store_id', 'internal_product_id', 'date'])\n",
        "\n",
        "# Group by store-product pairs\n",
        "grouped = daily_data.groupby(['internal_store_id', 'internal_product_id'])\n",
        "\n",
        "# Lag features (previous days)\n",
        "daily_data['quantity_lag_1'] = grouped['total_quantity'].shift(1)\n",
        "daily_data['quantity_lag_2'] = grouped['total_quantity'].shift(2) \n",
        "daily_data['quantity_lag_3'] = grouped['total_quantity'].shift(3)\n",
        "daily_data['quantity_lag_7'] = grouped['total_quantity'].shift(7)  # Same day last week\n",
        "\n",
        "# Rolling averages (using only past data)\n",
        "daily_data['quantity_rolling_3d'] = grouped['total_quantity'].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "daily_data['quantity_rolling_7d'] = grouped['total_quantity'].shift(1).rolling(window=7, min_periods=1).mean()\n",
        "daily_data['quantity_rolling_30d'] = grouped['total_quantity'].shift(1).rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "# Trend features (growth rates)\n",
        "daily_data['quantity_growth_1d'] = (daily_data['quantity_lag_1'] - daily_data['quantity_lag_2']) / (daily_data['quantity_lag_2'] + 1)\n",
        "daily_data['quantity_growth_7d'] = (daily_data['quantity_lag_1'] - daily_data['quantity_lag_7']) / (daily_data['quantity_lag_7'] + 1)\n",
        "\n",
        "# Days since last sale\n",
        "daily_data['days_since_last_sale'] = grouped['date'].diff().dt.days\n",
        "\n",
        "# Fill NaN values with 0 for lag features\n",
        "lag_columns = [col for col in daily_data.columns if 'lag' in col or 'rolling' in col or 'growth' in col or 'days_since' in col]\n",
        "daily_data[lag_columns] = daily_data[lag_columns].fillna(0)\n",
        "\n",
        "print(f\"Created {len(lag_columns)} lag/trend features:\")\n",
        "for col in lag_columns:\n",
        "    print(f\"- {col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating aggregate features...\n",
            "Created aggregate features:\n",
            "- Store average quantity (historical)\n",
            "- Product average quantity (historical)\n",
            "- Category average quantity (historical)\n"
          ]
        }
      ],
      "source": [
        "# Create store and product aggregate features (using only past data)\n",
        "print(\"Creating aggregate features...\")\n",
        "\n",
        "# Store-level daily averages (excluding current day)\n",
        "store_daily_avg = daily_data.groupby(['internal_store_id', 'date'])['total_quantity'].sum().reset_index()\n",
        "store_daily_avg.columns = ['internal_store_id', 'date', 'store_daily_total']\n",
        "store_daily_avg = store_daily_avg.sort_values(['internal_store_id', 'date'])\n",
        "store_daily_avg['store_avg_quantity'] = (\n",
        "    store_daily_avg.groupby('internal_store_id')['store_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Product-level daily averages (excluding current day) \n",
        "product_daily_avg = daily_data.groupby(['internal_product_id', 'date'])['total_quantity'].sum().reset_index()\n",
        "product_daily_avg.columns = ['internal_product_id', 'date', 'product_daily_total']\n",
        "product_daily_avg = product_daily_avg.sort_values(['internal_product_id', 'date'])\n",
        "product_daily_avg['product_avg_quantity'] = (\n",
        "    product_daily_avg.groupby('internal_product_id')['product_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Category-level daily averages (excluding current day)\n",
        "category_daily_avg = daily_data.groupby(['categoria', 'date'])['total_quantity'].sum().reset_index()\n",
        "category_daily_avg.columns = ['categoria', 'date', 'category_daily_total'] \n",
        "category_daily_avg = category_daily_avg.sort_values(['categoria', 'date'])\n",
        "category_daily_avg['category_avg_quantity'] = (\n",
        "    category_daily_avg.groupby('categoria')['category_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Merge aggregate features\n",
        "daily_data = daily_data.merge(\n",
        "    store_daily_avg[['internal_store_id', 'date', 'store_avg_quantity']], \n",
        "    on=['internal_store_id', 'date'], how='left'\n",
        ")\n",
        "\n",
        "daily_data = daily_data.merge(\n",
        "    product_daily_avg[['internal_product_id', 'date', 'product_avg_quantity']],\n",
        "    on=['internal_product_id', 'date'], how='left'\n",
        ")\n",
        "\n",
        "daily_data = daily_data.merge(\n",
        "    category_daily_avg[['categoria', 'date', 'category_avg_quantity']],\n",
        "    on=['categoria', 'date'], how='left'\n",
        ")\n",
        "\n",
        "# Fill NaN values with 0\n",
        "daily_data[['store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity']] = (\n",
        "    daily_data[['store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity']].fillna(0)\n",
        ")\n",
        "\n",
        "print(\"Created aggregate features:\")\n",
        "print(\"- Store average quantity (historical)\")\n",
        "print(\"- Product average quantity (historical)\")\n",
        "print(\"- Category average quantity (historical)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing training data...\n",
            "Training data: 5,725,705 samples\n",
            "Validation data: 532,891 samples\n",
            "Training date range: 2022-01-08 00:00:00 to 2022-11-30 00:00:00\n",
            "Validation date range: 2022-12-01 00:00:00 to 2022-12-31 00:00:00\n",
            "Features: 42 (4 categorical, 38 numerical)\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data with proper time-series split\n",
        "print(\"Preparing training data...\")\n",
        "\n",
        "# Define feature columns\n",
        "categorical_features = ['categoria', 'marca', 'premise', 'categoria_pdv']\n",
        "numerical_features = [\n",
        "    'dayofweek', 'dayofyear', 'month', 'day', 'week', 'quarter',\n",
        "    'dayofweek_sin', 'dayofweek_cos', 'dayofyear_sin', 'dayofyear_cos',\n",
        "    'month_sin', 'month_cos', 'week_sin', 'week_cos',\n",
        "    'is_weekend', 'is_month_start', 'is_month_end',\n",
        "    'num_transactions', 'avg_quantity_per_transaction',\n",
        "    'total_gross_value', 'avg_gross_value', 'total_net_value', 'avg_net_value',\n",
        "    'total_gross_profit', 'avg_gross_profit',\n",
        "    'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3', 'quantity_lag_7',\n",
        "    'quantity_rolling_3d', 'quantity_rolling_7d', 'quantity_rolling_30d',\n",
        "    'quantity_growth_1d', 'quantity_growth_7d', 'days_since_last_sale',\n",
        "    'store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity'\n",
        "]\n",
        "\n",
        "all_features = categorical_features + numerical_features\n",
        "\n",
        "# Ensure categorical features are strings\n",
        "for col in categorical_features:\n",
        "    daily_data[col] = daily_data[col].astype(str)\n",
        "\n",
        "# Filter training data (skip first few days due to lag features)\n",
        "min_date = daily_data['date'].min() + pd.Timedelta(days=7)  # Skip first 7 days\n",
        "train_data = daily_data[daily_data['date'] >= min_date].copy()\n",
        "\n",
        "# Time-based split for validation (last 30 days)\n",
        "max_date = train_data['date'].max()\n",
        "split_date = max_date - pd.Timedelta(days=30)\n",
        "\n",
        "train_mask = train_data['date'] < split_date\n",
        "val_mask = train_data['date'] >= split_date\n",
        "\n",
        "X_train = train_data[train_mask][all_features]\n",
        "y_train = train_data[train_mask]['total_quantity']\n",
        "X_val = train_data[val_mask][all_features]\n",
        "y_val = train_data[val_mask]['total_quantity']\n",
        "\n",
        "# Apply log transformation to target\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log = np.log1p(y_val)\n",
        "\n",
        "print(f\"Training data: {len(X_train):,} samples\")\n",
        "print(f\"Validation data: {len(X_val):,} samples\")\n",
        "print(f\"Training date range: {train_data[train_mask]['date'].min()} to {train_data[train_mask]['date'].max()}\")\n",
        "print(f\"Validation date range: {train_data[val_mask]['date'].min()} to {train_data[val_mask]['date'].max()}\")\n",
        "print(f\"Features: {len(all_features)} ({len(categorical_features)} categorical, {len(numerical_features)} numerical)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training CatBoost model...\n",
            "Using 12 CPU threads\n",
            "Model parameters:\n",
            "  iterations: 2000\n",
            "  learning_rate: 0.02\n",
            "  depth: 8\n",
            "  l2_leaf_reg: 10\n",
            "  bootstrap_type: Bayesian\n",
            "  bagging_temperature: 0.8\n",
            "  random_strength: 1.0\n",
            "  border_count: 200\n",
            "  loss_function: MAE\n",
            "  eval_metric: MAE\n",
            "  random_seed: 42\n",
            "  verbose: 200\n",
            "  early_stopping_rounds: 300\n",
            "  thread_count: 12\n",
            "  used_ram_limit: 12GB\n",
            "\\nStarting training...\n",
            "0:\tlearn: 0.5389186\ttest: 0.5405347\tbest: 0.5405347 (0)\ttotal: 1.66s\tremaining: 55m 25s\n",
            "200:\tlearn: 0.0207765\ttest: 0.0213139\tbest: 0.0213139 (200)\ttotal: 3m 25s\tremaining: 30m 39s\n",
            "400:\tlearn: 0.0060040\ttest: 0.0070514\tbest: 0.0070514 (400)\ttotal: 6m 21s\tremaining: 25m 22s\n",
            "600:\tlearn: 0.0044686\ttest: 0.0052880\tbest: 0.0052880 (600)\ttotal: 9m 3s\tremaining: 21m 5s\n",
            "800:\tlearn: 0.0035722\ttest: 0.0043448\tbest: 0.0043448 (800)\ttotal: 11m 51s\tremaining: 17m 45s\n",
            "1000:\tlearn: 0.0030962\ttest: 0.0037685\tbest: 0.0037685 (1000)\ttotal: 14m 48s\tremaining: 14m 47s\n",
            "1200:\tlearn: 0.0027297\ttest: 0.0033648\tbest: 0.0033648 (1200)\ttotal: 17m 54s\tremaining: 11m 54s\n",
            "1400:\tlearn: 0.0024647\ttest: 0.0030801\tbest: 0.0030801 (1400)\ttotal: 20m 54s\tremaining: 8m 56s\n",
            "1600:\tlearn: 0.0022563\ttest: 0.0028685\tbest: 0.0028685 (1600)\ttotal: 23m 57s\tremaining: 5m 58s\n",
            "1800:\tlearn: 0.0020923\ttest: 0.0027245\tbest: 0.0027245 (1800)\ttotal: 26m 53s\tremaining: 2m 58s\n",
            "1999:\tlearn: 0.0019944\ttest: 0.0026354\tbest: 0.0026354 (1999)\ttotal: 29m 54s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.002635404587\n",
            "bestIteration = 1999\n",
            "\n",
            "\\nModel training completed!\n"
          ]
        }
      ],
      "source": [
        "# Train CatBoost model optimized for daily forecasting\n",
        "print(\"Training CatBoost model...\")\n",
        "\n",
        "# Get categorical feature indices\n",
        "cat_feature_indices = [all_features.index(col) for col in categorical_features]\n",
        "\n",
        "# Optimize for Apple Silicon or use CPU cores\n",
        "n_threads = max(1, int(os.cpu_count() * 0.9))\n",
        "print(f\"Using {n_threads} CPU threads\")\n",
        "\n",
        "# Model parameters optimized for daily data\n",
        "model_params = {\n",
        "    'iterations': 2000,\n",
        "    'learning_rate': 0.02,\n",
        "    'depth': 8,\n",
        "    'l2_leaf_reg': 10,\n",
        "    'bootstrap_type': 'Bayesian',\n",
        "    'bagging_temperature': 0.8,\n",
        "    'random_strength': 1.0,\n",
        "    'border_count': 200,\n",
        "    'loss_function': 'MAE',\n",
        "    'eval_metric': 'MAE',\n",
        "    'random_seed': 42,\n",
        "    'verbose': 200,\n",
        "    'early_stopping_rounds': 300,\n",
        "    'thread_count': n_threads,\n",
        "    'used_ram_limit': '12GB',\n",
        "}\n",
        "\n",
        "print(\"Model parameters:\")\n",
        "for key, value in model_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Initialize and train model\n",
        "model = CatBoostRegressor(**model_params)\n",
        "\n",
        "print(\"\\\\nStarting training...\")\n",
        "model.fit(\n",
        "    X_train, y_train_log,\n",
        "    eval_set=(X_val, y_val_log),\n",
        "    cat_features=cat_feature_indices,\n",
        "    early_stopping_rounds=300,\n",
        "    verbose=200,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "print(\"\\\\nModel training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model performance...\n",
            "\\n==================================================\n",
            "MODEL PERFORMANCE METRICS\n",
            "==================================================\n",
            "Training WMAPE: 0.71%\n",
            "Validation WMAPE: 0.95%\n",
            "Training MAPE: 38.35%\n",
            "Validation MAPE: 0.31%\n",
            "Training MAE: 0.04\n",
            "Validation MAE: 0.04\n",
            "\\nOverfitting Check:\n",
            "WMAPE difference: 0.24%\n",
            "MAPE difference: 38.03%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating model performance...\")\n",
        "\n",
        "train_pred_log = model.predict(X_train)\n",
        "val_pred_log = model.predict(X_val)\n",
        "\n",
        "train_pred = np.expm1(train_pred_log)\n",
        "val_pred = np.expm1(val_pred_log)\n",
        "train_actual = y_train.values\n",
        "val_actual = y_val.values\n",
        "\n",
        "train_pred = np.maximum(0, train_pred)\n",
        "val_pred = np.maximum(0, val_pred)\n",
        "\n",
        "def calculate_wmape(y_true, y_pred):\n",
        "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100 if np.sum(y_true) > 0 else 0\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    mask = y_true > 0\n",
        "    if not mask.any():\n",
        "        return 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "train_wmape = calculate_wmape(train_actual, train_pred)\n",
        "val_wmape = calculate_wmape(val_actual, val_pred)\n",
        "train_mape = calculate_mape(train_actual, train_pred)\n",
        "val_mape = calculate_mape(val_actual, val_pred)\n",
        "\n",
        "train_mae = np.mean(np.abs(train_actual - train_pred))\n",
        "val_mae = np.mean(np.abs(val_actual - val_pred))\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training WMAPE: {train_wmape:.2f}%\")\n",
        "print(f\"Validation WMAPE: {val_wmape:.2f}%\")\n",
        "print(f\"Training MAPE: {train_mape:.2f}%\")\n",
        "print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "wmape_diff = abs(val_wmape - train_wmape)\n",
        "mape_diff = abs(val_mape - train_mape)\n",
        "print(f\"\\\\nOverfitting Check:\")\n",
        "print(f\"WMAPE difference: {wmape_diff:.2f}%\")\n",
        "print(f\"MAPE difference: {mape_diff:.2f}%\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating daily predictions for January 2023...\n",
            "Using 447,758 active store-product pairs for prediction\n",
            "Latest features available for 447,758 pairs\n",
            "Predicting for 2023-01-01 (1/31)\n",
            "Predicting for 2023-01-02 (2/31)\n",
            "Predicting for 2023-01-03 (3/31)\n",
            "Predicting for 2023-01-04 (4/31)\n",
            "Predicting for 2023-01-05 (5/31)\n",
            "Predicting for 2023-01-06 (6/31)\n",
            "Predicting for 2023-01-07 (7/31)\n",
            "Predicting for 2023-01-08 (8/31)\n",
            "Predicting for 2023-01-09 (9/31)\n",
            "Predicting for 2023-01-10 (10/31)\n",
            "Predicting for 2023-01-11 (11/31)\n",
            "Predicting for 2023-01-12 (12/31)\n",
            "Predicting for 2023-01-13 (13/31)\n",
            "Predicting for 2023-01-14 (14/31)\n",
            "Predicting for 2023-01-15 (15/31)\n",
            "Predicting for 2023-01-16 (16/31)\n",
            "Predicting for 2023-01-17 (17/31)\n",
            "Predicting for 2023-01-18 (18/31)\n",
            "Predicting for 2023-01-19 (19/31)\n",
            "Predicting for 2023-01-20 (20/31)\n",
            "Predicting for 2023-01-21 (21/31)\n",
            "Predicting for 2023-01-22 (22/31)\n",
            "Predicting for 2023-01-23 (23/31)\n",
            "Predicting for 2023-01-24 (24/31)\n",
            "Predicting for 2023-01-25 (25/31)\n",
            "Predicting for 2023-01-26 (26/31)\n",
            "Predicting for 2023-01-27 (27/31)\n",
            "Predicting for 2023-01-28 (28/31)\n",
            "Predicting for 2023-01-29 (29/31)\n",
            "Predicting for 2023-01-30 (30/31)\n",
            "Predicting for 2023-01-31 (31/31)\n",
            "\\nGenerated 13,880,498 daily predictions for January 2023\n",
            "Date range: 2023-01-01 00:00:00 to 2023-01-31 00:00:00\n",
            "Total predicted quantity: 73,149,231\n",
            "Average daily quantity per store-product: 5.27\n"
          ]
        }
      ],
      "source": [
        "# Generate daily predictions for January 2023 only (31 days)\n",
        "print(\"Generating daily predictions for January 2023...\")\n",
        "\n",
        "# Get unique store-product combinations from recent data (last 60 days)\n",
        "recent_date = daily_data['date'].max() - pd.Timedelta(days=60)\n",
        "recent_data = daily_data[daily_data['date'] >= recent_date]\n",
        "active_pairs = recent_data.groupby(['internal_store_id', 'internal_product_id']).agg({\n",
        "    'total_quantity': 'sum',\n",
        "    'date': 'count'\n",
        "}).reset_index()\n",
        "active_pairs.columns = ['internal_store_id', 'internal_product_id', 'total_qty', 'days_active']\n",
        "\n",
        "# Use all active pairs for prediction generation (we'll filter to top 1.5M later)\n",
        "active_pairs = active_pairs.sort_values(['total_qty', 'days_active'], ascending=[False, False])\n",
        "selected_pairs = active_pairs[['internal_store_id', 'internal_product_id']]\n",
        "\n",
        "print(f\"Using {len(selected_pairs):,} active store-product pairs for prediction\")\n",
        "\n",
        "# Get latest features for each pair\n",
        "latest_features = daily_data.loc[daily_data.groupby(['internal_store_id', 'internal_product_id'])['date'].idxmax()]\n",
        "latest_features = latest_features.merge(selected_pairs, on=['internal_store_id', 'internal_product_id'], how='inner')\n",
        "\n",
        "print(f\"Latest features available for {len(latest_features):,} pairs\")\n",
        "\n",
        "# Generate predictions for January 2023 only (31 days)\n",
        "january_dates = pd.date_range('2023-01-01', '2023-01-31', freq='D')\n",
        "all_daily_predictions = []\n",
        "\n",
        "for i, pred_date in enumerate(january_dates):\n",
        "    print(f\"Predicting for {pred_date.strftime('%Y-%m-%d')} ({i+1}/{len(january_dates)})\")\n",
        "    \n",
        "    # Create prediction data\n",
        "    pred_data = latest_features.copy()\n",
        "    \n",
        "    # Update temporal features for prediction date\n",
        "    pred_data['date'] = pred_date\n",
        "    pred_data['year'] = pred_date.year\n",
        "    pred_data['month'] = pred_date.month\n",
        "    pred_data['day'] = pred_date.day\n",
        "    pred_data['dayofweek'] = pred_date.dayofweek\n",
        "    pred_data['dayofyear'] = pred_date.dayofyear\n",
        "    pred_data['week'] = pred_date.isocalendar().week\n",
        "    pred_data['quarter'] = 1  # January is Q1\n",
        "    \n",
        "    # Update cyclical features\n",
        "    pred_data['dayofweek_sin'] = np.sin(2 * np.pi * pred_data['dayofweek'] / 7)\n",
        "    pred_data['dayofweek_cos'] = np.cos(2 * np.pi * pred_data['dayofweek'] / 7)\n",
        "    pred_data['dayofyear_sin'] = np.sin(2 * np.pi * pred_data['dayofyear'] / 365)\n",
        "    pred_data['dayofyear_cos'] = np.cos(2 * np.pi * pred_data['dayofyear'] / 365)\n",
        "    pred_data['month_sin'] = np.sin(2 * np.pi * pred_data['month'] / 12)\n",
        "    pred_data['month_cos'] = np.cos(2 * np.pi * pred_data['month'] / 12)\n",
        "    pred_data['week_sin'] = np.sin(2 * np.pi * pred_data['week'] / 52)\n",
        "    pred_data['week_cos'] = np.cos(2 * np.pi * pred_data['week'] / 52)\n",
        "    \n",
        "    # Update binary features\n",
        "    pred_data['is_weekend'] = (pred_data['dayofweek'] >= 5).astype(int)\n",
        "    pred_data['is_month_start'] = (pred_data['day'] <= 3).astype(int)\n",
        "    pred_data['is_month_end'] = (pred_data['day'] >= 28).astype(int)\n",
        "    \n",
        "    # Make predictions\n",
        "    X_pred = pred_data[all_features]\n",
        "    pred_log = model.predict(X_pred)\n",
        "    predictions = np.expm1(pred_log)\n",
        "    predictions = np.maximum(0, predictions).round().astype(int)\n",
        "    \n",
        "    # Store daily predictions\n",
        "    daily_pred_df = pd.DataFrame({\n",
        "        'date': pred_date,\n",
        "        'pdv': pred_data['internal_store_id'].astype(int),\n",
        "        'produto': pred_data['internal_product_id'].astype(int),\n",
        "        'quantidade_diaria': predictions\n",
        "    })\n",
        "    \n",
        "    all_daily_predictions.append(daily_pred_df)\n",
        "\n",
        "# Combine all daily predictions\n",
        "daily_predictions_df = pd.concat(all_daily_predictions, ignore_index=True)\n",
        "\n",
        "print(f\"\\\\nGenerated {len(daily_predictions_df):,} daily predictions for January 2023\")\n",
        "print(f\"Date range: {daily_predictions_df['date'].min()} to {daily_predictions_df['date'].max()}\")\n",
        "print(f\"Total predicted quantity: {daily_predictions_df['quantidade_diaria'].sum():,}\")\n",
        "print(f\"Average daily quantity per store-product: {daily_predictions_df['quantidade_diaria'].mean():.2f}\")\n",
        "\n",
        "# Note: We'll select top 1.5M predictions after weekly aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregating daily predictions to weekly format...\n",
            "\\nWeekly aggregation completed:\n",
            "Total weekly predictions before filtering: 2,238,790\n",
            "Weeks covered: 1 to 5\n",
            "Total predicted quantity before filtering: 73,149,231\n",
            "\\nSelecting top 1.5M weekly predictions across all weeks...\n",
            "Selected top 1,500,000 weekly predictions\n",
            "Total predicted quantity: 68,896,368\n",
            "Average weekly quantity per store-product: 45.93\n",
            "Min weekly quantity: 7\n",
            "Max weekly quantity: 1666\n",
            "\\nFinal weekly predictions:\n",
            "Unique stores: 11,177\n",
            "Unique products: 4,356\n",
            "\\nSample predictions:\n",
            "    semana               pdv              produto  quantidade\n",
            "0        1  1833563591805356   519576013096278235          42\n",
            "2        1  1833563591805356  3371701102715312626          14\n",
            "3        1  1833563591805356  3726736891643803768          14\n",
            "4        1  1833563591805356  7370724865268543987          14\n",
            "5        1  2039259415232404   155732775661148005           7\n",
            "6        1  2039259415232404   200407080435025895           7\n",
            "7        1  2039259415232404   347520348779130794           7\n",
            "8        1  2039259415232404   470856825659366361           7\n",
            "9        1  2039259415232404   576514643033029430          35\n",
            "10       1  2039259415232404   637448570435263100          14\n",
            "\\nWeekly summary after filtering:\n",
            "        predictions_count  total_quantity  avg_quantity\n",
            "semana                                                 \n",
            "1                  353661        15872692         44.88\n",
            "2                  387244        16108320         41.60\n",
            "3                  237106        15047501         63.46\n",
            "4                  358632        15901773         44.34\n",
            "5                  163357         5966082         36.52\n"
          ]
        }
      ],
      "source": [
        "print(\"Aggregating daily predictions to weekly format...\")\n",
        "\n",
        "daily_predictions_df['week'] = ((daily_predictions_df['date'] - pd.Timestamp('2023-01-01')).dt.days // 7) + 1\n",
        "\n",
        "# Aggregate by week-store-product\n",
        "weekly_predictions = daily_predictions_df.groupby(['week', 'pdv', 'produto']).agg({\n",
        "    'quantidade_diaria': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Rename for final output format\n",
        "weekly_predictions.rename(columns={\n",
        "    'week': 'semana',\n",
        "    'quantidade_diaria': 'quantidade'\n",
        "}, inplace=True)\n",
        "\n",
        "# Ensure proper data types\n",
        "weekly_predictions['semana'] = weekly_predictions['semana'].astype(int)\n",
        "weekly_predictions['pdv'] = weekly_predictions['pdv'].astype(int)\n",
        "weekly_predictions['produto'] = weekly_predictions['produto'].astype(int)\n",
        "weekly_predictions['quantidade'] = weekly_predictions['quantidade'].astype(int)\n",
        "\n",
        "print(f\"\\\\nWeekly aggregation completed:\")\n",
        "print(f\"Total weekly predictions before filtering: {len(weekly_predictions):,}\")\n",
        "print(f\"Weeks covered: {weekly_predictions['semana'].min()} to {weekly_predictions['semana'].max()}\")\n",
        "print(f\"Total predicted quantity before filtering: {weekly_predictions['quantidade'].sum():,}\")\n",
        "\n",
        "# Now select top 1.5M weekly predictions based on quantity\n",
        "print(\"\\\\nSelecting top 1.5M weekly predictions across all weeks...\")\n",
        "weekly_predictions = weekly_predictions.sort_values('quantidade', ascending=False)\n",
        "top_weekly_predictions = weekly_predictions.head(1500000)\n",
        "\n",
        "print(f\"Selected top {len(top_weekly_predictions):,} weekly predictions\")\n",
        "print(f\"Total predicted quantity: {top_weekly_predictions['quantidade'].sum():,}\")\n",
        "print(f\"Average weekly quantity per store-product: {top_weekly_predictions['quantidade'].mean():.2f}\")\n",
        "print(f\"Min weekly quantity: {top_weekly_predictions['quantidade'].min()}\")\n",
        "print(f\"Max weekly quantity: {top_weekly_predictions['quantidade'].max()}\")\n",
        "\n",
        "# Update weekly predictions with filtered data\n",
        "weekly_predictions = top_weekly_predictions.copy()\n",
        "\n",
        "# Sort by week, store, product for final output\n",
        "weekly_predictions = weekly_predictions.sort_values(['semana', 'pdv', 'produto'])\n",
        "\n",
        "print(f\"\\\\nFinal weekly predictions:\")\n",
        "print(f\"Unique stores: {weekly_predictions['pdv'].nunique():,}\")\n",
        "print(f\"Unique products: {weekly_predictions['produto'].nunique():,}\")\n",
        "\n",
        "# Show sample of predictions\n",
        "print(f\"\\\\nSample predictions:\")\n",
        "print(weekly_predictions.head(10))\n",
        "\n",
        "# Weekly summary after filtering\n",
        "weekly_summary = weekly_predictions.groupby('semana').agg({\n",
        "    'quantidade': ['count', 'sum', 'mean']\n",
        "}).round(2)\n",
        "weekly_summary.columns = ['predictions_count', 'total_quantity', 'avg_quantity']\n",
        "print(f\"\\\\nWeekly summary after filtering:\")\n",
        "print(weekly_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving predictions...\n",
            "Weekly predictions saved:\n",
            "- CSV: sales_predictions_v3.csv\n",
            "- Parquet: sales_predictions_v3.parquet\n",
            "- Rows: 1,500,000\n",
            "- Columns: ['semana', 'pdv', 'produto', 'quantidade']\n",
            "\\nDaily predictions also saved to: sales_predictions_daily_v3.csv\n",
            "\\nForecasting completed successfully.\n",
            "Model trained on daily data with proper time-series validation.\n",
            "Predictions generated for 5 weeks of January 2023.\n"
          ]
        }
      ],
      "source": [
        "# Save predictions to CSV and Parquet files\n",
        "print(\"Saving predictions...\")\n",
        "\n",
        "# Save weekly predictions (main output)\n",
        "csv_filename = \"sales_predictions_v3.csv\"\n",
        "parquet_filename = \"sales_predictions_v3.parquet\"\n",
        "\n",
        "weekly_predictions.to_csv(csv_filename, sep=';', index=False, encoding='utf-8')\n",
        "weekly_predictions.to_parquet(parquet_filename, index=False)\n",
        "\n",
        "print(f\"Weekly predictions saved:\")\n",
        "print(f\"- CSV: {csv_filename}\")\n",
        "print(f\"- Parquet: {parquet_filename}\")\n",
        "print(f\"- Rows: {len(weekly_predictions):,}\")\n",
        "print(f\"- Columns: {list(weekly_predictions.columns)}\")\n",
        "\n",
        "# Save daily predictions for analysis (optional)\n",
        "daily_csv_filename = \"sales_predictions_daily_v3.csv\"\n",
        "daily_predictions_df.to_csv(daily_csv_filename, sep=';', index=False, encoding='utf-8')\n",
        "print(f\"\\\\nDaily predictions also saved to: {daily_csv_filename}\")\n",
        "\n",
        "print(\"\\\\nForecasting completed successfully.\")\n",
        "print(\"Model trained on daily data with proper time-series validation.\")\n",
        "print(\"Predictions generated for 5 weeks of January 2023.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
