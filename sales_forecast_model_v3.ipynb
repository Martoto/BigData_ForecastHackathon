{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecast Model V3 - Daily Training Approach\n",
        "\n",
        "This notebook implements a daily-based forecasting model that:\n",
        "- Trains on daily transaction data (not weekly aggregations)\n",
        "- Properly handles duplicates and data quality issues\n",
        "- Captures day-of-week patterns and daily seasonality\n",
        "- Aggregates daily predictions to weekly output\n",
        "- Uses proper time-series validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import os\n",
        "import platform\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"System: {platform.system()} {platform.machine()}\")\n",
        "print(f\"CPU cores: {os.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all parquet files from data directory\n",
        "data_path = \"data/\"\n",
        "parquet_files = [f for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
        "print(f\"Found {len(parquet_files)} parquet files\")\n",
        "\n",
        "transactions = None\n",
        "products = None\n",
        "stores = None\n",
        "\n",
        "for file in parquet_files:\n",
        "    df = pd.read_parquet(os.path.join(data_path, file))\n",
        "    print(f\"{file}: Shape {df.shape}\")\n",
        "    \n",
        "    if 'internal_store_id' in df.columns and 'quantity' in df.columns:\n",
        "        transactions = df\n",
        "        print(\"-> Identified as TRANSACTIONS data\")\n",
        "    elif 'produto' in df.columns and 'categoria' in df.columns:\n",
        "        products = df\n",
        "        print(\"-> Identified as PRODUCTS data\")\n",
        "    elif 'pdv' in df.columns and 'premise' in df.columns:\n",
        "        stores = df\n",
        "        print(\"-> Identified as STORES data\")\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"- Transactions: {transactions.shape[0]:,} rows\")\n",
        "print(f\"- Products: {products.shape[0]:,} rows\") \n",
        "print(f\"- Stores: {stores.shape[0]:,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean transactions data: remove nulls, duplicates, outliers, and filter to 2022\n",
        "print(\"Cleaning transactions data...\")\n",
        "\n",
        "initial_rows = len(transactions)\n",
        "print(f\"Initial rows: {initial_rows:,}\")\n",
        "\n",
        "# Remove null values\n",
        "transactions = transactions.dropna(subset=['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date'])\n",
        "print(f\"After removing nulls: {len(transactions):,} rows\")\n",
        "\n",
        "# Remove duplicates based on key columns\n",
        "before_dedup = len(transactions)\n",
        "transactions = transactions.drop_duplicates(subset=['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity', 'gross_value'])\n",
        "print(f\"Removed {before_dedup - len(transactions):,} duplicate rows\")\n",
        "\n",
        "# Keep only positive quantities\n",
        "transactions = transactions[transactions['quantity'] > 0]\n",
        "print(f\"After removing zero/negative quantities: {len(transactions):,} rows\")\n",
        "\n",
        "# Remove extreme outliers\n",
        "value_per_unit = transactions['gross_value'] / transactions['quantity']\n",
        "q01 = value_per_unit.quantile(0.005)\n",
        "q99 = value_per_unit.quantile(0.995)\n",
        "valid_value_mask = (value_per_unit >= q01) & (value_per_unit <= q99)\n",
        "\n",
        "quantity_q99 = transactions['quantity'].quantile(0.995)\n",
        "valid_qty_mask = transactions['quantity'] <= quantity_q99\n",
        "\n",
        "before_outliers = len(transactions)\n",
        "transactions = transactions[valid_value_mask & valid_qty_mask]\n",
        "print(f\"Removed {before_outliers - len(transactions):,} outlier rows\")\n",
        "\n",
        "# Convert dates and filter to 2022\n",
        "transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])\n",
        "transactions = transactions[transactions['transaction_date'].dt.year == 2022]\n",
        "print(f\"Final 2022 data: {len(transactions):,} rows\")\n",
        "\n",
        "print(f\"Total reduction: {((initial_rows - len(transactions))/initial_rows)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean and prepare products and stores data\n",
        "print(\"Cleaning products and stores data...\")\n",
        "\n",
        "# Clean products\n",
        "products['descricao'] = products['descricao'].fillna('Unknown')\n",
        "products['categoria'] = products['categoria'].fillna('Other') \n",
        "products['marca'] = products['marca'].fillna('Unknown')\n",
        "\n",
        "# Clean stores\n",
        "stores['categoria_pdv'] = stores['categoria_pdv'].fillna('Other')\n",
        "stores['premise'] = stores['premise'].fillna('Unknown')\n",
        "\n",
        "print(f\"Products cleaned: {len(products):,} rows\")\n",
        "print(f\"Stores cleaned: {len(stores):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge transactions with products and stores\n",
        "print(\"Merging data...\")\n",
        "\n",
        "# Merge with products\n",
        "merged_data = transactions.merge(\n",
        "    products, \n",
        "    left_on='internal_product_id', \n",
        "    right_on='produto', \n",
        "    how='left'\n",
        ")\n",
        "print(f\"After product merge: {len(merged_data):,} rows\")\n",
        "\n",
        "# Merge with stores\n",
        "merged_data = merged_data.merge(\n",
        "    stores,\n",
        "    left_on='internal_store_id', \n",
        "    right_on='pdv',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"After store merge: {len(merged_data):,} rows\")\n",
        "\n",
        "print(\"Data merge completed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create daily aggregations with temporal features\n",
        "print(\"Creating daily aggregations...\")\n",
        "\n",
        "# Extract date components\n",
        "merged_data['date'] = merged_data['transaction_date'].dt.date\n",
        "merged_data['year'] = merged_data['transaction_date'].dt.year\n",
        "merged_data['month'] = merged_data['transaction_date'].dt.month\n",
        "merged_data['day'] = merged_data['transaction_date'].dt.day\n",
        "merged_data['dayofweek'] = merged_data['transaction_date'].dt.dayofweek  # 0=Monday\n",
        "merged_data['dayofyear'] = merged_data['transaction_date'].dt.dayofyear\n",
        "merged_data['week'] = merged_data['transaction_date'].dt.isocalendar().week\n",
        "merged_data['quarter'] = merged_data['transaction_date'].dt.quarter\n",
        "\n",
        "# Aggregate by day-store-product\n",
        "daily_data = merged_data.groupby([\n",
        "    'date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'week', 'quarter',\n",
        "    'internal_store_id', 'internal_product_id',\n",
        "    'categoria', 'marca', 'premise', 'categoria_pdv'\n",
        "]).agg({\n",
        "    'quantity': ['sum', 'mean', 'count'],\n",
        "    'gross_value': ['sum', 'mean'],\n",
        "    'net_value': ['sum', 'mean'],\n",
        "    'gross_profit': ['sum', 'mean']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "daily_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in daily_data.columns.values]\n",
        "\n",
        "# Rename for clarity\n",
        "daily_data.rename(columns={\n",
        "    'quantity_sum': 'total_quantity',\n",
        "    'quantity_mean': 'avg_quantity_per_transaction', \n",
        "    'quantity_count': 'num_transactions',\n",
        "    'gross_value_sum': 'total_gross_value',\n",
        "    'gross_value_mean': 'avg_gross_value',\n",
        "    'net_value_sum': 'total_net_value',\n",
        "    'net_value_mean': 'avg_net_value',\n",
        "    'gross_profit_sum': 'total_gross_profit',\n",
        "    'gross_profit_mean': 'avg_gross_profit'\n",
        "}, inplace=True)\n",
        "\n",
        "print(f\"Created daily aggregations: {len(daily_data):,} rows\")\n",
        "print(f\"Date range: {daily_data['date'].min()} to {daily_data['date'].max()}\")\n",
        "print(f\"Unique store-product pairs: {daily_data[['internal_store_id', 'internal_product_id']].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create daily temporal features including day-of-week effects\n",
        "print(\"Creating daily temporal features...\")\n",
        "\n",
        "# Convert date to datetime for sorting\n",
        "daily_data['date'] = pd.to_datetime(daily_data['date'])\n",
        "daily_data = daily_data.sort_values(['internal_store_id', 'internal_product_id', 'date'])\n",
        "\n",
        "# Day-of-week cyclical features (0=Monday, 6=Sunday)\n",
        "daily_data['dayofweek_sin'] = np.sin(2 * np.pi * daily_data['dayofweek'] / 7)\n",
        "daily_data['dayofweek_cos'] = np.cos(2 * np.pi * daily_data['dayofweek'] / 7)\n",
        "\n",
        "# Day-of-year cyclical features (seasonality)\n",
        "daily_data['dayofyear_sin'] = np.sin(2 * np.pi * daily_data['dayofyear'] / 365)\n",
        "daily_data['dayofyear_cos'] = np.cos(2 * np.pi * daily_data['dayofyear'] / 365)\n",
        "\n",
        "# Month cyclical features\n",
        "daily_data['month_sin'] = np.sin(2 * np.pi * daily_data['month'] / 12)\n",
        "daily_data['month_cos'] = np.cos(2 * np.pi * daily_data['month'] / 12)\n",
        "\n",
        "# Week cyclical features\n",
        "daily_data['week_sin'] = np.sin(2 * np.pi * daily_data['week'] / 52)\n",
        "daily_data['week_cos'] = np.cos(2 * np.pi * daily_data['week'] / 52)\n",
        "\n",
        "# Is weekend flag\n",
        "daily_data['is_weekend'] = (daily_data['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Is month start/end\n",
        "daily_data['is_month_start'] = (daily_data['day'] <= 3).astype(int)\n",
        "daily_data['is_month_end'] = (daily_data['day'] >= 28).astype(int)\n",
        "\n",
        "print(\"Daily temporal features created:\")\n",
        "print(\"- Day-of-week sine/cosine encoding\")\n",
        "print(\"- Day-of-year sine/cosine encoding\") \n",
        "print(\"- Month sine/cosine encoding\")\n",
        "print(\"- Week sine/cosine encoding\")\n",
        "print(\"- Weekend flag\")\n",
        "print(\"- Month start/end flags\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create lag features without data leakage (using only past information)\n",
        "print(\"Creating lag features...\")\n",
        "\n",
        "# Sort to ensure proper lag calculation\n",
        "daily_data = daily_data.sort_values(['internal_store_id', 'internal_product_id', 'date'])\n",
        "\n",
        "# Group by store-product pairs\n",
        "grouped = daily_data.groupby(['internal_store_id', 'internal_product_id'])\n",
        "\n",
        "# Lag features (previous days)\n",
        "daily_data['quantity_lag_1'] = grouped['total_quantity'].shift(1)\n",
        "daily_data['quantity_lag_2'] = grouped['total_quantity'].shift(2) \n",
        "daily_data['quantity_lag_3'] = grouped['total_quantity'].shift(3)\n",
        "daily_data['quantity_lag_7'] = grouped['total_quantity'].shift(7)  # Same day last week\n",
        "\n",
        "# Rolling averages (using only past data)\n",
        "daily_data['quantity_rolling_3d'] = grouped['total_quantity'].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "daily_data['quantity_rolling_7d'] = grouped['total_quantity'].shift(1).rolling(window=7, min_periods=1).mean()\n",
        "daily_data['quantity_rolling_30d'] = grouped['total_quantity'].shift(1).rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "# Trend features (growth rates)\n",
        "daily_data['quantity_growth_1d'] = (daily_data['quantity_lag_1'] - daily_data['quantity_lag_2']) / (daily_data['quantity_lag_2'] + 1)\n",
        "daily_data['quantity_growth_7d'] = (daily_data['quantity_lag_1'] - daily_data['quantity_lag_7']) / (daily_data['quantity_lag_7'] + 1)\n",
        "\n",
        "# Days since last sale\n",
        "daily_data['days_since_last_sale'] = grouped['date'].diff().dt.days\n",
        "\n",
        "# Fill NaN values with 0 for lag features\n",
        "lag_columns = [col for col in daily_data.columns if 'lag' in col or 'rolling' in col or 'growth' in col or 'days_since' in col]\n",
        "daily_data[lag_columns] = daily_data[lag_columns].fillna(0)\n",
        "\n",
        "print(f\"Created {len(lag_columns)} lag/trend features:\")\n",
        "for col in lag_columns:\n",
        "    print(f\"- {col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create store and product aggregate features (using only past data)\n",
        "print(\"Creating aggregate features...\")\n",
        "\n",
        "# Store-level daily averages (excluding current day)\n",
        "store_daily_avg = daily_data.groupby(['internal_store_id', 'date'])['total_quantity'].sum().reset_index()\n",
        "store_daily_avg.columns = ['internal_store_id', 'date', 'store_daily_total']\n",
        "store_daily_avg = store_daily_avg.sort_values(['internal_store_id', 'date'])\n",
        "store_daily_avg['store_avg_quantity'] = (\n",
        "    store_daily_avg.groupby('internal_store_id')['store_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Product-level daily averages (excluding current day) \n",
        "product_daily_avg = daily_data.groupby(['internal_product_id', 'date'])['total_quantity'].sum().reset_index()\n",
        "product_daily_avg.columns = ['internal_product_id', 'date', 'product_daily_total']\n",
        "product_daily_avg = product_daily_avg.sort_values(['internal_product_id', 'date'])\n",
        "product_daily_avg['product_avg_quantity'] = (\n",
        "    product_daily_avg.groupby('internal_product_id')['product_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Category-level daily averages (excluding current day)\n",
        "category_daily_avg = daily_data.groupby(['categoria', 'date'])['total_quantity'].sum().reset_index()\n",
        "category_daily_avg.columns = ['categoria', 'date', 'category_daily_total'] \n",
        "category_daily_avg = category_daily_avg.sort_values(['categoria', 'date'])\n",
        "category_daily_avg['category_avg_quantity'] = (\n",
        "    category_daily_avg.groupby('categoria')['category_daily_total']\n",
        "    .shift(1).expanding(min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Merge aggregate features\n",
        "daily_data = daily_data.merge(\n",
        "    store_daily_avg[['internal_store_id', 'date', 'store_avg_quantity']], \n",
        "    on=['internal_store_id', 'date'], how='left'\n",
        ")\n",
        "\n",
        "daily_data = daily_data.merge(\n",
        "    product_daily_avg[['internal_product_id', 'date', 'product_avg_quantity']],\n",
        "    on=['internal_product_id', 'date'], how='left'\n",
        ")\n",
        "\n",
        "daily_data = daily_data.merge(\n",
        "    category_daily_avg[['categoria', 'date', 'category_avg_quantity']],\n",
        "    on=['categoria', 'date'], how='left'\n",
        ")\n",
        "\n",
        "# Fill NaN values with 0\n",
        "daily_data[['store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity']] = (\n",
        "    daily_data[['store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity']].fillna(0)\n",
        ")\n",
        "\n",
        "print(\"Created aggregate features:\")\n",
        "print(\"- Store average quantity (historical)\")\n",
        "print(\"- Product average quantity (historical)\")\n",
        "print(\"- Category average quantity (historical)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with proper time-series split\n",
        "print(\"Preparing training data...\")\n",
        "\n",
        "# Define feature columns\n",
        "categorical_features = ['categoria', 'marca', 'premise', 'categoria_pdv']\n",
        "numerical_features = [\n",
        "    'dayofweek', 'dayofyear', 'month', 'day', 'week', 'quarter',\n",
        "    'dayofweek_sin', 'dayofweek_cos', 'dayofyear_sin', 'dayofyear_cos',\n",
        "    'month_sin', 'month_cos', 'week_sin', 'week_cos',\n",
        "    'is_weekend', 'is_month_start', 'is_month_end',\n",
        "    'num_transactions', 'avg_quantity_per_transaction',\n",
        "    'total_gross_value', 'avg_gross_value', 'total_net_value', 'avg_net_value',\n",
        "    'total_gross_profit', 'avg_gross_profit',\n",
        "    'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3', 'quantity_lag_7',\n",
        "    'quantity_rolling_3d', 'quantity_rolling_7d', 'quantity_rolling_30d',\n",
        "    'quantity_growth_1d', 'quantity_growth_7d', 'days_since_last_sale',\n",
        "    'store_avg_quantity', 'product_avg_quantity', 'category_avg_quantity'\n",
        "]\n",
        "\n",
        "all_features = categorical_features + numerical_features\n",
        "\n",
        "# Ensure categorical features are strings\n",
        "for col in categorical_features:\n",
        "    daily_data[col] = daily_data[col].astype(str)\n",
        "\n",
        "# Filter training data (skip first few days due to lag features)\n",
        "min_date = daily_data['date'].min() + pd.Timedelta(days=7)  # Skip first 7 days\n",
        "train_data = daily_data[daily_data['date'] >= min_date].copy()\n",
        "\n",
        "# Time-based split for validation (last 30 days)\n",
        "max_date = train_data['date'].max()\n",
        "split_date = max_date - pd.Timedelta(days=30)\n",
        "\n",
        "train_mask = train_data['date'] < split_date\n",
        "val_mask = train_data['date'] >= split_date\n",
        "\n",
        "X_train = train_data[train_mask][all_features]\n",
        "y_train = train_data[train_mask]['total_quantity']\n",
        "X_val = train_data[val_mask][all_features]\n",
        "y_val = train_data[val_mask]['total_quantity']\n",
        "\n",
        "# Apply log transformation to target\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log = np.log1p(y_val)\n",
        "\n",
        "print(f\"Training data: {len(X_train):,} samples\")\n",
        "print(f\"Validation data: {len(X_val):,} samples\")\n",
        "print(f\"Training date range: {train_data[train_mask]['date'].min()} to {train_data[train_mask]['date'].max()}\")\n",
        "print(f\"Validation date range: {train_data[val_mask]['date'].min()} to {train_data[val_mask]['date'].max()}\")\n",
        "print(f\"Features: {len(all_features)} ({len(categorical_features)} categorical, {len(numerical_features)} numerical)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost model optimized for daily forecasting\n",
        "print(\"Training CatBoost model...\")\n",
        "\n",
        "# Get categorical feature indices\n",
        "cat_feature_indices = [all_features.index(col) for col in categorical_features]\n",
        "\n",
        "# Optimize for Apple Silicon or use CPU cores\n",
        "n_threads = max(1, int(os.cpu_count() * 0.9))\n",
        "print(f\"Using {n_threads} CPU threads\")\n",
        "\n",
        "# Model parameters optimized for daily data\n",
        "model_params = {\n",
        "    'iterations': 2000,\n",
        "    'learning_rate': 0.02,\n",
        "    'depth': 8,\n",
        "    'l2_leaf_reg': 10,\n",
        "    'bootstrap_type': 'Bayesian',\n",
        "    'bagging_temperature': 0.8,\n",
        "    'random_strength': 1.0,\n",
        "    'border_count': 200,\n",
        "    'loss_function': 'MAE',\n",
        "    'eval_metric': 'MAE',\n",
        "    'random_seed': 42,\n",
        "    'verbose': 200,\n",
        "    'early_stopping_rounds': 300,\n",
        "    'thread_count': n_threads,\n",
        "    'used_ram_limit': '12GB',\n",
        "}\n",
        "\n",
        "print(\"Model parameters:\")\n",
        "for key, value in model_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Initialize and train model\n",
        "model = CatBoostRegressor(**model_params)\n",
        "\n",
        "print(\"\\\\nStarting training...\")\n",
        "model.fit(\n",
        "    X_train, y_train_log,\n",
        "    eval_set=(X_val, y_val_log),\n",
        "    cat_features=cat_feature_indices,\n",
        "    early_stopping_rounds=300,\n",
        "    verbose=200,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "print(\"\\\\nModel training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating model performance...\")\n",
        "\n",
        "train_pred_log = model.predict(X_train)\n",
        "val_pred_log = model.predict(X_val)\n",
        "\n",
        "train_pred = np.expm1(train_pred_log)\n",
        "val_pred = np.expm1(val_pred_log)\n",
        "train_actual = y_train.values\n",
        "val_actual = y_val.values\n",
        "\n",
        "train_pred = np.maximum(0, train_pred)\n",
        "val_pred = np.maximum(0, val_pred)\n",
        "\n",
        "def calculate_wmape(y_true, y_pred):\n",
        "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100 if np.sum(y_true) > 0 else 0\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    mask = y_true > 0\n",
        "    if not mask.any():\n",
        "        return 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "train_wmape = calculate_wmape(train_actual, train_pred)\n",
        "val_wmape = calculate_wmape(val_actual, val_pred)\n",
        "train_mape = calculate_mape(train_actual, train_pred)\n",
        "val_mape = calculate_mape(val_actual, val_pred)\n",
        "\n",
        "train_mae = np.mean(np.abs(train_actual - train_pred))\n",
        "val_mae = np.mean(np.abs(val_actual - val_pred))\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training WMAPE: {train_wmape:.2f}%\")\n",
        "print(f\"Validation WMAPE: {val_wmape:.2f}%\")\n",
        "print(f\"Training MAPE: {train_mape:.2f}%\")\n",
        "print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "wmape_diff = abs(val_wmape - train_wmape)\n",
        "mape_diff = abs(val_mape - train_mape)\n",
        "print(f\"\\\\nOverfitting Check:\")\n",
        "print(f\"WMAPE difference: {wmape_diff:.2f}%\")\n",
        "print(f\"MAPE difference: {mape_diff:.2f}%\")\n",
        "\n",
        "if wmape_diff < 3 and mape_diff < 15:\n",
        "    print(\"Status: Good generalization\")\n",
        "elif wmape_diff < 6 and mape_diff < 25:\n",
        "    print(\"Status: Moderate overfitting\")\n",
        "else:\n",
        "    print(\"Status: High overfitting risk\")\n",
        "\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate daily predictions for January 2023 (5 weeks)\n",
        "print(\"Generating daily predictions for January 2023...\")\n",
        "\n",
        "# Get unique store-product combinations from recent data (last 60 days)\n",
        "recent_date = daily_data['date'].max() - pd.Timedelta(days=60)\n",
        "recent_data = daily_data[daily_data['date'] >= recent_date]\n",
        "active_pairs = recent_data.groupby(['internal_store_id', 'internal_product_id']).agg({\n",
        "    'total_quantity': 'sum',\n",
        "    'date': 'count'\n",
        "}).reset_index()\n",
        "active_pairs.columns = ['internal_store_id', 'internal_product_id', 'total_qty', 'days_active']\n",
        "\n",
        "max_pairs = 290000  # Stay within 1.5M prediction limit\n",
        "active_pairs = active_pairs.sort_values(['total_qty', 'days_active'], ascending=[False, False])\n",
        "selected_pairs = active_pairs.head(max_pairs)[['internal_store_id', 'internal_product_id']]\n",
        "\n",
        "print(f\"Selected {len(selected_pairs):,} active store-product pairs\")\n",
        "\n",
        "# Get latest features for each pair\n",
        "latest_features = daily_data.loc[daily_data.groupby(['internal_store_id', 'internal_product_id'])['date'].idxmax()]\n",
        "latest_features = latest_features.merge(selected_pairs, on=['internal_store_id', 'internal_product_id'], how='inner')\n",
        "\n",
        "print(f\"Latest features available for {len(latest_features):,} pairs\")\n",
        "\n",
        "# Generate predictions for January 2023 (days 1-35, covering 5 weeks)\n",
        "january_dates = pd.date_range('2023-01-01', periods=35, freq='D')\n",
        "all_daily_predictions = []\n",
        "\n",
        "for i, pred_date in enumerate(january_dates):\n",
        "    print(f\"Predicting for {pred_date.strftime('%Y-%m-%d')} ({i+1}/{len(january_dates)})\")\n",
        "    \n",
        "    # Create prediction data\n",
        "    pred_data = latest_features.copy()\n",
        "    \n",
        "    # Update temporal features for prediction date\n",
        "    pred_data['date'] = pred_date\n",
        "    pred_data['year'] = pred_date.year\n",
        "    pred_data['month'] = pred_date.month\n",
        "    pred_data['day'] = pred_date.day\n",
        "    pred_data['dayofweek'] = pred_date.dayofweek\n",
        "    pred_data['dayofyear'] = pred_date.dayofyear\n",
        "    pred_data['week'] = pred_date.isocalendar().week\n",
        "    pred_data['quarter'] = 1  # January is Q1\n",
        "    \n",
        "    # Update cyclical features\n",
        "    pred_data['dayofweek_sin'] = np.sin(2 * np.pi * pred_data['dayofweek'] / 7)\n",
        "    pred_data['dayofweek_cos'] = np.cos(2 * np.pi * pred_data['dayofweek'] / 7)\n",
        "    pred_data['dayofyear_sin'] = np.sin(2 * np.pi * pred_data['dayofyear'] / 365)\n",
        "    pred_data['dayofyear_cos'] = np.cos(2 * np.pi * pred_data['dayofyear'] / 365)\n",
        "    pred_data['month_sin'] = np.sin(2 * np.pi * pred_data['month'] / 12)\n",
        "    pred_data['month_cos'] = np.cos(2 * np.pi * pred_data['month'] / 12)\n",
        "    pred_data['week_sin'] = np.sin(2 * np.pi * pred_data['week'] / 52)\n",
        "    pred_data['week_cos'] = np.cos(2 * np.pi * pred_data['week'] / 52)\n",
        "    \n",
        "    # Update binary features\n",
        "    pred_data['is_weekend'] = (pred_data['dayofweek'] >= 5).astype(int)\n",
        "    pred_data['is_month_start'] = (pred_data['day'] <= 3).astype(int)\n",
        "    pred_data['is_month_end'] = (pred_data['day'] >= 28).astype(int)\n",
        "    \n",
        "    # Make predictions\n",
        "    X_pred = pred_data[all_features]\n",
        "    pred_log = model.predict(X_pred)\n",
        "    predictions = np.expm1(pred_log)\n",
        "    predictions = np.maximum(0, predictions).round().astype(int)\n",
        "    \n",
        "    # Store daily predictions\n",
        "    daily_pred_df = pd.DataFrame({\n",
        "        'date': pred_date,\n",
        "        'pdv': pred_data['internal_store_id'].astype(int),\n",
        "        'produto': pred_data['internal_product_id'].astype(int),\n",
        "        'quantidade_diaria': predictions\n",
        "    })\n",
        "    \n",
        "    all_daily_predictions.append(daily_pred_df)\n",
        "\n",
        "# Combine all daily predictions\n",
        "daily_predictions_df = pd.concat(all_daily_predictions, ignore_index=True)\n",
        "\n",
        "print(f\"\\\\nGenerated {len(daily_predictions_df):,} daily predictions\")\n",
        "print(f\"Date range: {daily_predictions_df['date'].min()} to {daily_predictions_df['date'].max()}\")\n",
        "print(f\"Total predicted quantity: {daily_predictions_df['quantidade_diaria'].sum():,}\")\n",
        "print(f\"Average daily quantity per store-product: {daily_predictions_df['quantidade_diaria'].mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Aggregating daily predictions to weekly format...\")\n",
        "\n",
        "daily_predictions_df['week'] = ((daily_predictions_df['date'] - pd.Timestamp('2023-01-01')).dt.days // 7) + 1\n",
        "\n",
        "# Aggregate by week-store-product\n",
        "weekly_predictions = daily_predictions_df.groupby(['week', 'pdv', 'produto']).agg({\n",
        "    'quantidade_diaria': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Rename for final output format\n",
        "weekly_predictions.rename(columns={\n",
        "    'week': 'semana',\n",
        "    'quantidade_diaria': 'quantidade'\n",
        "}, inplace=True)\n",
        "\n",
        "# Ensure proper data types\n",
        "weekly_predictions['semana'] = weekly_predictions['semana'].astype(int)\n",
        "weekly_predictions['pdv'] = weekly_predictions['pdv'].astype(int)\n",
        "weekly_predictions['produto'] = weekly_predictions['produto'].astype(int)\n",
        "weekly_predictions['quantidade'] = weekly_predictions['quantidade'].astype(int)\n",
        "\n",
        "# Sort by week, store, product\n",
        "weekly_predictions = weekly_predictions.sort_values(['semana', 'pdv', 'produto'])\n",
        "\n",
        "print(f\"\\\\nWeekly aggregation completed:\")\n",
        "print(f\"Total weekly predictions: {len(weekly_predictions):,}\")\n",
        "print(f\"Weeks covered: {weekly_predictions['semana'].min()} to {weekly_predictions['semana'].max()}\")\n",
        "print(f\"Unique stores: {weekly_predictions['pdv'].nunique():,}\")\n",
        "print(f\"Unique products: {weekly_predictions['produto'].nunique():,}\")\n",
        "print(f\"Total predicted quantity: {weekly_predictions['quantidade'].sum():,}\")\n",
        "\n",
        "# Show sample of predictions\n",
        "print(f\"\\\\nSample predictions:\")\n",
        "print(weekly_predictions.head(10))\n",
        "\n",
        "# Weekly summary\n",
        "weekly_summary = weekly_predictions.groupby('semana').agg({\n",
        "    'quantidade': ['count', 'sum', 'mean']\n",
        "}).round(2)\n",
        "weekly_summary.columns = ['predictions_count', 'total_quantity', 'avg_quantity']\n",
        "print(f\"\\\\nWeekly summary:\")\n",
        "print(weekly_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions to CSV and Parquet files\n",
        "print(\"Saving predictions...\")\n",
        "\n",
        "# Save weekly predictions (main output)\n",
        "csv_filename = \"sales_predictions_v3.csv\"\n",
        "parquet_filename = \"sales_predictions_v3.parquet\"\n",
        "\n",
        "weekly_predictions.to_csv(csv_filename, sep=';', index=False, encoding='utf-8')\n",
        "weekly_predictions.to_parquet(parquet_filename, index=False)\n",
        "\n",
        "print(f\"Weekly predictions saved:\")\n",
        "print(f\"- CSV: {csv_filename}\")\n",
        "print(f\"- Parquet: {parquet_filename}\")\n",
        "print(f\"- Rows: {len(weekly_predictions):,}\")\n",
        "print(f\"- Columns: {list(weekly_predictions.columns)}\")\n",
        "\n",
        "# Save daily predictions for analysis (optional)\n",
        "daily_csv_filename = \"sales_predictions_daily_v3.csv\"\n",
        "daily_predictions_df.to_csv(daily_csv_filename, sep=';', index=False, encoding='utf-8')\n",
        "print(f\"\\\\nDaily predictions also saved to: {daily_csv_filename}\")\n",
        "\n",
        "print(\"\\\\nForecasting completed successfully.\")\n",
        "print(\"Model trained on daily data with proper time-series validation.\")\n",
        "print(\"Predictions generated for 5 weeks of January 2023.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
