{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martoto/BigData_ForecastHackathon/blob/main/silicionauta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbfde9a",
      "metadata": {
        "id": "5cbfde9a"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "559b0055",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/memoryman/code/Users/daniel.sant.salles\n"
          ]
        }
      ],
      "source": [
        "%cd Users/daniel.sant.salles/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4472b044",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: pyarrow in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (21.0.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (0.13.2)\n",
            "Collecting plotly\n",
            "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from seaborn) (3.10.3)\n",
            "Collecting narwhals>=1.15.1 (from plotly)\n",
            "  Downloading narwhals-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
            "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/9.8 MB 4.7 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 1.0/9.8 MB 2.6 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.3/9.8 MB 2.4 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 2.1/9.8 MB 2.6 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 2.6/9.8 MB 2.6 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 3.1/9.8 MB 2.7 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 3.9/9.8 MB 2.7 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 4.7/9.8 MB 2.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 5.2/9.8 MB 2.8 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 6.0/9.8 MB 2.9 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.6/9.8 MB 2.9 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 7.3/9.8 MB 2.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 7.9/9.8 MB 2.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 8.1/9.8 MB 2.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.7/9.8 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.7/9.8 MB 2.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.8/9.8 MB 2.9 MB/s eta 0:00:00\n",
            "Downloading narwhals-2.4.0-py3-none-any.whl (406 kB)\n",
            "Installing collected packages: narwhals, plotly\n",
            "Successfully installed narwhals-2.4.0 plotly-6.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas pyarrow seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13da2d6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13da2d6c",
        "outputId": "817e4941-beea-44e1-feb1-d9ef4b53a41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hackathon_2025_templates.zip found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "file_path = '/content/hackathon_2025_templates.zip'\n",
        "file_id = '1Ed-nqUyCT0z-T0AvHwLMsGqHYqjw2248' # This is the file ID\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f'{file_path} not found. Downloading...')\n",
        "\n",
        "    # Construct the direct download URL\n",
        "    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(download_url, stream=True)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f'{file_path} downloaded successfully.')\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        print(\"Please ensure the file is shared publicly or with 'Anyone with the link'.\")\n",
        "\n",
        "else:\n",
        "    print(f'{file_path} found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "de6d0ad1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted hackathon_2025_templates.zip to ./data/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = 'hackathon_2025_templates.zip'\n",
        "extract_dir = './data/'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f'Extracted {zip_path} to {extract_dir}')\n",
        "else:\n",
        "    print(f'{zip_path} not found in the current directory.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0bbfcf34",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdv</th>\n",
              "      <th>premise</th>\n",
              "      <th>categoria_pdv</th>\n",
              "      <th>zipcode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2204965430669363375</td>\n",
              "      <td>On Premise</td>\n",
              "      <td>Mexican Rest</td>\n",
              "      <td>30741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5211957289528622910</td>\n",
              "      <td>On Premise</td>\n",
              "      <td>Hotel/Motel</td>\n",
              "      <td>80011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9024493554530757353</td>\n",
              "      <td>Off Premise</td>\n",
              "      <td>Convenience</td>\n",
              "      <td>80751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8659197371382902429</td>\n",
              "      <td>On Premise</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>80439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1400854873763881130</td>\n",
              "      <td>On Premise</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>30093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   pdv      premise categoria_pdv  zipcode\n",
              "0  2204965430669363375   On Premise  Mexican Rest    30741\n",
              "1  5211957289528622910   On Premise   Hotel/Motel    80011\n",
              "2  9024493554530757353  Off Premise   Convenience    80751\n",
              "3  8659197371382902429   On Premise    Restaurant    80439\n",
              "4  1400854873763881130   On Premise    Restaurant    30093"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "file_path = './data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
        "df_pdv = pd.read_parquet(file_path)\n",
        "display(df_pdv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0ba2fc66",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>internal_store_id</th>\n",
              "      <th>internal_product_id</th>\n",
              "      <th>distributor_id</th>\n",
              "      <th>transaction_date</th>\n",
              "      <th>reference_date</th>\n",
              "      <th>quantity</th>\n",
              "      <th>gross_value</th>\n",
              "      <th>net_value</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>discount</th>\n",
              "      <th>taxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7384367747233276219</td>\n",
              "      <td>328903483604537190</td>\n",
              "      <td>9</td>\n",
              "      <td>2022-07-13</td>\n",
              "      <td>2022-07-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38.125000</td>\n",
              "      <td>37.890625</td>\n",
              "      <td>10.042625</td>\n",
              "      <td>3.950000</td>\n",
              "      <td>0.234375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3536908514005606262</td>\n",
              "      <td>5418855670645487653</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-03-21</td>\n",
              "      <td>2022-03-01</td>\n",
              "      <td>6.0</td>\n",
              "      <td>107.250000</td>\n",
              "      <td>106.440002</td>\n",
              "      <td>24.732002</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3138231730993449825</td>\n",
              "      <td>1087005562675741887</td>\n",
              "      <td>6</td>\n",
              "      <td>2022-09-06</td>\n",
              "      <td>2022-09-01</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.625000</td>\n",
              "      <td>56.220001</td>\n",
              "      <td>14.124002</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3681167389484217654</td>\n",
              "      <td>1401422983880045188</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-09-11</td>\n",
              "      <td>2022-09-01</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1037.160023</td>\n",
              "      <td>1037.160023</td>\n",
              "      <td>156.348026</td>\n",
              "      <td>479.880006</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7762413312337359369</td>\n",
              "      <td>6614994347738381720</td>\n",
              "      <td>4</td>\n",
              "      <td>2022-02-18</td>\n",
              "      <td>2022-02-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.230000</td>\n",
              "      <td>23.950241</td>\n",
              "      <td>6.550241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.279758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     internal_store_id  internal_product_id distributor_id transaction_date  \\\n",
              "0  7384367747233276219   328903483604537190              9       2022-07-13   \n",
              "1  3536908514005606262  5418855670645487653              5       2022-03-21   \n",
              "2  3138231730993449825  1087005562675741887              6       2022-09-06   \n",
              "3  3681167389484217654  1401422983880045188              5       2022-09-11   \n",
              "4  7762413312337359369  6614994347738381720              4       2022-02-18   \n",
              "\n",
              "  reference_date  quantity  gross_value    net_value  gross_profit  \\\n",
              "0     2022-07-01       1.0    38.125000    37.890625     10.042625   \n",
              "1     2022-03-01       6.0   107.250000   106.440002     24.732002   \n",
              "2     2022-09-01       3.0    56.625000    56.220001     14.124002   \n",
              "3     2022-09-01     129.0  1037.160023  1037.160023    156.348026   \n",
              "4     2022-02-01       1.0    26.230000    23.950241      6.550241   \n",
              "\n",
              "     discount     taxes  \n",
              "0    3.950000  0.234375  \n",
              "1   17.100000  0.810000  \n",
              "2    5.250000  0.405000  \n",
              "3  479.880006  0.000000  \n",
              "4    0.000000  2.279758  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "file_path = './data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet'\n",
        "df_transactions = pd.read_parquet(file_path)\n",
        "display(df_transactions.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6e80f95",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7,092 unique product IDs in transactions\n",
            "Total rows processed: 192,356,316\n",
            "Total rows kept: 192,356,316\n",
            "Filtering efficiency: 100.00%\n",
            "Final products dataframe shape: (192356316, 8)\n",
            "\n",
            "First few rows of filtered products:\n",
            "Total rows processed: 192,356,316\n",
            "Total rows kept: 192,356,316\n",
            "Filtering efficiency: 100.00%\n",
            "Final products dataframe shape: (192356316, 8)\n",
            "\n",
            "First few rows of filtered products:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>produto</th>\n",
              "      <th>categoria</th>\n",
              "      <th>descricao</th>\n",
              "      <th>tipos</th>\n",
              "      <th>label</th>\n",
              "      <th>subcategoria</th>\n",
              "      <th>marca</th>\n",
              "      <th>fabricante</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69753381296344216</td>\n",
              "      <td>Package</td>\n",
              "      <td>MONDAY NIGHT BREW DON'T CALL IT HOTLANTA 6/4/1...</td>\n",
              "      <td>Package</td>\n",
              "      <td>In&amp;Out</td>\n",
              "      <td>Red Wine</td>\n",
              "      <td>Monday Night Don't Call It Hotlanta</td>\n",
              "      <td>Monday Night Brewing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5019314249828979377</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>BARRELL V907 RED PHONE BOOTH &amp; TOWER</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>Specialty</td>\n",
              "      <td>Lager / Pilsner</td>\n",
              "      <td>Barrell Bourbon- Private Label</td>\n",
              "      <td>Barrell Craft Spirits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4016404282141162328</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>DRINKWORKS SIMPLY REFRESHING GIN &amp; TONIC 12/4/...</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>Core</td>\n",
              "      <td>Lager</td>\n",
              "      <td>Drinkworks Simply Refreshing Gin &amp; Tonic</td>\n",
              "      <td>AB Drinkworks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6217366559810422145</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>CENZON REPOSADO 100% AGAVE 6/750ML 80PF</td>\n",
              "      <td>Distilled Spirits</td>\n",
              "      <td>None</td>\n",
              "      <td>Lager</td>\n",
              "      <td>Cenzon Reposado 100% Agave Tequila</td>\n",
              "      <td>Sazerac Spirits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7356488787409434558</td>\n",
              "      <td>Draft</td>\n",
              "      <td>REFORMATION STARK 1/4 KEG</td>\n",
              "      <td>Draft</td>\n",
              "      <td>Core</td>\n",
              "      <td>Red Wine</td>\n",
              "      <td>Reformation Stark</td>\n",
              "      <td>Reformation Brewery</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               produto          categoria  \\\n",
              "0    69753381296344216            Package   \n",
              "1  5019314249828979377  Distilled Spirits   \n",
              "2  4016404282141162328  Distilled Spirits   \n",
              "3  6217366559810422145  Distilled Spirits   \n",
              "4  7356488787409434558              Draft   \n",
              "\n",
              "                                           descricao              tipos  \\\n",
              "0  MONDAY NIGHT BREW DON'T CALL IT HOTLANTA 6/4/1...            Package   \n",
              "1               BARRELL V907 RED PHONE BOOTH & TOWER  Distilled Spirits   \n",
              "2  DRINKWORKS SIMPLY REFRESHING GIN & TONIC 12/4/...  Distilled Spirits   \n",
              "3            CENZON REPOSADO 100% AGAVE 6/750ML 80PF  Distilled Spirits   \n",
              "4                          REFORMATION STARK 1/4 KEG              Draft   \n",
              "\n",
              "       label     subcategoria                                     marca  \\\n",
              "0     In&Out         Red Wine       Monday Night Don't Call It Hotlanta   \n",
              "1  Specialty  Lager / Pilsner            Barrell Bourbon- Private Label   \n",
              "2       Core            Lager  Drinkworks Simply Refreshing Gin & Tonic   \n",
              "3       None            Lager        Cenzon Reposado 100% Agave Tequila   \n",
              "4       Core         Red Wine                         Reformation Stark   \n",
              "\n",
              "              fabricante  \n",
              "0   Monday Night Brewing  \n",
              "1  Barrell Craft Spirits  \n",
              "2          AB Drinkworks  \n",
              "3        Sazerac Spirits  \n",
              "4    Reformation Brewery  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Get unique product IDs from transactions for filtering\n",
        "valid_product_ids = set(df_transactions['internal_product_id'].unique())\n",
        "print(f\"Found {len(valid_product_ids):,} unique product IDs in transactions\")\n",
        "\n",
        "file_path = './data/part-00000-tid-6364321654468257203-dc13a5d6-36ae-48c6-a018-37d8cfe34cf6-263-1-c000.snappy.parquet'\n",
        "table = pq.ParquetFile(file_path)\n",
        "\n",
        "# Collect filtered chunks\n",
        "filtered_chunks = []\n",
        "total_rows_processed = 0\n",
        "total_rows_kept = 0\n",
        "\n",
        "for batch in table.iter_batches(batch_size=5000):\n",
        "    df_chunk = batch.to_pandas()\n",
        "    total_rows_processed += len(df_chunk)\n",
        "    \n",
        "    # Filter to keep only products that exist in transactions\n",
        "    filtered_chunk = df_chunk[df_chunk['produto'].isin(valid_product_ids)]\n",
        "    total_rows_kept += len(filtered_chunk)\n",
        "    \n",
        "    if len(filtered_chunk) > 0:\n",
        "        filtered_chunks.append(filtered_chunk)\n",
        "\n",
        "# Combine all filtered chunks\n",
        "df_products = pd.concat(filtered_chunks, ignore_index=True) if filtered_chunks else pd.DataFrame()\n",
        "\n",
        "print(f\"Total rows processed: {total_rows_processed:,}\")\n",
        "print(f\"Total rows kept: {total_rows_kept:,}\")\n",
        "print(f\"Filtering efficiency: {(total_rows_kept/total_rows_processed)*100:.2f}%\")\n",
        "print(f\"Final products dataframe shape: {df_products.shape}\")\n",
        "print(\"\\nFirst few rows of filtered products:\")\n",
        "display(df_products.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794e610a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification of the filtering results\n",
        "print(\"üîç FILTERING VERIFICATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check unique products in filtered dataset\n",
        "unique_products_filtered = df_products['produto'].nunique()\n",
        "print(f\"Unique products in filtered dataset: {unique_products_filtered:,}\")\n",
        "\n",
        "# Check if all transaction products are covered\n",
        "transaction_products = set(df_transactions['internal_product_id'].unique())\n",
        "filtered_products = set(df_products['produto'].unique())\n",
        "\n",
        "print(f\"Unique products in transactions: {len(transaction_products):,}\")\n",
        "print(f\"Unique products in filtered products: {len(filtered_products):,}\")\n",
        "\n",
        "# Check intersection\n",
        "intersection = transaction_products.intersection(filtered_products)\n",
        "print(f\"Products in both datasets: {len(intersection):,}\")\n",
        "\n",
        "# Check if there are products in transactions not in products\n",
        "missing_in_products = transaction_products - filtered_products\n",
        "if missing_in_products:\n",
        "    print(f\"‚ö†Ô∏è  Products in transactions but missing in products: {len(missing_in_products)}\")\n",
        "    print(f\"Examples: {list(missing_in_products)[:10]}\")\n",
        "else:\n",
        "    print(\"‚úÖ All transaction products are covered in the products dataset\")\n",
        "\n",
        "# Check if there are products in products not in transactions\n",
        "missing_in_transactions = filtered_products - transaction_products\n",
        "if missing_in_transactions:\n",
        "    print(f\"‚ö†Ô∏è  Products in products but missing in transactions: {len(missing_in_transactions)}\")\n",
        "    print(f\"Examples: {list(missing_in_transactions)[:10]}\")\n",
        "else:\n",
        "    print(\"‚úÖ All products have corresponding transactions\")\n",
        "\n",
        "print(f\"\\nüìä Data consistency: {'Perfect match!' if len(missing_in_products) == 0 and len(missing_in_transactions) == 0 else 'Some mismatches found'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d4a337",
      "metadata": {},
      "source": [
        "1- pdv\tpremise\tcategoria_pdv\tzipcode \n",
        "2- internal_store_id\tinternal_product_id\tdistributor_id\ttransaction_date\treference_date\tquantity\tgross_value\tnet_value\tgross_profit\tdiscount\ttaxes\n",
        "3- produto categoria descricao tipos label subcategoria marca fabricante               \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe54ba0",
      "metadata": {},
      "source": [
        "## Vis√£o Geral da Estrutura dos Dados\n",
        "\n",
        "### üìä Dataset 1: Ponto de Venda (PDV)\n",
        "| Coluna | Descri√ß√£o |\n",
        "|--------|-----------|\n",
        "| `pdv` | Identificador do Ponto de Venda |\n",
        "| `premise` | Informa√ß√µes da loja/estabelecimento |\n",
        "| `categoria_pdv` | Classifica√ß√£o da categoria do PDV |\n",
        "| `zipcode` | C√≥digo postal/CEP |\n",
        "\n",
        "### üí≥ Dataset 2: Transa√ß√µes\n",
        "| Coluna | Descri√ß√£o |\n",
        "|--------|-----------|\n",
        "| `internal_store_id` | Identificador interno da loja |\n",
        "| `internal_product_id` | Identificador interno do produto |\n",
        "| `distributor_id` | Identificador do distribuidor |\n",
        "| `transaction_date` | Data da transa√ß√£o |\n",
        "| `reference_date` | Data de refer√™ncia |\n",
        "| `quantity` | Quantidade da transa√ß√£o |\n",
        "| `gross_value` | Valor bruto da transa√ß√£o |\n",
        "| `net_value` | Valor l√≠quido da transa√ß√£o |\n",
        "| `gross_profit` | Lucro bruto |\n",
        "| `discount` | Desconto aplicado |\n",
        "| `taxes` | Valor dos impostos |\n",
        "\n",
        "### üõçÔ∏è Dataset 3: Produtos\n",
        "| Coluna | Descri√ß√£o |\n",
        "|--------|-----------|\n",
        "| `produto` | Identificador do produto |\n",
        "| `categoria` | Categoria do produto |\n",
        "| `descricao` | Descri√ß√£o do produto |\n",
        "| `tipos` | Tipos do produto |\n",
        "| `label` | R√≥tulo do produto |\n",
        "| `subcategoria` | Subcategoria do produto |\n",
        "| `marca` | Marca |\n",
        "| `fabricante` | Fabricante |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd45d3a",
      "metadata": {},
      "source": [
        "# üöÄ EXPLORATORY DATA ANALYSIS - HACKATHON 2025\n",
        "## Going HAM on the Data! üí™\n",
        "\n",
        "Let's dive deep into this retail dataset and uncover insights for forecasting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "823fad08",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Libraries loaded successfully! Let's go HAM on this data!\n"
          ]
        }
      ],
      "source": [
        "# Import comprehensive analysis libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure visualization settings\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"üöÄ Libraries loaded successfully! Let's go HAM on this data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff35384",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading all three datasets...\n",
            "‚úÖ PDV Dataset loaded: (14419, 4)\n",
            "‚úÖ Transactions Dataset loaded: (6560698, 11)\n"
          ]
        }
      ],
      "source": [
        "# üìä LOAD ALL DATASETS\n",
        "print(\"üîÑ Loading all three datasets...\")\n",
        "\n",
        "# Dataset 1: PDV (Points of Sale)\n",
        "pdv_file = './data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
        "df_pdv = pd.read_parquet(pdv_file)\n",
        "print(f\"‚úÖ PDV Dataset loaded: {df_pdv.shape}\")\n",
        "\n",
        "# Dataset 2: Transactions\n",
        "transactions_file = './data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet'\n",
        "df_transactions = pd.read_parquet(transactions_file)\n",
        "print(f\"‚úÖ Transactions Dataset loaded: {df_transactions.shape}\")\n",
        "\n",
        "# Dataset 3: Products (using chunked reading for large file)\n",
        "products_file = './data/part-00000-tid-6364321654468257203-dc13a5d6-36ae-48c6-a018-37d8cfe34cf6-263-1-c000.snappy.parquet'\n",
        "df_products = pd.read_parquet(products_file)\n",
        "print(f\"‚úÖ Products Dataset loaded: {df_products.shape}\")\n",
        "\n",
        "print(f\"\\nüéØ Total datasets loaded: 3\")\n",
        "print(f\"üìà Combined data points: {df_pdv.shape[0] + df_transactions.shape[0] + df_products.shape[0]:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d24ce161",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîç DEEP DIVE INTO DATASET STRUCTURES\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df_pdv' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç DEEP DIVE INTO DATASET STRUCTURES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      6\u001b[39m datasets = {\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPDV\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mdf_pdv\u001b[49m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTransactions\u001b[39m\u001b[33m'\u001b[39m: df_transactions,\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mProducts\u001b[39m\u001b[33m'\u001b[39m: df_products\n\u001b[32m     10\u001b[39m }\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m datasets.items():\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m DATASET ANALYSIS:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'df_pdv' is not defined"
          ]
        }
      ],
      "source": [
        "# üîç DATASET STRUCTURE ANALYSIS - Going Deep!\n",
        "print(\"=\"*80)\n",
        "print(\"üîç DEEP DIVE INTO DATASET STRUCTURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "datasets = {\n",
        "    'PDV': df_pdv,\n",
        "    'Transactions': df_transactions,\n",
        "    'Products': df_products\n",
        "}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\nüìä {name.upper()} DATASET ANALYSIS:\")\n",
        "    print(f\"   üìè Shape: {df.shape}\")\n",
        "    print(f\"   üìã Columns: {list(df.columns)}\")\n",
        "    print(f\"   üè∑Ô∏è  Data Types:\")\n",
        "    for col, dtype in df.dtypes.items():\n",
        "        print(f\"      {col}: {dtype}\")\n",
        "    print(f\"   üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"   üï≥Ô∏è  Missing Values: {df.isnull().sum().sum()}\")\n",
        "    print(f\"   üî¢ Unique Values per Column:\")\n",
        "    for col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"      {col}: {unique_count:,}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ba9777",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üßπ DATA QUALITY ASSESSMENT - The Deep Clean!\n",
        "print(\"=\"*80)\n",
        "print(\"üßπ DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def analyze_data_quality(df, name):\n",
        "    print(f\"\\nüîç {name} QUALITY REPORT:\")\n",
        "    \n",
        "    # Missing values analysis\n",
        "    missing_values = df.isnull().sum()\n",
        "    missing_percent = (missing_values / len(df)) * 100\n",
        "    \n",
        "    print(f\"üìä Missing Values Analysis:\")\n",
        "    for col in df.columns:\n",
        "        if missing_values[col] > 0:\n",
        "            print(f\"   {col}: {missing_values[col]:,} ({missing_percent[col]:.2f}%)\")\n",
        "    \n",
        "    # Duplicates analysis\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"üîÑ Duplicate Rows: {duplicates:,} ({(duplicates/len(df)*100):.2f}%)\")\n",
        "    \n",
        "    # Numerical columns analysis\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numerical_cols) > 0:\n",
        "        print(f\"üìà Numerical Columns Analysis:\")\n",
        "        for col in numerical_cols:\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            outliers = ((df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))).sum()\n",
        "            print(f\"   {col}: Min={df[col].min():.2f}, Max={df[col].max():.2f}, Outliers={outliers:,}\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Analyze each dataset\n",
        "for name, df in datasets.items():\n",
        "    analyze_data_quality(df, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f011f8bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üí∞ SALES TRANSACTION ANALYSIS - The Money Maker!\n",
        "print(\"=\"*80)\n",
        "print(\"üí∞ SALES TRANSACTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert date columns to datetime\n",
        "if 'transaction_date' in df_transactions.columns:\n",
        "    df_transactions['transaction_date'] = pd.to_datetime(df_transactions['transaction_date'])\n",
        "if 'reference_date' in df_transactions.columns:\n",
        "    df_transactions['reference_date'] = pd.to_datetime(df_transactions['reference_date'])\n",
        "\n",
        "# Time period analysis\n",
        "print(f\"üìÖ Transaction Date Range:\")\n",
        "if 'transaction_date' in df_transactions.columns:\n",
        "    print(f\"   From: {df_transactions['transaction_date'].min()}\")\n",
        "    print(f\"   To: {df_transactions['transaction_date'].max()}\")\n",
        "    print(f\"   Duration: {(df_transactions['transaction_date'].max() - df_transactions['transaction_date'].min()).days} days\")\n",
        "\n",
        "# Financial metrics overview\n",
        "numerical_cols = ['quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes']\n",
        "available_cols = [col for col in numerical_cols if col in df_transactions.columns]\n",
        "\n",
        "print(f\"\\nüíµ Financial Metrics Summary:\")\n",
        "if available_cols:\n",
        "    for col in available_cols:\n",
        "        print(f\"   {col}:\")\n",
        "        print(f\"      Total: ${df_transactions[col].sum():,.2f}\")\n",
        "        print(f\"      Average: ${df_transactions[col].mean():.2f}\")\n",
        "        print(f\"      Median: ${df_transactions[col].median():.2f}\")\n",
        "        print(f\"      Std Dev: ${df_transactions[col].std():.2f}\")\n",
        "\n",
        "# Top performing stores and products\n",
        "print(f\"\\nüè™ Top 10 Stores by Total Revenue:\")\n",
        "if 'internal_store_id' in df_transactions.columns and 'gross_value' in df_transactions.columns:\n",
        "    top_stores = df_transactions.groupby('internal_store_id')['gross_value'].sum().sort_values(ascending=False).head(10)\n",
        "    for store_id, revenue in top_stores.items():\n",
        "        print(f\"   Store {store_id}: ${revenue:,.2f}\")\n",
        "\n",
        "print(f\"\\nüõçÔ∏è Top 10 Products by Total Sales:\")\n",
        "if 'internal_product_id' in df_transactions.columns and 'gross_value' in df_transactions.columns:\n",
        "    top_products = df_transactions.groupby('internal_product_id')['gross_value'].sum().sort_values(ascending=False).head(10)\n",
        "    for product_id, revenue in top_products.items():\n",
        "        print(f\"   Product {product_id}: ${revenue:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd05a43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà TIME SERIES ANALYSIS - The Time Machine!\n",
        "print(\"=\"*80)\n",
        "print(\"üìà TIME SERIES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'transaction_date' in df_transactions.columns:\n",
        "    # Extract time components\n",
        "    df_transactions['year'] = df_transactions['transaction_date'].dt.year\n",
        "    df_transactions['month'] = df_transactions['transaction_date'].dt.month\n",
        "    df_transactions['day'] = df_transactions['transaction_date'].dt.day\n",
        "    df_transactions['weekday'] = df_transactions['transaction_date'].dt.day_name()\n",
        "    df_transactions['week'] = df_transactions['transaction_date'].dt.isocalendar().week\n",
        "    \n",
        "    # Daily sales analysis\n",
        "    daily_sales = df_transactions.groupby('transaction_date').agg({\n",
        "        'gross_value': ['sum', 'mean', 'count'],\n",
        "        'quantity': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    print(f\"üìä Daily Sales Statistics:\")\n",
        "    print(f\"   Total Days with Transactions: {len(daily_sales)}\")\n",
        "    print(f\"   Average Daily Revenue: ${daily_sales[('gross_value', 'sum')].mean():,.2f}\")\n",
        "    print(f\"   Best Day Revenue: ${daily_sales[('gross_value', 'sum')].max():,.2f}\")\n",
        "    print(f\"   Worst Day Revenue: ${daily_sales[('gross_value', 'sum')].min():,.2f}\")\n",
        "    \n",
        "    # Weekly patterns\n",
        "    weekly_patterns = df_transactions.groupby('weekday')['gross_value'].sum().sort_values(ascending=False)\n",
        "    print(f\"\\nüìÖ Weekly Sales Patterns:\")\n",
        "    for day, revenue in weekly_patterns.items():\n",
        "        print(f\"   {day}: ${revenue:,.2f}\")\n",
        "    \n",
        "    # Monthly trends\n",
        "    if 'gross_value' in df_transactions.columns:\n",
        "        monthly_trends = df_transactions.groupby(['year', 'month'])['gross_value'].sum()\n",
        "        print(f\"\\nüóìÔ∏è Monthly Revenue Trends:\")\n",
        "        for (year, month), revenue in monthly_trends.items():\n",
        "            print(f\"   {year}-{month:02d}: ${revenue:,.2f}\")\n",
        "\n",
        "# Seasonality detection\n",
        "print(f\"\\nüåü Seasonality Insights:\")\n",
        "if 'month' in df_transactions.columns:\n",
        "    seasonal_pattern = df_transactions.groupby('month')['gross_value'].mean().round(2)\n",
        "    peak_month = seasonal_pattern.idxmax()\n",
        "    low_month = seasonal_pattern.idxmin()\n",
        "    print(f\"   Peak Sales Month: {peak_month} (${seasonal_pattern[peak_month]:,.2f} avg)\")\n",
        "    print(f\"   Lowest Sales Month: {low_month} (${seasonal_pattern[low_month]:,.2f} avg)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c572c185",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üõçÔ∏è PRODUCT PORTFOLIO ANALYSIS - The Product Universe!\n",
        "print(\"=\"*80)\n",
        "print(\"üõçÔ∏è PRODUCT PORTFOLIO ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Product categories analysis\n",
        "print(\"üì¶ Product Categories Overview:\")\n",
        "if 'categoria' in df_products.columns:\n",
        "    category_counts = df_products['categoria'].value_counts()\n",
        "    print(f\"   Total Categories: {len(category_counts)}\")\n",
        "    print(\"   Top 10 Categories:\")\n",
        "    for cat, count in category_counts.head(10).items():\n",
        "        print(f\"      {cat}: {count:,} products\")\n",
        "\n",
        "# Brand analysis\n",
        "print(f\"\\nüè∑Ô∏è Brand Analysis:\")\n",
        "if 'marca' in df_products.columns:\n",
        "    brand_counts = df_products['marca'].value_counts()\n",
        "    print(f\"   Total Brands: {len(brand_counts)}\")\n",
        "    print(\"   Top 10 Brands:\")\n",
        "    for brand, count in brand_counts.head(10).items():\n",
        "        print(f\"      {brand}: {count:,} products\")\n",
        "\n",
        "# Manufacturer analysis\n",
        "print(f\"\\nüè≠ Manufacturer Analysis:\")\n",
        "if 'fabricante' in df_products.columns:\n",
        "    mfg_counts = df_products['fabricante'].value_counts()\n",
        "    print(f\"   Total Manufacturers: {len(mfg_counts)}\")\n",
        "    print(\"   Top 10 Manufacturers:\")\n",
        "    for mfg, count in mfg_counts.head(10).items():\n",
        "        print(f\"      {mfg}: {count:,} products\")\n",
        "\n",
        "# Product types analysis\n",
        "print(f\"\\nüéØ Product Types Distribution:\")\n",
        "if 'tipos' in df_products.columns:\n",
        "    type_counts = df_products['tipos'].value_counts()\n",
        "    print(f\"   Total Types: {len(type_counts)}\")\n",
        "    for ptype, count in type_counts.head(15).items():\n",
        "        print(f\"      {ptype}: {count:,} products\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ee36b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üó∫Ô∏è GEOGRAPHICAL ANALYSIS - The Map Masters!\n",
        "print(\"=\"*80)\n",
        "print(\"üó∫Ô∏è GEOGRAPHICAL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# PDV location analysis\n",
        "if 'zipcode' in df_pdv.columns:\n",
        "    zipcode_analysis = df_pdv['zipcode'].value_counts()\n",
        "    print(f\"üìç Zipcode Distribution:\")\n",
        "    print(f\"   Total Unique Zipcodes: {len(zipcode_analysis)}\")\n",
        "    print(\"   Top 15 Zipcodes by Store Count:\")\n",
        "    for zip_code, count in zipcode_analysis.head(15).items():\n",
        "        print(f\"      {zip_code}: {count:,} stores\")\n",
        "\n",
        "# PDV category analysis\n",
        "if 'categoria_pdv' in df_pdv.columns:\n",
        "    pdv_categories = df_pdv['categoria_pdv'].value_counts()\n",
        "    print(f\"\\nüè™ PDV Categories:\")\n",
        "    print(f\"   Total Categories: {len(pdv_categories)}\")\n",
        "    for category, count in pdv_categories.items():\n",
        "        print(f\"      {category}: {count:,} stores\")\n",
        "\n",
        "# Store premise analysis\n",
        "if 'premise' in df_pdv.columns:\n",
        "    premise_analysis = df_pdv['premise'].value_counts()\n",
        "    print(f\"\\nüè¢ Store Premise Types:\")\n",
        "    print(f\"   Total Premise Types: {len(premise_analysis)}\")\n",
        "    for premise, count in premise_analysis.head(10).items():\n",
        "        print(f\"      {premise}: {count:,} stores\")\n",
        "\n",
        "print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14eefe09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà ADVANCED VISUALIZATIONS - The Eye Candy!\n",
        "print(\"=\"*80)\n",
        "print(\"üìà CREATING KILLER VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a subplot figure for comprehensive analysis\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Daily Sales Trend', 'Weekly Pattern', 'Top Categories', 'Revenue Distribution'),\n",
        "    specs=[[{\"secondary_y\": True}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"pie\"}, {\"type\": \"histogram\"}]]\n",
        ")\n",
        "\n",
        "# 1. Daily sales trend\n",
        "if 'transaction_date' in df_transactions.columns and 'gross_value' in df_transactions.columns:\n",
        "    daily_sales = df_transactions.groupby('transaction_date')['gross_value'].sum().reset_index()\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=daily_sales['transaction_date'], y=daily_sales['gross_value'],\n",
        "                  mode='lines+markers', name='Daily Sales'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# 2. Weekly patterns\n",
        "if 'weekday' in df_transactions.columns:\n",
        "    weekly_data = df_transactions.groupby('weekday')['gross_value'].sum().reset_index()\n",
        "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    weekly_data['weekday'] = pd.Categorical(weekly_data['weekday'], categories=weekday_order, ordered=True)\n",
        "    weekly_data = weekly_data.sort_values('weekday')\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=weekly_data['weekday'], y=weekly_data['gross_value'],\n",
        "               name='Weekly Sales', marker_color='lightblue'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# 3. Top product categories (if we can link products to transactions)\n",
        "if 'categoria' in df_products.columns:\n",
        "    top_categories = df_products['categoria'].value_counts().head(8)\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=top_categories.index, values=top_categories.values,\n",
        "               name=\"Categories\"),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# 4. Revenue distribution\n",
        "if 'gross_value' in df_transactions.columns:\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df_transactions['gross_value'], name='Revenue Distribution',\n",
        "                    nbinsx=50, marker_color='orange'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(height=800, showlegend=True, \n",
        "                  title_text=\"üöÄ Comprehensive Retail Analytics Dashboard\")\n",
        "fig.show()\n",
        "\n",
        "print(\"‚úÖ Interactive dashboard created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bde639",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîó CORRELATION ANALYSIS - Finding Hidden Relationships!\n",
        "print(\"=\"*80)\n",
        "print(\"üîó CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Financial metrics correlation\n",
        "financial_cols = ['quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes']\n",
        "available_financial = [col for col in financial_cols if col in df_transactions.columns]\n",
        "\n",
        "if len(available_financial) > 1:\n",
        "    correlation_matrix = df_transactions[available_financial].corr()\n",
        "    \n",
        "    # Create correlation heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "    plt.title('üí∞ Financial Metrics Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üîç Key Correlations:\")\n",
        "    # Find strongest correlations\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_val = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_val) > 0.7:  # Strong correlation\n",
        "                col1, col2 = correlation_matrix.columns[i], correlation_matrix.columns[j]\n",
        "                print(f\"   {col1} ‚Üî {col2}: {corr_val:.3f}\")\n",
        "\n",
        "# Distributor performance analysis\n",
        "if 'distributor_id' in df_transactions.columns and 'gross_value' in df_transactions.columns:\n",
        "    distributor_performance = df_transactions.groupby('distributor_id').agg({\n",
        "        'gross_value': ['sum', 'mean', 'count'],\n",
        "        'quantity': 'sum',\n",
        "        'gross_profit': 'sum' if 'gross_profit' in df_transactions.columns else 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    print(f\"\\nüöö Top 10 Distributors by Revenue:\")\n",
        "    top_distributors = df_transactions.groupby('distributor_id')['gross_value'].sum().sort_values(ascending=False).head(10)\n",
        "    for dist_id, revenue in top_distributors.items():\n",
        "        transactions_count = df_transactions[df_transactions['distributor_id'] == dist_id].shape[0]\n",
        "        print(f\"   Distributor {dist_id}: ${revenue:,.2f} ({transactions_count:,} transactions)\")\n",
        "\n",
        "print(\"-\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
