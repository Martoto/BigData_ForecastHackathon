{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecast Model V2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries and setup environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Optional: Optuna for hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Warning: Optuna not available. Install with: pip install optuna\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the main SalesForecastModelV2 class with initialization\n",
        "class SalesForecastModelV2:\n",
        "    def __init__(self, data_path=\"data/\"):\n",
        "        self.data_path = data_path\n",
        "        self.transactions = None\n",
        "        self.products = None\n",
        "        self.stores = None\n",
        "        self.model = None\n",
        "        self.label_encoders = {}\n",
        "        self.validation_metrics = {}\n",
        "        self.best_params = None\n",
        "        self.target_log_transformed = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from parquet files and identify data types\n",
        "def load_data(self):\n",
        "    print(\"Loading data...\")\n",
        "    parquet_files = [f for f in os.listdir(self.data_path) if f.endswith('.parquet')]\n",
        "    print(f\"Found {len(parquet_files)} parquet files\")\n",
        "    \n",
        "    for file in parquet_files:\n",
        "        df = pd.read_parquet(os.path.join(self.data_path, file))\n",
        "        print(f\"{file}: Shape {df.shape}\")\n",
        "        \n",
        "        if 'internal_store_id' in df.columns and 'quantity' in df.columns:\n",
        "            self.transactions = df\n",
        "            print(\"-> Identified as TRANSACTIONS data\")\n",
        "        elif 'produto' in df.columns and 'categoria' in df.columns:\n",
        "            self.products = df\n",
        "            print(\"-> Identified as PRODUCTS data\")\n",
        "        elif 'pdv' in df.columns and 'premise' in df.columns:\n",
        "            self.stores = df\n",
        "            print(\"-> Identified as STORES data\")\n",
        "    \n",
        "    print(f\"Data loaded successfully:\")\n",
        "    print(f\"- Transactions: {self.transactions.shape[0]:,} rows\")\n",
        "    print(f\"- Products: {self.products.shape[0]:,} rows\")\n",
        "    print(f\"- Stores: {self.stores.shape[0]:,} rows\")\n",
        "\n",
        "SalesForecastModelV2.load_data = load_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data by removing nulls, outliers, and filtering to 2022 data\n",
        "def cleanse_data(self):\n",
        "    print(\"\\nCleansing data...\")\n",
        "    \n",
        "    initial_rows = len(self.transactions)\n",
        "    self.transactions = self.transactions.dropna(subset=['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date'])\n",
        "    print(f\"Removed {initial_rows - len(self.transactions):,} rows with null values\")\n",
        "    \n",
        "    self.transactions = self.transactions[self.transactions['quantity'] > 0]\n",
        "    print(f\"Kept {len(self.transactions):,} rows with positive quantities\")\n",
        "    \n",
        "    # Remove extreme value outliers (likely data errors)\n",
        "    print(\"Removing extreme value outliers...\")\n",
        "    initial_rows = len(self.transactions)\n",
        "    \n",
        "    # Calculate value per unit to detect unrealistic transactions\n",
        "    value_per_unit = self.transactions['gross_value'] / self.transactions['quantity']\n",
        "    q01 = value_per_unit.quantile(0.005)\n",
        "    q99 = value_per_unit.quantile(0.995)\n",
        "    \n",
        "    # Remove transactions with extreme value per unit\n",
        "    valid_value_mask = (value_per_unit >= q01) & (value_per_unit <= q99)\n",
        "    self.transactions = self.transactions[valid_value_mask]\n",
        "    \n",
        "    # Also cap extreme quantities (likely bulk orders or errors)\n",
        "    quantity_q99 = self.transactions['quantity'].quantile(0.995)\n",
        "    extreme_qty_mask = self.transactions['quantity'] <= quantity_q99\n",
        "    self.transactions = self.transactions[extreme_qty_mask]\n",
        "    \n",
        "    print(f\"Removed {initial_rows - len(self.transactions):,} outlier transactions ({((initial_rows - len(self.transactions))/initial_rows)*100:.2f}%)\")\n",
        "    \n",
        "    self.transactions[['transaction_date', 'reference_date']] = self.transactions[['transaction_date', 'reference_date']].apply(pd.to_datetime)\n",
        "    \n",
        "    self.transactions = self.transactions[\n",
        "        (self.transactions['transaction_date'].dt.year == 2022)\n",
        "    ]\n",
        "    print(f\"Filtered to 2022 data: {len(self.transactions):,} rows\")\n",
        "    \n",
        "    print(\"Cleaning products and stores...\")\n",
        "    self.products['descricao'] = self.products['descricao'].fillna('Unknown')\n",
        "    self.products['categoria'] = self.products['categoria'].fillna('Other')\n",
        "    self.products['marca'] = self.products['marca'].fillna('Unknown')\n",
        "    \n",
        "    self.stores['categoria_pdv'] = self.stores['categoria_pdv'].fillna('Other')\n",
        "    self.stores['premise'] = self.stores['premise'].fillna('Unknown')\n",
        "\n",
        "SalesForecastModelV2.cleanse_data = cleanse_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge transactions with product and store data\n",
        "def merge_data(self):\n",
        "    print(\"\\nMerging data...\")\n",
        "    \n",
        "    merged_data = self.transactions.merge(\n",
        "        self.products, \n",
        "        left_on='internal_product_id', \n",
        "        right_on='produto', \n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"After product merge: {len(merged_data):,} rows\")\n",
        "    \n",
        "    merged_data = merged_data.merge(\n",
        "        self.stores,\n",
        "        left_on='internal_store_id',\n",
        "        right_on='pdv',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"After store merge: {len(merged_data):,} rows\")\n",
        "    \n",
        "    self.merged_data = merged_data\n",
        "    print(\"Data merge completed\")\n",
        "\n",
        "SalesForecastModelV2.merge_data = merge_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create weekly aggregations and temporal features\n",
        "def create_weekly_aggregations(self):\n",
        "    print(\"\\nCreating weekly aggregations...\")\n",
        "    \n",
        "    dt_info = self.merged_data['transaction_date'].dt\n",
        "    self.merged_data['year'] = dt_info.year\n",
        "    self.merged_data['week'] = dt_info.isocalendar().week\n",
        "    self.merged_data['month'] = dt_info.month\n",
        "    self.merged_data['quarter'] = dt_info.quarter\n",
        "    self.merged_data['year_week'] = self.merged_data['year'].astype(str) + '_' + self.merged_data['week'].astype(str).str.zfill(2)\n",
        "    \n",
        "    weekly_data = self.merged_data.groupby([\n",
        "        'year_week', 'week', 'month', 'quarter', 'internal_store_id', 'internal_product_id',\n",
        "        'categoria', 'marca', 'premise', 'categoria_pdv'\n",
        "    ]).agg({\n",
        "        'quantity': ['sum', 'mean', 'count'],\n",
        "        'gross_value': ['sum', 'mean'],\n",
        "        'net_value': ['sum', 'mean'],\n",
        "        'gross_profit': ['sum', 'mean']\n",
        "    }).reset_index()\n",
        "    \n",
        "    weekly_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in weekly_data.columns.values]\n",
        "    \n",
        "    column_mapping = {\n",
        "        'quantity_sum': 'total_quantity',\n",
        "        'quantity_mean': 'avg_quantity_per_transaction',\n",
        "        'quantity_count': 'num_transactions',\n",
        "        'gross_value_sum': 'total_gross_value',\n",
        "        'gross_value_mean': 'avg_gross_value',\n",
        "        'net_value_sum': 'total_net_value',\n",
        "        'net_value_mean': 'avg_net_value',\n",
        "        'gross_profit_sum': 'total_gross_profit',\n",
        "        'gross_profit_mean': 'avg_gross_profit'\n",
        "    }\n",
        "    weekly_data.rename(columns=column_mapping, inplace=True)\n",
        "    \n",
        "    self.weekly_data = weekly_data\n",
        "    print(f\"Created weekly aggregations: {len(self.weekly_data):,} rows\")\n",
        "    \n",
        "    # Cap weekly quantity outliers per store-product pair\n",
        "    print(\"Capping weekly quantity outliers...\")\n",
        "    self.cap_weekly_outliers()\n",
        "\n",
        "SalesForecastModelV2.create_weekly_aggregations = create_weekly_aggregations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cap extreme weekly quantities using robust per-group statistics\n",
        "def cap_weekly_outliers(self):\n",
        "    # Calculate caps per store-product pair (99.5th percentile)\n",
        "    store_product_caps = self.weekly_data.groupby(['internal_store_id', 'internal_product_id'])['total_quantity'].quantile(0.995).reset_index()\n",
        "    store_product_caps.rename(columns={'total_quantity': 'sp_quantity_cap'}, inplace=True)\n",
        "    \n",
        "    # Calculate global cap as backup (99.8th percentile)\n",
        "    global_cap = self.weekly_data['total_quantity'].quantile(0.998)\n",
        "    \n",
        "    # Merge caps\n",
        "    self.weekly_data = self.weekly_data.merge(\n",
        "        store_product_caps, \n",
        "        on=['internal_store_id', 'internal_product_id'], \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Apply caps (use store-product cap, fallback to global cap)\n",
        "    before_sum = self.weekly_data['total_quantity'].sum()\n",
        "    before_max = self.weekly_data['total_quantity'].max()\n",
        "    \n",
        "    self.weekly_data['sp_quantity_cap'] = self.weekly_data['sp_quantity_cap'].fillna(global_cap)\n",
        "    self.weekly_data['total_quantity'] = np.minimum(\n",
        "        self.weekly_data['total_quantity'], \n",
        "        self.weekly_data['sp_quantity_cap']\n",
        "    )\n",
        "    \n",
        "    after_sum = self.weekly_data['total_quantity'].sum()\n",
        "    after_max = self.weekly_data['total_quantity'].max()\n",
        "    \n",
        "    # Clean up temporary column\n",
        "    self.weekly_data.drop('sp_quantity_cap', axis=1, inplace=True)\n",
        "    \n",
        "    print(f\"  Before capping: max={before_max:,.0f}, total={before_sum:,.0f}\")\n",
        "    print(f\"  After capping:  max={after_max:,.0f}, total={after_sum:,.0f}\")\n",
        "    print(f\"  Reduction: {((before_sum - after_sum)/before_sum)*100:.2f}% of total volume capped\")\n",
        "\n",
        "SalesForecastModelV2.cap_weekly_outliers = cap_weekly_outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build enhanced features including temporal, lag, trend, and interaction features\n",
        "def build_features_v2(self):\n",
        "    print(\"\\nBuilding enhanced features...\")\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.sort_values(['internal_store_id', 'internal_product_id', 'week'])\n",
        "    \n",
        "    print(\"Creating temporal features...\")\n",
        "    self.weekly_data['week_sin'] = np.sin(2 * np.pi * self.weekly_data['week'] / 52)\n",
        "    self.weekly_data['week_cos'] = np.cos(2 * np.pi * self.weekly_data['week'] / 52)\n",
        "    self.weekly_data['month_sin'] = np.sin(2 * np.pi * self.weekly_data['month'] / 12)\n",
        "    self.weekly_data['month_cos'] = np.cos(2 * np.pi * self.weekly_data['month'] / 12)\n",
        "    \n",
        "    print(\"Creating trend features...\")\n",
        "    grouped = self.weekly_data.groupby(['internal_store_id', 'internal_product_id'])\n",
        "    \n",
        "    self.weekly_data['quantity_lag_1'] = grouped['total_quantity'].shift(1)\n",
        "    self.weekly_data['quantity_lag_2'] = grouped['total_quantity'].shift(2)\n",
        "    \n",
        "    for window in [2, 4]:\n",
        "        past_total = grouped['total_quantity'].shift(1)\n",
        "        self.weekly_data[f'quantity_rolling_avg_{window}'] = past_total.rolling(window=window, min_periods=1).mean()\n",
        "    \n",
        "    self.weekly_data['quantity_trend_2w'] = (self.weekly_data['quantity_lag_1'] - self.weekly_data['quantity_lag_2']) / (self.weekly_data['quantity_lag_2'] + 1)\n",
        "    self.weekly_data['quantity_lag_3'] = grouped['total_quantity'].shift(3)\n",
        "    self.weekly_data['quantity_growth_rate'] = (self.weekly_data['quantity_lag_1'] - self.weekly_data['quantity_lag_3']) / (self.weekly_data['quantity_lag_3'] + 1)\n",
        "    \n",
        "    print(\"Creating lifecycle features...\")\n",
        "    first_sale = grouped['week'].transform('min')\n",
        "    last_sale = grouped['week'].transform('max')\n",
        "    self.weekly_data['weeks_since_first_sale'] = self.weekly_data['week'] - first_sale\n",
        "    self.weekly_data['weeks_since_last_sale'] = self.weekly_data['week'] - last_sale\n",
        "    \n",
        "    print(\"Creating store-product interaction features...\")\n",
        "    store_product_stats = self.weekly_data.groupby(['internal_store_id', 'internal_product_id']).agg({\n",
        "        'total_quantity': ['mean', 'std', 'count'],\n",
        "        'num_transactions': 'mean'\n",
        "    })\n",
        "    \n",
        "    store_product_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in store_product_stats.columns.values]\n",
        "    store_product_stats.rename(columns={\n",
        "        'total_quantity_mean': 'store_product_avg_quantity',\n",
        "        'total_quantity_std': 'store_product_std_quantity',\n",
        "        'total_quantity_count': 'store_product_weeks_active',\n",
        "        'num_transactions_mean': 'store_product_avg_transactions'\n",
        "    }, inplace=True)\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.merge(\n",
        "        store_product_stats,\n",
        "        left_on=['internal_store_id', 'internal_product_id'],\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    print(\"Creating category performance features...\")\n",
        "    self.weekly_data = self.weekly_data.sort_values(['categoria', 'week'])\n",
        "    self.weekly_data['category_weekly_avg'] = (\n",
        "        self.weekly_data.groupby('categoria')['total_quantity']\n",
        "        .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.sort_values(['internal_store_id', 'week'])\n",
        "    self.weekly_data['store_weekly_avg'] = (\n",
        "        self.weekly_data.groupby('internal_store_id')['total_quantity']\n",
        "        .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.fillna(0)\n",
        "    print(\"Enhanced feature engineering completed\")\n",
        "\n",
        "SalesForecastModelV2.build_features_v2 = build_features_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with feature selection and target transformation\n",
        "def prepare_training_data_v2(self):\n",
        "    print(\"\\nPreparing training data...\")\n",
        "    \n",
        "    categorical_features = ['categoria', 'marca', 'premise', 'categoria_pdv']\n",
        "    \n",
        "    # Keep original categorical columns for CatBoost native handling\n",
        "    for feature in categorical_features:\n",
        "        self.weekly_data[feature] = self.weekly_data[feature].astype(str)\n",
        "    \n",
        "    feature_columns = [\n",
        "        'week', 'month', 'quarter',\n",
        "        'week_sin', 'week_cos', 'month_sin', 'month_cos',\n",
        "        'num_transactions', 'avg_quantity_per_transaction',\n",
        "        'total_gross_value', 'avg_gross_value',\n",
        "        'total_net_value', 'avg_net_value',\n",
        "        'total_gross_profit', 'avg_gross_profit',\n",
        "        'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3',\n",
        "        'quantity_rolling_avg_2', 'quantity_rolling_avg_4',\n",
        "        'quantity_trend_2w', 'quantity_growth_rate',\n",
        "        'weeks_since_first_sale', 'weeks_since_last_sale',\n",
        "        'store_product_avg_quantity', 'store_product_std_quantity',\n",
        "        'store_product_weeks_active', 'store_product_avg_transactions',\n",
        "        'category_weekly_avg', 'store_weekly_avg'\n",
        "    ] + categorical_features\n",
        "    \n",
        "    self.categorical_features = categorical_features\n",
        "    \n",
        "    train_data = self.weekly_data[self.weekly_data['week'] >= 5].copy()\n",
        "    \n",
        "    X = train_data[feature_columns]\n",
        "    y = train_data['total_quantity']\n",
        "    \n",
        "    if self.target_log_transformed:\n",
        "        y = np.log1p(y)\n",
        "        print(\"Applied log1p transformation to target\")\n",
        "    \n",
        "    print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
        "    \n",
        "    return X, y, feature_columns, train_data\n",
        "\n",
        "SalesForecastModelV2.prepare_training_data_v2 = prepare_training_data_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Weighted Mean Absolute Percentage Error (WMAPE) and create time-series split\n",
        "def calculate_wmape(self, y_true, y_pred):\n",
        "    if self.target_log_transformed:\n",
        "        y_true = np.expm1(y_true)\n",
        "        y_pred = np.expm1(y_pred)\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "    denom = np.sum(y_true)\n",
        "    return 0.0 if denom == 0 else np.sum(np.abs(y_true - y_pred)) / denom * 100\n",
        "\n",
        "def time_series_split(self, X, y, test_weeks=6):\n",
        "    train_data = self.weekly_data[self.weekly_data['week'] >= 5].copy()\n",
        "    max_week = train_data['week'].max()\n",
        "    split_week = max_week - test_weeks + 1\n",
        "    \n",
        "    train_mask = train_data['week'] < split_week\n",
        "    val_mask = train_data['week'] >= split_week\n",
        "    \n",
        "    X_train = X[train_mask]\n",
        "    X_val = X[val_mask]\n",
        "    y_train = y[train_mask]\n",
        "    y_val = y[val_mask]\n",
        "    \n",
        "    print(f\"Time-based split: Train weeks 5-{split_week-1}, Validation weeks {split_week}-{max_week}\")\n",
        "    print(f\"Train samples: {len(X_train):,}, Validation samples: {len(X_val):,}\")\n",
        "    \n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "SalesForecastModelV2.calculate_wmape = calculate_wmape\n",
        "SalesForecastModelV2.time_series_split = time_series_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost model with GPU/multicore acceleration and optimized hyperparameters\n",
        "def train_model_v2(self, X, y, optimize_hyperparams=False, n_trials=150, use_gpu=True):\n",
        "    print(\"\\nTraining enhanced CatBoost model...\")\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = self.time_series_split(X, y)\n",
        "    \n",
        "    # Check GPU availability\n",
        "    gpu_available = False\n",
        "    if use_gpu:\n",
        "        try:\n",
        "            # Try creating a small GPU model to test\n",
        "            test_model = CatBoostRegressor(iterations=1, task_type=\"GPU\", verbose=False)\n",
        "            test_model.fit(X_train.head(100), y_train.head(100), verbose=False)\n",
        "            gpu_available = True\n",
        "            print(\"GPU acceleration available and enabled\")\n",
        "        except Exception as e:\n",
        "            print(f\"GPU not available: {str(e)}\")\n",
        "            gpu_available = False\n",
        "    \n",
        "    # Get optimal thread count for CPU\n",
        "    import os\n",
        "    n_threads = os.cpu_count()\n",
        "    print(f\"ðŸ”§ Using {n_threads} CPU threads\")\n",
        "    \n",
        "    model_params = {\n",
        "        'iterations': 1500,\n",
        "        'learning_rate': 0.03,\n",
        "        'depth': 8,\n",
        "        'l2_leaf_reg': 15,\n",
        "        'bagging_temperature': 0.8,\n",
        "        'random_strength': 1.5,\n",
        "        'border_count': 200,\n",
        "        'loss_function': 'MAE',\n",
        "        'eval_metric': 'MAE',\n",
        "        'random_seed': 42,\n",
        "        'verbose': 100,\n",
        "        'early_stopping_rounds': 200,\n",
        "        'thread_count': n_threads,  # Use all available CPU cores\n",
        "    }\n",
        "    \n",
        "    # Add GPU-specific parameters if available\n",
        "    if gpu_available and use_gpu:\n",
        "        model_params.update({\n",
        "            'task_type': 'GPU',\n",
        "            'devices': '0',  # Use first GPU\n",
        "            'gpu_ram_part': 0.5,  # Use 50% of GPU memory\n",
        "        })\n",
        "        print(\"GPU acceleration enabled\")\n",
        "    else:\n",
        "        print(\"Using CPU training with multicore support\")\n",
        "    \n",
        "    print(f\"Using parameters: {model_params}\")\n",
        "    self.model = CatBoostRegressor(**model_params)\n",
        "    \n",
        "    cat_feature_indices = [X_train.columns.get_loc(col) for col in self.categorical_features if col in X_train.columns]\n",
        "    \n",
        "    self.model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        cat_features=cat_feature_indices,\n",
        "        early_stopping_rounds=150,\n",
        "        verbose=100,\n",
        "        use_best_model=True\n",
        "    )\n",
        "    \n",
        "    self.evaluate_model_v2(X_train, X_val, y_train, y_val)\n",
        "\n",
        "SalesForecastModelV2.train_model_v2 = train_model_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance on training and validation sets\n",
        "def evaluate_model_v2(self, X_train, X_val, y_train, y_val):\n",
        "    train_pred = self.model.predict(X_train)\n",
        "    val_pred = self.model.predict(X_val)\n",
        "    \n",
        "    train_wmape = self.calculate_wmape(y_train, train_pred)\n",
        "    val_wmape = self.calculate_wmape(y_val, val_pred)\n",
        "    \n",
        "    if self.target_log_transformed:\n",
        "        y_train_orig = np.expm1(y_train)\n",
        "        y_val_orig = np.expm1(y_val)\n",
        "        train_pred_orig = np.expm1(train_pred)\n",
        "        val_pred_orig = np.expm1(val_pred)\n",
        "    else:\n",
        "        y_train_orig = y_train\n",
        "        y_val_orig = y_val\n",
        "        train_pred_orig = train_pred\n",
        "        val_pred_orig = val_pred\n",
        "    \n",
        "    train_pred_orig = np.maximum(0, train_pred_orig)\n",
        "    val_pred_orig = np.maximum(0, val_pred_orig)\n",
        "    \n",
        "    train_mape = mean_absolute_percentage_error(y_train_orig, train_pred_orig) * 100\n",
        "    val_mape = mean_absolute_percentage_error(y_val_orig, val_pred_orig) * 100\n",
        "    \n",
        "    self.validation_metrics = {\n",
        "        'train_mape': train_mape,\n",
        "        'val_mape': val_mape,\n",
        "        'train_wmape': train_wmape,\n",
        "        'val_wmape': val_wmape,\n",
        "        'train_samples': len(y_train),\n",
        "        'val_samples': len(y_val)\n",
        "    }\n",
        "    \n",
        "    print(f\"Training MAPE: {train_mape:.2f}%\")\n",
        "    print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
        "    print(f\"Training WMAPE: {train_wmape:.2f}%\")\n",
        "    print(f\"Validation WMAPE: {val_wmape:.2f}%\")\n",
        "    \n",
        "    return self.validation_metrics\n",
        "\n",
        "SalesForecastModelV2.evaluate_model_v2 = evaluate_model_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost model optimized for Apple M4 with maximum CPU performance\n",
        "def train_model_v2(self, X, y, optimize_hyperparams=False, n_trials=150, use_gpu=True):\n",
        "    print(\"\\nTraining enhanced CatBoost model...\")\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = self.time_series_split(X, y)\n",
        "    \n",
        "    # System detection and optimization\n",
        "    import platform\n",
        "    import os\n",
        "    system_info = platform.system()\n",
        "    machine_info = platform.machine()\n",
        "    print(f\"System: {system_info} {machine_info}\")\n",
        "    \n",
        "    # GPU availability check (CatBoost supports NVIDIA CUDA only)\n",
        "    gpu_available = False\n",
        "    if use_gpu:\n",
        "        try:\n",
        "            test_model = CatBoostRegressor(iterations=1, task_type=\"GPU\", verbose=False)\n",
        "            test_model.fit(X_train.head(100), y_train.head(100), verbose=False)\n",
        "            gpu_available = True\n",
        "            print(\"NVIDIA GPU acceleration enabled\")\n",
        "        except Exception:\n",
        "            print(\"CUDA GPU not available - using optimized CPU training\")\n",
        "            print(\"Note: Apple Silicon GPU cores not supported by CatBoost\")\n",
        "            gpu_available = False\n",
        "    \n",
        "    # Thread optimization for Apple Silicon\n",
        "    n_threads = os.cpu_count()\n",
        "    if system_info == \"Darwin\" and (\"arm64\" in machine_info or \"arm\" in machine_info):\n",
        "        print(\"Apple Silicon detected - optimizing thread allocation\")\n",
        "        n_threads = max(1, int(n_threads * 0.9))  # Reserve some cores for system\n",
        "    \n",
        "    print(f\"Using {n_threads} CPU threads\")\n",
        "    \n",
        "    # Optimized model parameters for Apple Silicon\n",
        "    model_params = {\n",
        "        'iterations': 1500,\n",
        "        'learning_rate': 0.03,\n",
        "        'depth': 8,\n",
        "        'l2_leaf_reg': 15,\n",
        "        'bootstrap_type': 'Bayesian',  # Fixed: Compatible with bagging_temperature\n",
        "        'bagging_temperature': 0.8,\n",
        "        'random_strength': 1.5,\n",
        "        'border_count': 200,\n",
        "        'loss_function': 'MAE',\n",
        "        'eval_metric': 'MAE',\n",
        "        'random_seed': 42,\n",
        "        'verbose': 100,\n",
        "        'early_stopping_rounds': 200,\n",
        "        'thread_count': n_threads,\n",
        "        'used_ram_limit': '12GB',  # Increased for M4 systems\n",
        "    }\n",
        "    \n",
        "    # GPU parameters if available\n",
        "    if gpu_available and use_gpu:\n",
        "        model_params.update({\n",
        "            'task_type': 'GPU',\n",
        "            'devices': '0',\n",
        "            'gpu_ram_part': 0.5,\n",
        "        })\n",
        "        print(\"GPU acceleration enabled\")\n",
        "    else:\n",
        "        print(\"CPU training optimized for Apple Silicon\")\n",
        "    \n",
        "    print(f\"Model parameters: {model_params}\")\n",
        "    self.model = CatBoostRegressor(**model_params)\n",
        "    \n",
        "    cat_feature_indices = [X_train.columns.get_loc(col) for col in self.categorical_features if col in X_train.columns]\n",
        "    \n",
        "    print(\"Starting model training...\")\n",
        "    self.model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        cat_features=cat_feature_indices,\n",
        "        early_stopping_rounds=150,\n",
        "        verbose=100,\n",
        "        use_best_model=True\n",
        "    )\n",
        "    \n",
        "    self.evaluate_model_v2(X_train, X_val, y_train, y_val)\n",
        "\n",
        "SalesForecastModelV2.train_model_v2 = train_model_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze prediction distribution, save predictions, and print performance report\n",
        "def analyze_predictions(self, predictions_df):\n",
        "    print(\"\\nPrediction Analysis:\")\n",
        "    print(f\"Total predictions: {len(predictions_df):,}\")\n",
        "    print(f\"Zero predictions: {(predictions_df['quantidade'] == 0).sum():,}\")\n",
        "    print(f\"Non-zero predictions: {(predictions_df['quantidade'] > 0).sum():,}\")\n",
        "    print(f\"Mean prediction: {predictions_df['quantidade'].mean():.2f}\")\n",
        "    print(f\"Median prediction: {predictions_df['quantidade'].median():.2f}\")\n",
        "    print(f\"Max prediction: {predictions_df['quantidade'].max():,}\")\n",
        "    print(f\"Std prediction: {predictions_df['quantidade'].std():.2f}\")\n",
        "    \n",
        "    quantiles = predictions_df['quantidade'].quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
        "    print(\"Prediction quantiles:\")\n",
        "    for q, val in quantiles.items():\n",
        "        print(f\"  {q*100:.0f}%: {val:.2f}\")\n",
        "    \n",
        "    weekly_stats = predictions_df.groupby('semana')['quantidade'].agg(['count', 'mean', 'sum']).round(2)\n",
        "    print(\"\\nWeekly prediction summary:\")\n",
        "    print(weekly_stats)\n",
        "\n",
        "def save_predictions(self, predictions_df, filename=\"sales_predictions_v2.csv\"):\n",
        "    print(f\"\\nSaving predictions to {filename}...\")\n",
        "    predictions_df.to_csv(filename, sep=';', index=False, encoding='utf-8')\n",
        "    \n",
        "    parquet_filename = filename.replace('.csv', '.parquet')\n",
        "    predictions_df.to_parquet(parquet_filename, index=False)\n",
        "    \n",
        "    print(f\"Predictions saved successfully!\")\n",
        "    print(f\"CSV File: {filename}\")\n",
        "    print(f\"Parquet File: {parquet_filename}\")\n",
        "    print(f\"Rows: {len(predictions_df):,}\")\n",
        "    print(f\"Sample:\")\n",
        "    print(predictions_df.head(10))\n",
        "\n",
        "def print_performance_report(self):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMANCE REPORT V2\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    if self.validation_metrics:\n",
        "        print(\"Model Validation Metrics:\")\n",
        "        print(f\"  Training MAPE: {self.validation_metrics['train_mape']:.2f}%\")\n",
        "        print(f\"  Validation MAPE: {self.validation_metrics['val_mape']:.2f}%\")\n",
        "        print(f\"  Training WMAPE: {self.validation_metrics['train_wmape']:.2f}%\")\n",
        "        print(f\"  Validation WMAPE: {self.validation_metrics['val_wmape']:.2f}%\")\n",
        "        print(f\"  Training samples: {self.validation_metrics['train_samples']:,}\")\n",
        "        print(f\"  Validation samples: {self.validation_metrics['val_samples']:,}\")\n",
        "        \n",
        "        wmape_diff = abs(self.validation_metrics['val_wmape'] - self.validation_metrics['train_wmape'])\n",
        "        mape_diff = abs(self.validation_metrics['val_mape'] - self.validation_metrics['train_mape'])\n",
        "        \n",
        "        print(f\"\\nOverfitting Check:\")\n",
        "        print(f\"  MAPE difference: {mape_diff:.2f}%\")\n",
        "        print(f\"  WMAPE difference: {wmape_diff:.2f}%\")\n",
        "        \n",
        "        if wmape_diff < 2 and mape_diff < 10:\n",
        "            print(\"  Status: Good generalization\")\n",
        "        elif wmape_diff < 5 and mape_diff < 20:\n",
        "            print(\"  Status: Moderate overfitting\")\n",
        "        else:\n",
        "            print(\"  Status: High overfitting risk\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "\n",
        "SalesForecastModelV2.analyze_predictions = analyze_predictions\n",
        "SalesForecastModelV2.save_predictions = save_predictions\n",
        "SalesForecastModelV2.print_performance_report = print_performance_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for the next 5 weeks with enhanced feature handling\n",
        "def generate_predictions_v2(self, feature_columns, train_data, max_rows=1500000, weeks_to_predict=5, recent_weeks=8):\n",
        "    print(\"\\nGenerating predictions for January 2023...\")\n",
        "    \n",
        "    # Discover unique store-product combinations\n",
        "    store_product_combinations = train_data[['internal_store_id', 'internal_product_id']].drop_duplicates()\n",
        "    print(f\"Found {len(store_product_combinations):,} unique store-product combinations\")\n",
        "    \n",
        "    # Build latest snapshot per pair with all features\n",
        "    latest_records = train_data.loc[train_data.groupby(['internal_store_id', 'internal_product_id'])['week'].idxmax()].copy()\n",
        "    \n",
        "    # Select top active pairs to respect the 1.5M-row portal limit\n",
        "    try:\n",
        "        pairs_limit = max_rows // weeks_to_predict\n",
        "        max_week = int(self.weekly_data['week'].max())\n",
        "        start_week = max(1, max_week - int(recent_weeks) + 1)\n",
        "        recent_slice = self.weekly_data[self.weekly_data['week'] >= start_week]\n",
        "        activity = recent_slice.groupby(['internal_store_id', 'internal_product_id']).agg(\n",
        "            recent_total_qty=('total_quantity', 'sum'),\n",
        "            weeks_with_sales=('total_quantity', lambda s: int((s > 0).sum())),\n",
        "            last_week_seen=('week', 'max')\n",
        "        ).reset_index()\n",
        "        activity = activity.sort_values(\n",
        "            by=['recent_total_qty', 'weeks_with_sales', 'last_week_seen'],\n",
        "            ascending=[False, False, False]\n",
        "        )\n",
        "        selected_pairs = activity.head(pairs_limit)[['internal_store_id', 'internal_product_id']]\n",
        "        before = len(latest_records)\n",
        "        latest_records = latest_records.merge(selected_pairs, on=['internal_store_id', 'internal_product_id'], how='inner')\n",
        "        after = len(latest_records)\n",
        "        print(f\"Selected top {after:,} active pairs out of {before:,} (recent_weeks={recent_weeks})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Pair selection step skipped due to error: {e}\")\n",
        "    \n",
        "    all_predictions = []\n",
        "    \n",
        "    for week in range(1, weeks_to_predict + 1):\n",
        "        print(f\"Predicting week {week}...\")\n",
        "        \n",
        "        week_data = latest_records.copy()\n",
        "        \n",
        "        # Update temporal features for the prediction week\n",
        "        week_data['week'] = week\n",
        "        week_data['month'] = 1  # January\n",
        "        week_data['quarter'] = 1  # Q1\n",
        "        \n",
        "        # Update seasonal features\n",
        "        week_data['week_sin'] = np.sin(2 * np.pi * week / 52)\n",
        "        week_data['week_cos'] = np.cos(2 * np.pi * week / 52)\n",
        "        week_data['month_sin'] = np.sin(2 * np.pi * 1 / 12)  # January\n",
        "        week_data['month_cos'] = np.cos(2 * np.pi * 1 / 12)  # January\n",
        "        \n",
        "        # Update lifecycle features\n",
        "        week_data['weeks_since_last_sale'] = 0  # Assume continuing sales\n",
        "        \n",
        "        # Ensure all required features are present\n",
        "        missing_features = [f for f in feature_columns if f not in week_data.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features for prediction: {missing_features}\")\n",
        "            # Fill missing features with zeros or appropriate defaults\n",
        "            for feature in missing_features:\n",
        "                week_data[feature] = 0\n",
        "        \n",
        "        # Prepare feature matrix\n",
        "        try:\n",
        "            features_matrix = week_data[feature_columns]\n",
        "            \n",
        "            # Make predictions\n",
        "            predictions = self.model.predict(features_matrix)\n",
        "            \n",
        "            # Handle log transformation if used\n",
        "            if self.target_log_transformed:\n",
        "                predictions = np.expm1(predictions)\n",
        "            \n",
        "            # Ensure non-negative predictions\n",
        "            predictions = np.maximum(0, predictions)\n",
        "            \n",
        "            # Create prediction dataframe\n",
        "            week_predictions = pd.DataFrame({\n",
        "                'semana': week,\n",
        "                'pdv': week_data['internal_store_id'].astype(int),\n",
        "                'produto': week_data['internal_product_id'].astype(int),\n",
        "                'quantidade': predictions.round().astype(int)\n",
        "            })\n",
        "            \n",
        "            all_predictions.append(week_predictions)\n",
        "            print(f\"Generated {len(week_predictions):,} predictions for week {week}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error generating predictions for week {week}: {e}\")\n",
        "            # Create fallback predictions with zeros\n",
        "            week_predictions = pd.DataFrame({\n",
        "                'semana': week,\n",
        "                'pdv': week_data['internal_store_id'].astype(int),\n",
        "                'produto': week_data['internal_product_id'].astype(int),\n",
        "                'quantidade': 0\n",
        "            })\n",
        "            all_predictions.append(week_predictions)\n",
        "    \n",
        "    predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
        "    \n",
        "    # Final safeguard\n",
        "    if len(predictions_df) > max_rows:\n",
        "        print(f\"Limiting predictions to {max_rows:,} rows (was {len(predictions_df):,})\")\n",
        "        predictions_df = predictions_df.head(max_rows)\n",
        "    \n",
        "    print(f\"Total predictions generated: {len(predictions_df):,}\")\n",
        "    self.analyze_predictions(predictions_df)\n",
        "    return predictions_df\n",
        "\n",
        "SalesForecastModelV2.generate_predictions_v2 = generate_predictions_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the complete pipeline: data loading, processing, training, and prediction\n",
        "def run_complete_pipeline(self):\n",
        "    print(\"=== Sales Forecast Model V2 - Enhanced ===\")\n",
        "    \n",
        "    self.load_data()\n",
        "    self.cleanse_data()\n",
        "    self.merge_data()\n",
        "    self.create_weekly_aggregations()\n",
        "    self.build_features_v2()\n",
        "    \n",
        "    X, y, feature_columns, train_data = self.prepare_training_data_v2()\n",
        "    self.train_model_v2(X, y, optimize_hyperparams=False, n_trials=150)\n",
        "    \n",
        "    predictions_df = self.generate_predictions_v2(feature_columns, train_data)\n",
        "    self.save_predictions(predictions_df)\n",
        "    self.print_performance_report()\n",
        "    \n",
        "    return predictions_df\n",
        "\n",
        "SalesForecastModelV2.run_complete_pipeline = run_complete_pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sales Forecast Model V2 - Enhanced ===\n",
            "Loading data...\n",
            "Found 3 parquet files\n",
            "part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet: Shape (6560698, 11)\n",
            "-> Identified as TRANSACTIONS data\n",
            "part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet: Shape (7092, 8)\n",
            "-> Identified as PRODUCTS data\n",
            "part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet: Shape (14419, 4)\n",
            "-> Identified as STORES data\n",
            "Data loaded successfully:\n",
            "- Transactions: 6,560,698 rows\n",
            "- Products: 7,092 rows\n",
            "- Stores: 14,419 rows\n",
            "\n",
            "Cleansing data...\n",
            "Removed 0 rows with null values\n",
            "Kept 6,430,161 rows with positive quantities\n",
            "Removing extreme value outliers...\n",
            "Removed 94,398 outlier transactions (1.47%)\n",
            "Filtered to 2022 data: 6,335,763 rows\n",
            "Cleaning products and stores...\n",
            "\n",
            "Merging data...\n",
            "After product merge: 6,335,763 rows\n",
            "After store merge: 6,335,763 rows\n",
            "Data merge completed\n",
            "\n",
            "Creating weekly aggregations...\n",
            "Created weekly aggregations: 6,067,972 rows\n",
            "Capping weekly quantity outliers...\n",
            "  Before capping: max=360, total=31,721,805\n",
            "  After capping:  max=355, total=31,656,366\n",
            "  Reduction: 0.21% of total volume capped\n",
            "\n",
            "Building enhanced features...\n",
            "Creating temporal features...\n",
            "Creating trend features...\n",
            "Creating lifecycle features...\n",
            "Creating store-product interaction features...\n",
            "Creating category performance features...\n",
            "Enhanced feature engineering completed\n",
            "\n",
            "Preparing training data...\n",
            "Applied log1p transformation to target\n",
            "Training data shape: X=(5735913, 34), y=(5735913,)\n",
            "\n",
            "Training enhanced CatBoost model...\n",
            "Time-based split: Train weeks 5-46, Validation weeks 47-52\n",
            "Train samples: 5,065,293, Validation samples: 670,620\n",
            "System: Darwin arm64\n",
            "CUDA GPU not available - using optimized CPU training\n",
            "Note: Apple Silicon GPU cores not supported by CatBoost\n",
            "Apple Silicon detected - optimizing thread allocation\n",
            "Using 12 CPU threads\n",
            "CPU training optimized for Apple Silicon\n",
            "Model parameters: {'iterations': 1500, 'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 15, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.8, 'random_strength': 1.5, 'border_count': 200, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': 42, 'verbose': 100, 'early_stopping_rounds': 200, 'thread_count': 12, 'used_ram_limit': '12GB'}\n",
            "Starting model training...\n",
            "0:\tlearn: 0.5467547\ttest: 0.5375071\tbest: 0.5375071 (0)\ttotal: 898ms\tremaining: 22m 26s\n",
            "100:\tlearn: 0.0488974\ttest: 0.0440138\tbest: 0.0440138 (100)\ttotal: 1m 27s\tremaining: 20m 5s\n",
            "200:\tlearn: 0.0152278\ttest: 0.0145785\tbest: 0.0145785 (200)\ttotal: 2m 46s\tremaining: 17m 56s\n",
            "300:\tlearn: 0.0102263\ttest: 0.0101216\tbest: 0.0101216 (300)\ttotal: 3m 56s\tremaining: 15m 41s\n",
            "400:\tlearn: 0.0083957\ttest: 0.0085491\tbest: 0.0085491 (400)\ttotal: 5m 3s\tremaining: 13m 52s\n",
            "500:\tlearn: 0.0064862\ttest: 0.0068516\tbest: 0.0068516 (500)\ttotal: 6m 17s\tremaining: 12m 33s\n",
            "600:\tlearn: 0.0053854\ttest: 0.0058834\tbest: 0.0058834 (600)\ttotal: 7m 32s\tremaining: 11m 16s\n",
            "700:\tlearn: 0.0047492\ttest: 0.0052348\tbest: 0.0052348 (700)\ttotal: 8m 48s\tremaining: 10m 2s\n",
            "800:\tlearn: 0.0042891\ttest: 0.0047220\tbest: 0.0047220 (800)\ttotal: 10m 2s\tremaining: 8m 45s\n",
            "900:\tlearn: 0.0039492\ttest: 0.0043460\tbest: 0.0043460 (900)\ttotal: 11m 14s\tremaining: 7m 28s\n",
            "1000:\tlearn: 0.0036971\ttest: 0.0041133\tbest: 0.0041133 (1000)\ttotal: 12m 28s\tremaining: 6m 13s\n",
            "1100:\tlearn: 0.0034931\ttest: 0.0038956\tbest: 0.0038956 (1100)\ttotal: 13m 42s\tremaining: 4m 58s\n",
            "1200:\tlearn: 0.0033234\ttest: 0.0037270\tbest: 0.0037270 (1200)\ttotal: 14m 57s\tremaining: 3m 43s\n",
            "1300:\tlearn: 0.0031845\ttest: 0.0035843\tbest: 0.0035843 (1300)\ttotal: 16m 10s\tremaining: 2m 28s\n",
            "1400:\tlearn: 0.0030813\ttest: 0.0034822\tbest: 0.0034822 (1400)\ttotal: 17m 21s\tremaining: 1m 13s\n",
            "1499:\tlearn: 0.0029615\ttest: 0.0033572\tbest: 0.0033572 (1499)\ttotal: 18m 33s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.003357198766\n",
            "bestIteration = 1499\n",
            "\n",
            "Training MAPE: 55.73%\n",
            "Validation MAPE: 0.40%\n",
            "Training WMAPE: 0.98%\n",
            "Validation WMAPE: 1.11%\n",
            "\n",
            "Generating predictions for January 2023...\n",
            "Found 958,984 unique store-product combinations\n",
            "Selected top 300,000 active pairs out of 958,984 (recent_weeks=8)\n",
            "Predicting week 1...\n",
            "Generated 300,000 predictions for week 1\n",
            "Predicting week 2...\n",
            "Generated 300,000 predictions for week 2\n",
            "Predicting week 3...\n",
            "Generated 300,000 predictions for week 3\n",
            "Predicting week 4...\n",
            "Generated 300,000 predictions for week 4\n",
            "Predicting week 5...\n",
            "Generated 300,000 predictions for week 5\n",
            "Total predictions generated: 1,500,000\n",
            "\n",
            "Prediction Analysis:\n",
            "Total predictions: 1,500,000\n",
            "Zero predictions: 35\n",
            "Non-zero predictions: 1,499,965\n",
            "Mean prediction: 7.09\n",
            "Median prediction: 3.00\n",
            "Max prediction: 273\n",
            "Std prediction: 14.02\n",
            "Prediction quantiles:\n",
            "  25%: 1.00\n",
            "  50%: 3.00\n",
            "  75%: 7.00\n",
            "  90%: 13.00\n",
            "  95%: 24.00\n",
            "  99%: 72.00\n",
            "\n",
            "Weekly prediction summary:\n",
            "         count  mean      sum\n",
            "semana                       \n",
            "1       300000  7.10  2128821\n",
            "2       300000  7.11  2133236\n",
            "3       300000  7.08  2123604\n",
            "4       300000  7.08  2123524\n",
            "5       300000  7.08  2124843\n",
            "\n",
            "Saving predictions to sales_predictions_v2.csv...\n",
            "Predictions saved successfully!\n",
            "CSV File: sales_predictions_v2.csv\n",
            "Parquet File: sales_predictions_v2.parquet\n",
            "Rows: 1,500,000\n",
            "Sample:\n",
            "   semana                  pdv              produto  quantidade\n",
            "0       1  1000237487041964405   777251454728290683           4\n",
            "1       1  1001371918471115422  1009179103632945474           2\n",
            "2       1  1001371918471115422  1029370090212151375           4\n",
            "3       1  1001371918471115422  1120490062981954254          10\n",
            "4       1  1001371918471115422  1371936917923350372          10\n",
            "5       1  1001371918471115422  1394381856358939027          13\n",
            "6       1  1001371918471115422  1454838625590783593           5\n",
            "7       1  1001371918471115422   145852603040678098           1\n",
            "8       1  1001371918471115422  1527082310248040324           1\n",
            "9       1  1001371918471115422  1625722803643187564           1\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE REPORT V2\n",
            "==================================================\n",
            "Model Validation Metrics:\n",
            "  Training MAPE: 55.73%\n",
            "  Validation MAPE: 0.40%\n",
            "  Training WMAPE: 0.98%\n",
            "  Validation WMAPE: 1.11%\n",
            "  Training samples: 5,065,293\n",
            "  Validation samples: 670,620\n",
            "\n",
            "Overfitting Check:\n",
            "  MAPE difference: 55.34%\n",
            "  WMAPE difference: 0.12%\n",
            "  Status: High overfitting risk\n",
            "==================================================\n",
            "\n",
            "=== V2 Pipeline completed successfully! ===\n",
            "Generated 1,500,000 predictions for 5 weeks of January 2023\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the complete sales forecasting pipeline\n",
        "try:\n",
        "    model = SalesForecastModelV2()\n",
        "    predictions = model.run_complete_pipeline()\n",
        "    \n",
        "    print(\"\\n=== V2 Pipeline completed successfully! ===\")\n",
        "    print(f\"Generated {len(predictions):,} predictions for 5 weeks of January 2023\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
