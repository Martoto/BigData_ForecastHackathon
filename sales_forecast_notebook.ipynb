{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfb7fe2",
   "metadata": {},
   "source": [
    "# Sales Forecast Model - Big Data Hackathon 2025\n",
    "\n",
    "This notebook implements a comprehensive sales forecasting model using CatBoost regression to predict retail sales for the next 5 weeks. The model processes transaction data, performs feature engineering, and generates accurate predictions for store-product combinations.\n",
    "\n",
    "## Overview\n",
    "- **Data Sources**: Transactions, Products, and Stores data in Parquet format\n",
    "- **Model**: CatBoost Regressor with optimized hyperparameters\n",
    "- **Features**: Lag variables, rolling averages, seasonal components, and interaction features\n",
    "- **Output**: Weekly sales predictions for January 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c0e2c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data processing, machine learning, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2ea88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: graphviz in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (3.10.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (2.2.6)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (6.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\110110110\\source\\tests\\venv\\lib\\site-packages (from plotly->catboost) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a41c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19ab76",
   "metadata": {},
   "source": [
    "## 2. Define SalesForecastModel Class\n",
    "\n",
    "The core class that handles all aspects of the sales forecasting pipeline including data loading, preprocessing, feature engineering, model training, and prediction generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "260dc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalesForecastModel class defined successfully!\n",
      "✅ Updated to use only 11 optimized features:\n",
      "   1. avg_quantity_per_transaction\n",
      "   2. store_product_max_quantity\n",
      "   3. categoria_pdv_encoded\n",
      "   4. marca_encoded\n",
      "   5. store_product_min_quantity\n",
      "   6. quantity_rolling_avg_2\n",
      "   7. quantity_lag_1\n",
      "   8. store_product_avg_transactions\n",
      "   9. store_product_std_quantity\n",
      "  10. week_cos\n",
      "  11. total_gross_value\n"
     ]
    }
   ],
   "source": [
    "class SalesForecastModel:\n",
    "    def __init__(self, data_path=\"data/\"):\n",
    "        self.data_path = data_path\n",
    "        self.transactions = None\n",
    "        self.products = None\n",
    "        self.stores = None\n",
    "        self.model = None\n",
    "        self.label_encoders = {}\n",
    "        self.validation_metrics = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        parquet_files = [f for f in os.listdir(self.data_path) if f.endswith('.parquet')]\n",
    "        print(f\"Found {len(parquet_files)} parquet files\")\n",
    "        \n",
    "        for file in parquet_files:\n",
    "            df = pd.read_parquet(os.path.join(self.data_path, file))\n",
    "            print(f\"{file}: Shape {df.shape}\")\n",
    "            \n",
    "            if 'internal_store_id' in df.columns and 'quantity' in df.columns:\n",
    "                self.transactions = df\n",
    "                print(\"-> Identified as TRANSACTIONS data\")\n",
    "            elif 'produto' in df.columns and 'categoria' in df.columns:\n",
    "                self.products = df\n",
    "                print(\"-> Identified as PRODUCTS data\")\n",
    "            elif 'pdv' in df.columns and 'premise' in df.columns:\n",
    "                self.stores = df\n",
    "                print(\"-> Identified as STORES data\")\n",
    "        \n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"- Transactions: {self.transactions.shape[0]:,} rows\")\n",
    "        print(f\"- Products: {self.products.shape[0]:,} rows\")\n",
    "        print(f\"- Stores: {self.stores.shape[0]:,} rows\")\n",
    "        \n",
    "    def cleanse_data(self):\n",
    "        print(\"\\nCleansing data...\")\n",
    "        \n",
    "        initial_rows = len(self.transactions)\n",
    "        self.transactions = self.transactions.dropna(subset=['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date'])\n",
    "        print(f\"Removed {initial_rows - len(self.transactions):,} rows with null values\")\n",
    "        \n",
    "        self.transactions = self.transactions[self.transactions['quantity'] > 0]\n",
    "        print(f\"Kept {len(self.transactions):,} rows with positive quantities\")\n",
    "        \n",
    "        self.transactions[['transaction_date', 'reference_date']] = self.transactions[['transaction_date', 'reference_date']].apply(pd.to_datetime)\n",
    "        \n",
    "        self.transactions = self.transactions[\n",
    "            (self.transactions['transaction_date'].dt.year == 2022)\n",
    "        ]\n",
    "        print(f\"Filtered to 2022 data: {len(self.transactions):,} rows\")\n",
    "        \n",
    "        print(\"Cleaning products and stores...\")\n",
    "        self.products['descricao'] = self.products['descricao'].fillna('Unknown')\n",
    "        self.products['categoria'] = self.products['categoria'].fillna('Other')\n",
    "        self.products['marca'] = self.products['marca'].fillna('Unknown')\n",
    "        \n",
    "        self.stores['categoria_pdv'] = self.stores['categoria_pdv'].fillna('Other')\n",
    "        self.stores['premise'] = self.stores['premise'].fillna('Unknown')\n",
    "        \n",
    "    def merge_data(self):\n",
    "        print(\"\\nMerging data...\")\n",
    "        \n",
    "        merged_data = self.transactions.merge(\n",
    "            self.products, \n",
    "            left_on='internal_product_id', \n",
    "            right_on='produto', \n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"After product merge: {len(merged_data):,} rows\")\n",
    "        \n",
    "        merged_data = merged_data.merge(\n",
    "            self.stores,\n",
    "            left_on='internal_store_id',\n",
    "            right_on='pdv',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"After store merge: {len(merged_data):,} rows\")\n",
    "        \n",
    "        self.merged_data = merged_data\n",
    "        print(\"Data merge completed\")\n",
    "        \n",
    "    def create_weekly_aggregations(self):\n",
    "        print(\"\\nCreating weekly aggregations...\")\n",
    "        \n",
    "        dt_info = self.merged_data['transaction_date'].dt\n",
    "        self.merged_data['year'] = dt_info.year\n",
    "        self.merged_data['week'] = dt_info.isocalendar().week\n",
    "        self.merged_data['year_week'] = self.merged_data['year'].astype(str) + '_' + self.merged_data['week'].astype(str).str.zfill(2)\n",
    "        \n",
    "        weekly_data = self.merged_data.groupby([\n",
    "            'year_week', 'week', 'internal_store_id', 'internal_product_id',\n",
    "            'categoria', 'marca', 'premise', 'categoria_pdv'\n",
    "        ]).agg({\n",
    "            'quantity': ['sum', 'mean', 'count'],\n",
    "            'gross_value': ['sum', 'mean'],\n",
    "            'net_value': ['sum', 'mean'],\n",
    "            'gross_profit': ['sum', 'mean']\n",
    "        }).reset_index()\n",
    "        \n",
    "        weekly_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in weekly_data.columns.values]\n",
    "        \n",
    "        column_mapping = {\n",
    "            'quantity_sum': 'total_quantity',\n",
    "            'quantity_mean': 'avg_quantity_per_transaction',\n",
    "            'quantity_count': 'num_transactions',\n",
    "            'gross_value_sum': 'total_gross_value',\n",
    "            'gross_value_mean': 'avg_gross_value',\n",
    "            'net_value_sum': 'total_net_value',\n",
    "            'net_value_mean': 'avg_net_value',\n",
    "            'gross_profit_sum': 'total_gross_profit',\n",
    "            'gross_profit_mean': 'avg_gross_profit'\n",
    "        }\n",
    "        weekly_data.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        self.weekly_data = weekly_data\n",
    "        print(f\"Created weekly aggregations: {len(self.weekly_data):,} rows\")\n",
    "        \n",
    "    def build_features(self):\n",
    "        print(\"\\nBuilding optimized features (11 selected features)...\")\n",
    "        \n",
    "        self.weekly_data = self.weekly_data.sort_values(['internal_store_id', 'internal_product_id', 'week'])\n",
    "        \n",
    "        # Create only quantity_lag_1 (not all lags)\n",
    "        print(\"Creating lag feature (lag_1 only)...\")\n",
    "        grouped = self.weekly_data.groupby(['internal_store_id', 'internal_product_id'])['total_quantity']\n",
    "        self.weekly_data['quantity_lag_1'] = grouped.shift(1)\n",
    "        \n",
    "        # Create only quantity_rolling_avg_2 (not all windows)\n",
    "        print(\"Creating rolling average (window=2 only)...\")\n",
    "        self.weekly_data['quantity_rolling_avg_2'] = grouped.rolling(window=2, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "        \n",
    "        # Create only week_cos (not week_sin)\n",
    "        print(\"Creating seasonal feature (cos only)...\")\n",
    "        self.weekly_data['week_cos'] = np.cos(2 * np.pi * self.weekly_data['week'] / 52)\n",
    "        \n",
    "        # Create store-product interaction features (all needed) - only if not already exist\n",
    "        store_product_cols = ['store_product_std_quantity', 'store_product_min_quantity', \n",
    "                             'store_product_max_quantity', 'store_product_avg_transactions']\n",
    "        \n",
    "        if not any(col in self.weekly_data.columns for col in store_product_cols):\n",
    "            print(\"Creating store-product interaction features...\")\n",
    "            agg_dict = {\n",
    "                'total_quantity': ['std', 'min', 'max'],\n",
    "                'num_transactions': 'mean'\n",
    "            }\n",
    "            store_product_stats = self.weekly_data.groupby(['internal_store_id', 'internal_product_id']).agg(agg_dict)\n",
    "            \n",
    "            store_product_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in store_product_stats.columns.values]\n",
    "            store_product_stats.rename(columns={\n",
    "                'total_quantity_std': 'store_product_std_quantity',\n",
    "                'total_quantity_min': 'store_product_min_quantity',\n",
    "                'total_quantity_max': 'store_product_max_quantity',\n",
    "                'num_transactions_mean': 'store_product_avg_transactions'\n",
    "            }, inplace=True)\n",
    "            \n",
    "            self.weekly_data = self.weekly_data.merge(\n",
    "                store_product_stats,\n",
    "                left_on=['internal_store_id', 'internal_product_id'],\n",
    "                right_index=True,\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            print(\"Store-product features already exist, skipping...\")\n",
    "        \n",
    "        self.weekly_data = self.weekly_data.fillna(0)\n",
    "        print(\"Optimized feature engineering completed\")\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        print(\"\\nPreparing training data with 11 selected features...\")\n",
    "        \n",
    "        # Create only needed categorical encodings\n",
    "        categorical_features = ['categoria_pdv', 'marca']  # Only these 2\n",
    "        \n",
    "        for feature in categorical_features:\n",
    "            if feature not in self.label_encoders:\n",
    "                self.label_encoders[feature] = LabelEncoder()\n",
    "                self.weekly_data[f'{feature}_encoded'] = self.label_encoders[feature].fit_transform(\n",
    "                    self.weekly_data[feature].astype(str)\n",
    "                )\n",
    "        \n",
    "        # Define exactly the 11 features requested\n",
    "        feature_columns = [\n",
    "            'avg_quantity_per_transaction',\n",
    "            'store_product_max_quantity', \n",
    "            'categoria_pdv_encoded',\n",
    "            'marca_encoded',\n",
    "            'store_product_min_quantity',\n",
    "            'quantity_rolling_avg_2',\n",
    "            'quantity_lag_1',\n",
    "            'store_product_avg_transactions',\n",
    "            'store_product_std_quantity',\n",
    "            'week_cos',\n",
    "            'total_gross_value'\n",
    "        ]\n",
    "        \n",
    "        train_data = self.weekly_data[self.weekly_data['week'] >= 5].copy()\n",
    "        \n",
    "        # Verify all features exist\n",
    "        missing_features = [f for f in feature_columns if f not in train_data.columns]\n",
    "        if missing_features:\n",
    "            print(f\"WARNING: Missing features: {missing_features}\")\n",
    "        \n",
    "        X = train_data[feature_columns]\n",
    "        y = train_data['total_quantity']\n",
    "        \n",
    "        print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "        print(f\"Selected features ({len(feature_columns)}):\")\n",
    "        for i, feature in enumerate(feature_columns, 1):\n",
    "            print(f\"  {i:2d}. {feature}\")\n",
    "        \n",
    "        return X, y, feature_columns, train_data\n",
    "    \n",
    "    def calculate_wmape(self, y_true, y_pred):\n",
    "        return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100\n",
    "    \n",
    "    def evaluate_model(self, X, y, train_data):\n",
    "        # Use temporal split: train on earlier weeks, validate on later weeks\n",
    "        max_week = train_data['week'].max()\n",
    "        # Use last 20% of weeks for validation (approximately)\n",
    "        week_cutoff = max_week - int(0.2 * (max_week - train_data['week'].min()))\n",
    "        \n",
    "        train_mask = train_data['week'] <= week_cutoff\n",
    "        val_mask = train_data['week'] > week_cutoff\n",
    "        \n",
    "        X_train, X_val = X[train_mask], X[val_mask]\n",
    "        y_train, y_val = y[train_mask], y[val_mask]\n",
    "        \n",
    "        print(f\"Temporal split: Train weeks ≤ {week_cutoff}, Validation weeks > {week_cutoff}\")\n",
    "        print(f\"Train weeks range: {train_data[train_mask]['week'].min()}-{train_data[train_mask]['week'].max()}\")\n",
    "        print(f\"Validation weeks range: {train_data[val_mask]['week'].min()}-{train_data[val_mask]['week'].max()}\")\n",
    "        \n",
    "        train_pred = self.model.predict(X_train)\n",
    "        val_pred = self.model.predict(X_val)\n",
    "        \n",
    "        train_mape = mean_absolute_percentage_error(y_train, train_pred) * 100\n",
    "        val_mape = mean_absolute_percentage_error(y_val, val_pred) * 100\n",
    "        \n",
    "        train_wmape = self.calculate_wmape(y_train, train_pred)\n",
    "        val_wmape = self.calculate_wmape(y_val, val_pred)\n",
    "        \n",
    "        self.validation_metrics = {\n",
    "            'train_mape': train_mape,\n",
    "            'val_mape': val_mape,\n",
    "            'train_wmape': train_wmape,\n",
    "            'val_wmape': val_wmape,\n",
    "            'train_samples': len(y_train),\n",
    "            'val_samples': len(y_val)\n",
    "        }\n",
    "        \n",
    "        print(f\"Training MAPE: {train_mape:.2f}%\")\n",
    "        print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
    "        print(f\"Training WMAPE: {train_wmape:.2f}%\")\n",
    "        print(f\"Validation WMAPE: {val_wmape:.2f}%\")\n",
    "        \n",
    "        return self.validation_metrics\n",
    "        \n",
    "    def train_model(self, X, y, train_data):\n",
    "        print(\"\\nTraining CatBoost model...\")\n",
    "        \n",
    "        # Use temporal split: train on earlier weeks, validate on later weeks\n",
    "        max_week = train_data['week'].max()\n",
    "        # Use last 20% of weeks for validation (approximately)\n",
    "        week_cutoff = max_week - int(0.2 * (max_week - train_data['week'].min()))\n",
    "        \n",
    "        train_mask = train_data['week'] <= week_cutoff\n",
    "        val_mask = train_data['week'] > week_cutoff\n",
    "        \n",
    "        X_train, X_val = X[train_mask], X[val_mask]\n",
    "        y_train, y_val = y[train_mask], y[val_mask]\n",
    "        \n",
    "        print(f\"Temporal split for training: Train weeks ≤ {week_cutoff}, Validation weeks > {week_cutoff}\")\n",
    "        print(f\"Training samples: {len(y_train):,}, Validation samples: {len(y_val):,}\")\n",
    "        \n",
    "        self.model = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='RMSE',\n",
    "            random_seed=42,\n",
    "            verbose=100\n",
    "        )\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100\n",
    "        )\n",
    "        \n",
    "        self.evaluate_model(X, y, train_data)\n",
    "        \n",
    "    def generate_predictions(self, feature_columns, train_data, max_rows=1500000, weeks_to_predict=5, recent_weeks=8):\n",
    "        print(\"\\nGenerating predictions for January 2023...\")\n",
    "        \n",
    "        # Discover unique pairs\n",
    "        store_product_combinations = train_data[['internal_store_id', 'internal_product_id']].drop_duplicates()\n",
    "        print(f\"Found {len(store_product_combinations):,} unique store-product combinations\")\n",
    "        \n",
    "        # Build latest snapshot per pair\n",
    "        latest_records = train_data.loc[train_data.groupby(['internal_store_id', 'internal_product_id'])['week'].idxmax()].copy()\n",
    "        \n",
    "        # Select top active pairs to respect the 1.5M-row portal limit\n",
    "        try:\n",
    "            pairs_limit = max_rows // weeks_to_predict\n",
    "            max_week = int(self.weekly_data['week'].max())\n",
    "            start_week = max(1, max_week - int(recent_weeks) + 1)\n",
    "            recent_slice = self.weekly_data[self.weekly_data['week'] >= start_week]\n",
    "            activity = recent_slice.groupby(['internal_store_id', 'internal_product_id']).agg(\n",
    "                recent_total_qty=('total_quantity', 'sum'),\n",
    "                weeks_with_sales=('total_quantity', lambda s: int((s > 0).sum())),\n",
    "                last_week_seen=('week', 'max')\n",
    "            ).reset_index()\n",
    "            activity = activity.sort_values(\n",
    "                by=['recent_total_qty', 'weeks_with_sales', 'last_week_seen'],\n",
    "                ascending=[False, False, False]\n",
    "            )\n",
    "            selected_pairs = activity.head(pairs_limit)[['internal_store_id', 'internal_product_id']]\n",
    "            before = len(latest_records)\n",
    "            latest_records = latest_records.merge(selected_pairs, on=['internal_store_id', 'internal_product_id'], how='inner')\n",
    "            after = len(latest_records)\n",
    "            print(f\"Selected top {after:,} active pairs out of {before:,} (recent_weeks={recent_weeks})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pair selection step skipped due to error: {e}\")\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        for week in range(1, weeks_to_predict + 1):\n",
    "            print(f\"Predicting week {week}...\")\n",
    "            \n",
    "            week_data = latest_records.copy()\n",
    "            week_data['week'] = week\n",
    "            week_data['week_cos'] = np.cos(2 * np.pi * week / 52)\n",
    "            \n",
    "            features_matrix = week_data[feature_columns].values\n",
    "            predictions = self.model.predict(features_matrix)\n",
    "            predictions = np.maximum(0, predictions)\n",
    "            \n",
    "            week_predictions = pd.DataFrame({\n",
    "                'semana': week,\n",
    "                'pdv': week_data['internal_store_id'].astype(int),\n",
    "                'produto': week_data['internal_product_id'].astype(int),\n",
    "                'quantidade': predictions.round().astype(int)\n",
    "            })\n",
    "            \n",
    "            all_predictions.append(week_predictions)\n",
    "            print(f\"Generated {len(week_predictions):,} predictions for week {week}\")\n",
    "        \n",
    "        predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "        \n",
    "        # Final safeguard\n",
    "        if len(predictions_df) > max_rows:\n",
    "            print(f\"Limiting predictions to {max_rows:,} rows (was {len(predictions_df):,})\")\n",
    "            predictions_df = predictions_df.head(max_rows)\n",
    "        \n",
    "        print(f\"Total predictions generated: {len(predictions_df):,}\")\n",
    "        self.analyze_predictions(predictions_df)\n",
    "        return predictions_df\n",
    "    \n",
    "    def analyze_predictions(self, predictions_df):\n",
    "        print(\"\\nPrediction Analysis:\")\n",
    "        print(f\"Total predictions: {len(predictions_df):,}\")\n",
    "        print(f\"Zero predictions: {(predictions_df['quantidade'] == 0).sum():,}\")\n",
    "        print(f\"Non-zero predictions: {(predictions_df['quantidade'] > 0).sum():,}\")\n",
    "        print(f\"Mean prediction: {predictions_df['quantidade'].mean():.2f}\")\n",
    "        print(f\"Median prediction: {predictions_df['quantidade'].median():.2f}\")\n",
    "        print(f\"Max prediction: {predictions_df['quantidade'].max():,}\")\n",
    "        print(f\"Std prediction: {predictions_df['quantidade'].std():.2f}\")\n",
    "        \n",
    "        quantiles = predictions_df['quantidade'].quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "        print(\"Prediction quantiles:\")\n",
    "        for q, val in quantiles.items():\n",
    "            print(f\"  {q*100:.0f}%: {val:.2f}\")\n",
    "        \n",
    "        weekly_stats = predictions_df.groupby('semana')['quantidade'].agg(['count', 'mean', 'sum']).round(2)\n",
    "        print(\"\\nWeekly prediction summary:\")\n",
    "        print(weekly_stats)\n",
    "        \n",
    "    def save_predictions(self, predictions_df, filename=\"sales_predictions.csv\"):\n",
    "        print(f\"\\nSaving predictions to {filename}...\")\n",
    "        \n",
    "        predictions_df.to_csv(filename, sep=';', index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"Predictions saved successfully!\")\n",
    "        print(f\"File: {filename}\")\n",
    "        print(f\"Rows: {len(predictions_df):,}\")\n",
    "        print(f\"Sample:\")\n",
    "        print(predictions_df.head(10))\n",
    "        \n",
    "    def print_performance_report(self):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PERFORMANCE REPORT\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.validation_metrics:\n",
    "            print(\"Model Validation Metrics:\")\n",
    "            print(f\"  Training MAPE: {self.validation_metrics['train_mape']:.2f}%\")\n",
    "            print(f\"  Validation MAPE: {self.validation_metrics['val_mape']:.2f}%\")\n",
    "            print(f\"  Training WMAPE: {self.validation_metrics['train_wmape']:.2f}%\")\n",
    "            print(f\"  Validation WMAPE: {self.validation_metrics['val_wmape']:.2f}%\")\n",
    "            print(f\"  Training samples: {self.validation_metrics['train_samples']:,}\")\n",
    "            print(f\"  Validation samples: {self.validation_metrics['val_samples']:,}\")\n",
    "            \n",
    "            wmape_diff = abs(self.validation_metrics['val_wmape'] - self.validation_metrics['train_wmape'])\n",
    "            mape_diff = abs(self.validation_metrics['val_mape'] - self.validation_metrics['train_mape'])\n",
    "            \n",
    "            print(f\"\\nOverfitting Check:\")\n",
    "            print(f\"  MAPE difference: {mape_diff:.2f}%\")\n",
    "            print(f\"  WMAPE difference: {wmape_diff:.2f}%\")\n",
    "            \n",
    "            if wmape_diff < 5 and mape_diff < 5:\n",
    "                print(\"  Status: Good generalization\")\n",
    "            elif wmape_diff < 10 and mape_diff < 10:\n",
    "                print(\"  Status: Moderate overfitting\")\n",
    "            else:\n",
    "                print(\"  Status: High overfitting risk\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "\n",
    "print(\"SalesForecastModel class defined successfully!\")\n",
    "print(\"✅ Updated to use only 11 optimized features:\")\n",
    "print(\"   1. avg_quantity_per_transaction\")\n",
    "print(\"   2. store_product_max_quantity\") \n",
    "print(\"   3. categoria_pdv_encoded\")\n",
    "print(\"   4. marca_encoded\")\n",
    "print(\"   5. store_product_min_quantity\")\n",
    "print(\"   6. quantity_rolling_avg_2\")\n",
    "print(\"   7. quantity_lag_1\")\n",
    "print(\"   8. store_product_avg_transactions\")\n",
    "print(\"   9. store_product_std_quantity\")\n",
    "print(\"  10. week_cos\")\n",
    "print(\"  11. total_gross_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a3068",
   "metadata": {},
   "source": [
    "## 3. Initialize Model and Load Data\n",
    "\n",
    "Create an instance of the SalesForecastModel and load the data from parquet files. The model automatically identifies transaction, product, and store data based on column patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "180538bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 3 parquet files\n",
      "part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet: Shape (14419, 4)\n",
      "-> Identified as STORES data\n",
      "part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet: Shape (6560698, 11)\n",
      "-> Identified as TRANSACTIONS data\n",
      "part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet: Shape (7092, 8)\n",
      "-> Identified as PRODUCTS data\n",
      "Data loaded successfully:\n",
      "- Transactions: 6,560,698 rows\n",
      "- Products: 7,092 rows\n",
      "- Stores: 14,419 rows\n",
      "\n",
      "=== Data Summary ===\n",
      "Transactions dataset columns: ['internal_store_id', 'internal_product_id', 'distributor_id', 'transaction_date', 'reference_date', 'quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes']\n",
      "part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet: Shape (6560698, 11)\n",
      "-> Identified as TRANSACTIONS data\n",
      "part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet: Shape (7092, 8)\n",
      "-> Identified as PRODUCTS data\n",
      "Data loaded successfully:\n",
      "- Transactions: 6,560,698 rows\n",
      "- Products: 7,092 rows\n",
      "- Stores: 14,419 rows\n",
      "\n",
      "=== Data Summary ===\n",
      "Transactions dataset columns: ['internal_store_id', 'internal_product_id', 'distributor_id', 'transaction_date', 'reference_date', 'quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes']\n",
      "Transaction date range: 2022-01-01 to 2022-12-31\n",
      "Products dataset columns: ['produto', 'categoria', 'descricao', 'tipos', 'label', 'subcategoria', 'marca', 'fabricante']\n",
      "Unique categories: 7\n",
      "Stores dataset columns: ['pdv', 'premise', 'categoria_pdv', 'zipcode']\n",
      "Unique store categories: 54\n",
      "Transaction date range: 2022-01-01 to 2022-12-31\n",
      "Products dataset columns: ['produto', 'categoria', 'descricao', 'tipos', 'label', 'subcategoria', 'marca', 'fabricante']\n",
      "Unique categories: 7\n",
      "Stores dataset columns: ['pdv', 'premise', 'categoria_pdv', 'zipcode']\n",
      "Unique store categories: 54\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the data path (fresh instance)\n",
    "model = SalesForecastModel(data_path=\"data/\")\n",
    "\n",
    "# Load the data\n",
    "model.load_data()\n",
    "\n",
    "# Display basic information about the loaded datasets\n",
    "print(\"\\n=== Data Summary ===\")\n",
    "if model.transactions is not None:\n",
    "    print(f\"Transactions dataset columns: {list(model.transactions.columns)}\")\n",
    "    print(f\"Transaction date range: {model.transactions['transaction_date'].min()} to {model.transactions['transaction_date'].max()}\")\n",
    "    \n",
    "if model.products is not None:\n",
    "    print(f\"Products dataset columns: {list(model.products.columns)}\")\n",
    "    print(f\"Unique categories: {model.products['categoria'].nunique()}\")\n",
    "    \n",
    "if model.stores is not None:\n",
    "    print(f\"Stores dataset columns: {list(model.stores.columns)}\")\n",
    "    print(f\"Unique store categories: {model.stores['categoria_pdv'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3095b06",
   "metadata": {},
   "source": [
    "## 4. Data Cleansing and Preprocessing\n",
    "\n",
    "Clean the data by removing null values, filtering for positive quantities, converting date columns, and handling missing values in categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28765020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleansing data...\n",
      "Removed 0 rows with null values\n",
      "Removed 0 rows with null values\n",
      "Kept 6,430,161 rows with positive quantities\n",
      "Kept 6,430,161 rows with positive quantities\n",
      "Filtered to 2022 data: 6,430,161 rows\n",
      "Cleaning products and stores...\n",
      "\n",
      "=== Data Quality Check ===\n",
      "Final transaction data shape: (6430161, 11)\n",
      "Date range after filtering: 2022-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "Positive quantities: 6,430,161\n",
      "Zero quantities: 0\n",
      "\n",
      "=== Missing Values Check ===\n",
      "Transactions missing values:\n",
      "Filtered to 2022 data: 6,430,161 rows\n",
      "Cleaning products and stores...\n",
      "\n",
      "=== Data Quality Check ===\n",
      "Final transaction data shape: (6430161, 11)\n",
      "Date range after filtering: 2022-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "Positive quantities: 6,430,161\n",
      "Zero quantities: 0\n",
      "\n",
      "=== Missing Values Check ===\n",
      "Transactions missing values:\n",
      "internal_store_id      0\n",
      "internal_product_id    0\n",
      "quantity               0\n",
      "transaction_date       0\n",
      "dtype: int64\n",
      "\n",
      "Products missing values:\n",
      "produto      0\n",
      "categoria    0\n",
      "marca        0\n",
      "descricao    0\n",
      "dtype: int64\n",
      "\n",
      "Stores missing values:\n",
      "pdv              0\n",
      "categoria_pdv    0\n",
      "premise          0\n",
      "dtype: int64\n",
      "internal_store_id      0\n",
      "internal_product_id    0\n",
      "quantity               0\n",
      "transaction_date       0\n",
      "dtype: int64\n",
      "\n",
      "Products missing values:\n",
      "produto      0\n",
      "categoria    0\n",
      "marca        0\n",
      "descricao    0\n",
      "dtype: int64\n",
      "\n",
      "Stores missing values:\n",
      "pdv              0\n",
      "categoria_pdv    0\n",
      "premise          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleanse the data\n",
    "model.cleanse_data()\n",
    "\n",
    "# Display data quality information after cleansing\n",
    "print(\"\\n=== Data Quality Check ===\")\n",
    "print(f\"Final transaction data shape: {model.transactions.shape}\")\n",
    "print(f\"Date range after filtering: {model.transactions['transaction_date'].min()} to {model.transactions['transaction_date'].max()}\")\n",
    "print(f\"Positive quantities: {(model.transactions['quantity'] > 0).sum():,}\")\n",
    "print(f\"Zero quantities: {(model.transactions['quantity'] == 0).sum():,}\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "print(\"\\n=== Missing Values Check ===\")\n",
    "print(\"Transactions missing values:\")\n",
    "print(model.transactions[['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date']].isnull().sum())\n",
    "\n",
    "print(\"\\nProducts missing values:\")\n",
    "print(model.products[['produto', 'categoria', 'marca', 'descricao']].isnull().sum())\n",
    "\n",
    "print(\"\\nStores missing values:\")\n",
    "print(model.stores[['pdv', 'categoria_pdv', 'premise']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e466a8",
   "metadata": {},
   "source": [
    "## 5. Data Merging and Weekly Aggregations\n",
    "\n",
    "Merge transaction data with product and store information, then create weekly aggregations of sales metrics including quantity, revenue, and profit measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fb5c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging data...\n",
      "After product merge: 6,430,161 rows\n",
      "After product merge: 6,430,161 rows\n",
      "After store merge: 6,430,161 rows\n",
      "Data merge completed\n",
      "\n",
      "Creating weekly aggregations...\n",
      "After store merge: 6,430,161 rows\n",
      "Data merge completed\n",
      "\n",
      "Creating weekly aggregations...\n",
      "Created weekly aggregations: 6,133,925 rows\n",
      "\n",
      "=== Weekly Aggregation Summary ===\n",
      "Weekly data shape: (6133925, 17)\n",
      "Columns in weekly data: ['year_week', 'week', 'internal_store_id', 'internal_product_id', 'categoria', 'marca', 'premise', 'categoria_pdv', 'total_quantity', 'avg_quantity_per_transaction', 'num_transactions', 'total_gross_value', 'avg_gross_value', 'total_net_value', 'avg_net_value', 'total_gross_profit', 'avg_gross_profit']\n",
      "Week range: 1 to 52\n",
      "Created weekly aggregations: 6,133,925 rows\n",
      "\n",
      "=== Weekly Aggregation Summary ===\n",
      "Weekly data shape: (6133925, 17)\n",
      "Columns in weekly data: ['year_week', 'week', 'internal_store_id', 'internal_product_id', 'categoria', 'marca', 'premise', 'categoria_pdv', 'total_quantity', 'avg_quantity_per_transaction', 'num_transactions', 'total_gross_value', 'avg_gross_value', 'total_net_value', 'avg_net_value', 'total_gross_profit', 'avg_gross_profit']\n",
      "Week range: 1 to 52\n",
      "Unique store-product combinations: 1,016,656\n",
      "\n",
      "=== Sample Weekly Data ===\n",
      "Unique store-product combinations: 1,016,656\n",
      "\n",
      "=== Sample Weekly Data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_week</th>\n",
       "      <th>week</th>\n",
       "      <th>internal_store_id</th>\n",
       "      <th>internal_product_id</th>\n",
       "      <th>categoria</th>\n",
       "      <th>marca</th>\n",
       "      <th>premise</th>\n",
       "      <th>categoria_pdv</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>avg_quantity_per_transaction</th>\n",
       "      <th>num_transactions</th>\n",
       "      <th>total_gross_value</th>\n",
       "      <th>avg_gross_value</th>\n",
       "      <th>total_net_value</th>\n",
       "      <th>avg_net_value</th>\n",
       "      <th>total_gross_profit</th>\n",
       "      <th>avg_gross_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1029370090212151375</td>\n",
       "      <td>Package</td>\n",
       "      <td>Michelob Ultra</td>\n",
       "      <td>Off Premise</td>\n",
       "      <td>Convenience</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>73.410728</td>\n",
       "      <td>73.410728</td>\n",
       "      <td>23.136728</td>\n",
       "      <td>23.136728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1120490062981954254</td>\n",
       "      <td>Package</td>\n",
       "      <td>Bud Light</td>\n",
       "      <td>Off Premise</td>\n",
       "      <td>Convenience</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>262.350006</td>\n",
       "      <td>262.350006</td>\n",
       "      <td>234.992905</td>\n",
       "      <td>234.992905</td>\n",
       "      <td>62.210903</td>\n",
       "      <td>62.210903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>2239307647969388381</td>\n",
       "      <td>Package</td>\n",
       "      <td>Natural Light</td>\n",
       "      <td>Off Premise</td>\n",
       "      <td>Convenience</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.950001</td>\n",
       "      <td>37.950001</td>\n",
       "      <td>31.870647</td>\n",
       "      <td>31.870647</td>\n",
       "      <td>8.182648</td>\n",
       "      <td>8.182648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>4353552881410365573</td>\n",
       "      <td>Package</td>\n",
       "      <td>Natural Light</td>\n",
       "      <td>Off Premise</td>\n",
       "      <td>Convenience</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>138.250000</td>\n",
       "      <td>138.250000</td>\n",
       "      <td>118.302116</td>\n",
       "      <td>118.302116</td>\n",
       "      <td>33.567116</td>\n",
       "      <td>33.567116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>4797439216678436447</td>\n",
       "      <td>Package</td>\n",
       "      <td>Bud Light Lime</td>\n",
       "      <td>Off Premise</td>\n",
       "      <td>Convenience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>24.470242</td>\n",
       "      <td>24.470242</td>\n",
       "      <td>7.524241</td>\n",
       "      <td>7.524241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year_week  week    internal_store_id  internal_product_id categoria  \\\n",
       "0   2022_01     1  1001371918471115422  1029370090212151375   Package   \n",
       "1   2022_01     1  1001371918471115422  1120490062981954254   Package   \n",
       "2   2022_01     1  1001371918471115422  2239307647969388381   Package   \n",
       "3   2022_01     1  1001371918471115422  4353552881410365573   Package   \n",
       "4   2022_01     1  1001371918471115422  4797439216678436447   Package   \n",
       "\n",
       "            marca      premise categoria_pdv  total_quantity  \\\n",
       "0  Michelob Ultra  Off Premise   Convenience             3.0   \n",
       "1       Bud Light  Off Premise   Convenience            18.0   \n",
       "2   Natural Light  Off Premise   Convenience             2.0   \n",
       "3   Natural Light  Off Premise   Convenience             7.0   \n",
       "4  Bud Light Lime  Off Premise   Convenience             1.0   \n",
       "\n",
       "   avg_quantity_per_transaction  num_transactions  total_gross_value  \\\n",
       "0                           3.0                 1          80.250000   \n",
       "1                          18.0                 1         262.350006   \n",
       "2                           2.0                 1          37.950001   \n",
       "3                           7.0                 1         138.250000   \n",
       "4                           1.0                 1          26.750000   \n",
       "\n",
       "   avg_gross_value  total_net_value  avg_net_value  total_gross_profit  \\\n",
       "0        80.250000        73.410728      73.410728           23.136728   \n",
       "1       262.350006       234.992905     234.992905           62.210903   \n",
       "2        37.950001        31.870647      31.870647            8.182648   \n",
       "3       138.250000       118.302116     118.302116           33.567116   \n",
       "4        26.750000        24.470242      24.470242            7.524241   \n",
       "\n",
       "   avg_gross_profit  \n",
       "0         23.136728  \n",
       "1         62.210903  \n",
       "2          8.182648  \n",
       "3         33.567116  \n",
       "4          7.524241  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weekly Quantity Statistics ===\n",
      "count    6.133925e+06\n",
      "mean     8.690512e+00\n",
      "std      8.317377e+01\n",
      "min      1.192093e-07\n",
      "25%      1.000000e+00\n",
      "50%      2.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      9.423000e+04\n",
      "Name: total_quantity, dtype: float64\n",
      "count    6.133925e+06\n",
      "mean     8.690512e+00\n",
      "std      8.317377e+01\n",
      "min      1.192093e-07\n",
      "25%      1.000000e+00\n",
      "50%      2.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      9.423000e+04\n",
      "Name: total_quantity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Merge transaction data with product and store information\n",
    "model.merge_data()\n",
    "\n",
    "# Create weekly aggregations\n",
    "model.create_weekly_aggregations()\n",
    "\n",
    "# Display aggregated data information\n",
    "print(\"\\n=== Weekly Aggregation Summary ===\")\n",
    "print(f\"Weekly data shape: {model.weekly_data.shape}\")\n",
    "print(f\"Columns in weekly data: {list(model.weekly_data.columns)}\")\n",
    "print(f\"Week range: {model.weekly_data['week'].min()} to {model.weekly_data['week'].max()}\")\n",
    "print(f\"Unique store-product combinations: {model.weekly_data[['internal_store_id', 'internal_product_id']].drop_duplicates().shape[0]:,}\")\n",
    "\n",
    "# Show sample of weekly aggregated data\n",
    "print(\"\\n=== Sample Weekly Data ===\")\n",
    "display(model.weekly_data.head())\n",
    "\n",
    "# Statistics on weekly quantities\n",
    "print(\"\\n=== Weekly Quantity Statistics ===\")\n",
    "print(model.weekly_data['total_quantity'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9e048",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Build features including lag variables, rolling averages, seasonal components, and store-product interaction features for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "863ad0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building optimized features (11 selected features)...\n",
      "Creating lag feature (lag_1 only)...\n",
      "Creating lag feature (lag_1 only)...\n",
      "Creating rolling average (window=2 only)...\n",
      "Creating rolling average (window=2 only)...\n",
      "Creating seasonal feature (cos only)...\n",
      "Creating store-product interaction features...\n",
      "Creating seasonal feature (cos only)...\n",
      "Creating store-product interaction features...\n",
      "Optimized feature engineering completed\n",
      "Optimized feature engineering completed\n",
      "\n",
      "=== Feature Engineering Summary ===\n",
      "Created 7 engineered features:\n",
      "  - quantity_lag_1\n",
      "  - quantity_rolling_avg_2\n",
      "  - week_cos\n",
      "  - store_product_std_quantity\n",
      "  - store_product_min_quantity\n",
      "  - store_product_max_quantity\n",
      "  - store_product_avg_transactions\n",
      "\n",
      "=== Sample Data with Features ===\n",
      "\n",
      "=== Feature Engineering Summary ===\n",
      "Created 7 engineered features:\n",
      "  - quantity_lag_1\n",
      "  - quantity_rolling_avg_2\n",
      "  - week_cos\n",
      "  - store_product_std_quantity\n",
      "  - store_product_min_quantity\n",
      "  - store_product_max_quantity\n",
      "  - store_product_avg_transactions\n",
      "\n",
      "=== Sample Data with Features ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>internal_store_id</th>\n",
       "      <th>internal_product_id</th>\n",
       "      <th>week</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>quantity_lag_1</th>\n",
       "      <th>quantity_rolling_avg_2</th>\n",
       "      <th>week_cos</th>\n",
       "      <th>store_product_max_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433888</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529002</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042472</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.822984</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521528</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891177</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599754</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.568065</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475319</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314756</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>1837429607327399565</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565795</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>4038588102284338370</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.464723</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433889</th>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>5429216175252037173</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           internal_store_id  internal_product_id  week  total_quantity  \\\n",
       "433888   1000237487041964405  1837429607327399565     6             1.0   \n",
       "529002   1000237487041964405  1837429607327399565     7             2.0   \n",
       "2042472  1000237487041964405  1837429607327399565    21             1.0   \n",
       "2521528  1000237487041964405  1837429607327399565    25             2.0   \n",
       "2891177  1000237487041964405  1837429607327399565    28             2.0   \n",
       "3599754  1000237487041964405  1837429607327399565    34             2.0   \n",
       "4475319  1000237487041964405  1837429607327399565    39             1.0   \n",
       "5314756  1000237487041964405  1837429607327399565    46             2.0   \n",
       "1565795  1000237487041964405  4038588102284338370    17             1.0   \n",
       "433889   1000237487041964405  5429216175252037173     6             2.0   \n",
       "\n",
       "         quantity_lag_1  quantity_rolling_avg_2  week_cos  \\\n",
       "433888              0.0                     1.0  0.748511   \n",
       "529002              1.0                     1.5  0.663123   \n",
       "2042472             2.0                     1.5 -0.822984   \n",
       "2521528             1.0                     1.5 -0.992709   \n",
       "2891177             2.0                     2.0 -0.970942   \n",
       "3599754             2.0                     2.0 -0.568065   \n",
       "4475319             2.0                     1.5      -0.0   \n",
       "5314756             1.0                     1.5  0.748511   \n",
       "1565795             0.0                     1.0 -0.464723   \n",
       "433889              0.0                     2.0  0.748511   \n",
       "\n",
       "         store_product_max_quantity  \n",
       "433888                          2.0  \n",
       "529002                          2.0  \n",
       "2042472                         2.0  \n",
       "2521528                         2.0  \n",
       "2891177                         2.0  \n",
       "3599754                         2.0  \n",
       "4475319                         2.0  \n",
       "5314756                         2.0  \n",
       "1565795                         1.0  \n",
       "433889                          2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Values After Feature Engineering ===\n",
      "Total missing values: 0\n",
      "Columns with missing values: {}\n",
      "Total missing values: 0\n",
      "Columns with missing values: {}\n"
     ]
    }
   ],
   "source": [
    "# Build features for the model\n",
    "model.build_features()\n",
    "\n",
    "# Display feature engineering results\n",
    "print(\"\\n=== Feature Engineering Summary ===\")\n",
    "feature_columns = [col for col in model.weekly_data.columns if any(x in col for x in ['lag', 'rolling', 'cos', 'store_product'])]\n",
    "print(f\"Created {len(feature_columns)} engineered features:\")\n",
    "for feature in feature_columns:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Show sample of data with new features (updated to use actual columns)\n",
    "print(\"\\n=== Sample Data with Features ===\")\n",
    "sample_cols = ['internal_store_id', 'internal_product_id', 'week', 'total_quantity', \n",
    "               'quantity_lag_1', 'quantity_rolling_avg_2', 'week_cos', 'store_product_max_quantity']\n",
    "display(model.weekly_data[sample_cols].head(10))\n",
    "\n",
    "# Check for missing values after feature engineering\n",
    "print(\"\\n=== Missing Values After Feature Engineering ===\")\n",
    "missing_counts = model.weekly_data.isnull().sum()\n",
    "print(f\"Total missing values: {missing_counts.sum()}\")\n",
    "print(f\"Columns with missing values: {missing_counts[missing_counts > 0].to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b35d99",
   "metadata": {},
   "source": [
    "## 7. Model Training with CatBoost\n",
    "\n",
    "Prepare training data with label encoding for categorical features and train the CatBoost regression model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f11bb268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training data with 11 selected features...\n",
      "Training data shape: X=(5800450, 11), y=(5800450,)\n",
      "Selected features (11):\n",
      "   1. avg_quantity_per_transaction\n",
      "   2. store_product_max_quantity\n",
      "   3. categoria_pdv_encoded\n",
      "   4. marca_encoded\n",
      "   5. store_product_min_quantity\n",
      "   6. quantity_rolling_avg_2\n",
      "   7. quantity_lag_1\n",
      "   8. store_product_avg_transactions\n",
      "   9. store_product_std_quantity\n",
      "  10. week_cos\n",
      "  11. total_gross_value\n",
      "\n",
      "=== Training Data Preparation ===\n",
      "Feature columns (11):\n",
      "   1. avg_quantity_per_transaction\n",
      "   2. store_product_max_quantity\n",
      "   3. categoria_pdv_encoded\n",
      "   4. marca_encoded\n",
      "   5. store_product_min_quantity\n",
      "   6. quantity_rolling_avg_2\n",
      "   7. quantity_lag_1\n",
      "   8. store_product_avg_transactions\n",
      "   9. store_product_std_quantity\n",
      "  10. week_cos\n",
      "  11. total_gross_value\n",
      "\n",
      "Training data statistics:\n",
      "- Features (X): (5800450, 11)\n",
      "- Target (y): (5800450,)\n",
      "- Target range: 0.00 to 94230.00\n",
      "- Target mean: 8.97\n",
      "- Target median: 2.00\n",
      "\n",
      "==================================================\n",
      "TRAINING CATBOOST MODEL\n",
      "==================================================\n",
      "\n",
      "Training CatBoost model...\n",
      "Training data shape: X=(5800450, 11), y=(5800450,)\n",
      "Selected features (11):\n",
      "   1. avg_quantity_per_transaction\n",
      "   2. store_product_max_quantity\n",
      "   3. categoria_pdv_encoded\n",
      "   4. marca_encoded\n",
      "   5. store_product_min_quantity\n",
      "   6. quantity_rolling_avg_2\n",
      "   7. quantity_lag_1\n",
      "   8. store_product_avg_transactions\n",
      "   9. store_product_std_quantity\n",
      "  10. week_cos\n",
      "  11. total_gross_value\n",
      "\n",
      "=== Training Data Preparation ===\n",
      "Feature columns (11):\n",
      "   1. avg_quantity_per_transaction\n",
      "   2. store_product_max_quantity\n",
      "   3. categoria_pdv_encoded\n",
      "   4. marca_encoded\n",
      "   5. store_product_min_quantity\n",
      "   6. quantity_rolling_avg_2\n",
      "   7. quantity_lag_1\n",
      "   8. store_product_avg_transactions\n",
      "   9. store_product_std_quantity\n",
      "  10. week_cos\n",
      "  11. total_gross_value\n",
      "\n",
      "Training data statistics:\n",
      "- Features (X): (5800450, 11)\n",
      "- Target (y): (5800450,)\n",
      "- Target range: 0.00 to 94230.00\n",
      "- Target mean: 8.97\n",
      "- Target median: 2.00\n",
      "\n",
      "==================================================\n",
      "TRAINING CATBOOST MODEL\n",
      "==================================================\n",
      "\n",
      "Training CatBoost model...\n",
      "Temporal split for training: Train weeks ≤ 43, Validation weeks > 43\n",
      "Training samples: 4,732,158, Validation samples: 1,068,292\n",
      "Temporal split for training: Train weeks ≤ 43, Validation weeks > 43\n",
      "Training samples: 4,732,158, Validation samples: 1,068,292\n",
      "0:\tlearn: 88.4567007\ttest: 14.4638799\tbest: 14.4638799 (0)\ttotal: 234ms\tremaining: 3m 53s\n",
      "0:\tlearn: 88.4567007\ttest: 14.4638799\tbest: 14.4638799 (0)\ttotal: 234ms\tremaining: 3m 53s\n",
      "100:\tlearn: 25.6125025\ttest: 14.5202484\tbest: 4.0559866 (49)\ttotal: 20.3s\tremaining: 3m\n",
      "100:\tlearn: 25.6125025\ttest: 14.5202484\tbest: 4.0559866 (49)\ttotal: 20.3s\tremaining: 3m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4.055986562\n",
      "bestIteration = 49\n",
      "\n",
      "Shrink model to first 50 iterations.\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 4.055986562\n",
      "bestIteration = 49\n",
      "\n",
      "Shrink model to first 50 iterations.\n",
      "Temporal split: Train weeks ≤ 43, Validation weeks > 43\n",
      "Temporal split: Train weeks ≤ 43, Validation weeks > 43\n",
      "Train weeks range: 5-43\n",
      "Train weeks range: 5-43\n",
      "Validation weeks range: 44-52\n",
      "Validation weeks range: 44-52\n",
      "Training MAPE: 8096.17%\n",
      "Validation MAPE: 74.60%\n",
      "Training WMAPE: 17.98%\n",
      "Validation WMAPE: 30.83%\n",
      "Training MAPE: 8096.17%\n",
      "Validation MAPE: 74.60%\n",
      "Training WMAPE: 17.98%\n",
      "Validation WMAPE: 30.83%\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "X, y, feature_columns, train_data = model.prepare_training_data()\n",
    "\n",
    "print(\"\\n=== Training Data Preparation ===\")\n",
    "print(f\"Feature columns ({len(feature_columns)}):\")\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(f\"  {i+1:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nTraining data statistics:\")\n",
    "print(f\"- Features (X): {X.shape}\")\n",
    "print(f\"- Target (y): {y.shape}\")\n",
    "print(f\"- Target range: {y.min():.2f} to {y.max():.2f}\")\n",
    "print(f\"- Target mean: {y.mean():.2f}\")\n",
    "print(f\"- Target median: {y.median():.2f}\")\n",
    "\n",
    "# Train the CatBoost model with temporal split\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING CATBOOST MODEL\")\n",
    "print(\"=\"*50)\n",
    "model.train_model(X, y, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3c2e6",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Validation\n",
    "\n",
    "Evaluate the trained model using MAPE and WMAPE metrics on both training and validation sets to assess performance and check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8895fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL EVALUATION RESULTS ===\n",
      "Training MAPE: 8096.167%\n",
      "Validation MAPE: 74.603%\n",
      "Training WMAPE: 17.984%\n",
      "Validation WMAPE: 30.828%\n",
      "\n",
      "Data Split:\n",
      "Training samples: 4,732,158\n",
      "Validation samples: 1,068,292\n",
      "\n",
      "Overfitting Analysis:\n",
      "MAPE difference (val - train): 8021.565%\n",
      "WMAPE difference (val - train): 12.844%\n",
      "Model Status: ❌ High overfitting risk\n",
      "\n",
      "=== TOP 10 FEATURE IMPORTANCES ===\n",
      " 1. avg_quantity_per_transaction   55.0207\n",
      " 2. store_product_max_quantity     17.7013\n",
      " 3. categoria_pdv_encoded          10.8308\n",
      " 4. marca_encoded                  7.5732\n",
      " 5. store_product_min_quantity     4.5030\n",
      " 6. quantity_rolling_avg_2         1.4686\n",
      " 7. store_product_avg_transactions 1.1425\n",
      " 8. quantity_lag_1                 1.0988\n",
      " 9. store_product_std_quantity     0.6612\n",
      "10. week_cos                       0.0000\n"
     ]
    }
   ],
   "source": [
    "# Display detailed validation metrics\n",
    "print(\"\\n=== MODEL EVALUATION RESULTS ===\")\n",
    "if model.validation_metrics:\n",
    "    metrics = model.validation_metrics\n",
    "    \n",
    "    print(f\"Training MAPE: {metrics['train_mape']:.3f}%\")\n",
    "    print(f\"Validation MAPE: {metrics['val_mape']:.3f}%\")\n",
    "    print(f\"Training WMAPE: {metrics['train_wmape']:.3f}%\")\n",
    "    print(f\"Validation WMAPE: {metrics['val_wmape']:.3f}%\")\n",
    "    \n",
    "    print(f\"\\nData Split:\")\n",
    "    print(f\"Training samples: {metrics['train_samples']:,}\")\n",
    "    print(f\"Validation samples: {metrics['val_samples']:,}\")\n",
    "    \n",
    "    # Calculate overfitting indicators\n",
    "    mape_diff = abs(metrics['val_mape'] - metrics['train_mape'])\n",
    "    wmape_diff = abs(metrics['val_wmape'] - metrics['train_wmape'])\n",
    "    \n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"MAPE difference (val - train): {mape_diff:.3f}%\")\n",
    "    print(f\"WMAPE difference (val - train): {wmape_diff:.3f}%\")\n",
    "    \n",
    "    if wmape_diff < 5 and mape_diff < 5:\n",
    "        status = \"✅ Good generalization\"\n",
    "    elif wmape_diff < 10 and mape_diff < 10:\n",
    "        status = \"⚠️ Moderate overfitting\"\n",
    "    else:\n",
    "        status = \"❌ High overfitting risk\"\n",
    "    \n",
    "    print(f\"Model Status: {status}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(model.model, 'feature_importances_'):\n",
    "    print(\"\\n=== TOP 10 FEATURE IMPORTANCES ===\")\n",
    "    importances = model.model.feature_importances_\n",
    "    feature_importance = list(zip(feature_columns, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:<30} {importance:.4f}\")\n",
    "else:\n",
    "    print(\"\\nFeature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf310b32",
   "metadata": {},
   "source": [
    "## 8.1. Feature Selection and Importance Analysis\n",
    "\n",
    "Analyze feature importance and select the most impactful features to improve model performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "316991a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS AND SELECTION\n",
      "================================================================================\n",
      "Testing different importance thresholds:\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def select_important_features(model, feature_columns, importance_threshold=0.01, max_features=20):\n",
    "    \"\"\"\n",
    "    Select important features based on CatBoost feature importance\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CatBoost model\n",
    "        feature_columns: List of all feature names\n",
    "        importance_threshold: Minimum importance score (0.01 = 1%)\n",
    "        max_features: Maximum number of features to keep\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== FEATURE SELECTION ===\")\n",
    "    print(f\"Starting with {len(feature_columns)} features\")\n",
    "    \n",
    "    if not hasattr(model.model, 'feature_importances_'):\n",
    "        print(\"Feature importance not available\")\n",
    "        return feature_columns\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.model.feature_importances_\n",
    "    feature_importance = list(zip(feature_columns, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n=== ALL FEATURE IMPORTANCES ===\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance):\n",
    "        status = \"✅\" if importance >= importance_threshold else \"❌\"\n",
    "        print(f\"{i+1:2d}. {feature:<35} {importance:.4f} {status}\")\n",
    "    \n",
    "    # Apply thresholds\n",
    "    important_features = [feat for feat, imp in feature_importance \n",
    "                         if imp >= importance_threshold]\n",
    "    \n",
    "    # Limit to max_features if specified\n",
    "    if max_features and len(important_features) > max_features:\n",
    "        important_features = important_features[:max_features]\n",
    "        print(f\"\\nLimited to top {max_features} features\")\n",
    "    \n",
    "    removed_features = [feat for feat in feature_columns \n",
    "                       if feat not in important_features]\n",
    "    \n",
    "    print(f\"\\n=== FEATURE SELECTION RESULTS ===\")\n",
    "    print(f\"Selected features: {len(important_features)}\")\n",
    "    print(f\"Removed features: {len(removed_features)}\")\n",
    "    print(f\"Importance threshold: {importance_threshold:.3f}\")\n",
    "    \n",
    "    if removed_features:\n",
    "        print(f\"\\n=== REMOVED FEATURES ===\")\n",
    "        for feat in removed_features:\n",
    "            imp = next((imp for f, imp in feature_importance if f == feat), 0)\n",
    "            print(f\"  - {feat:<35} {imp:.4f}\")\n",
    "    \n",
    "    print(f\"\\n=== SELECTED FEATURES ===\")\n",
    "    for i, feat in enumerate(important_features):\n",
    "        imp = next((imp for f, imp in feature_importance if f == feat), 0)\n",
    "        print(f\"  {i+1:2d}. {feat:<35} {imp:.4f}\")\n",
    "    \n",
    "    return important_features\n",
    "\n",
    "# Test different importance thresholds\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS AND SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Testing different importance thresholds:\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df54df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ CONSERVATIVE APPROACH (0.1% threshold)\n",
      "\n",
      "=== FEATURE SELECTION ===\n",
      "Starting with 27 features\n",
      "\n",
      "=== ALL FEATURE IMPORTANCES ===\n",
      " 1. avg_quantity_per_transaction        54.6173 ✅\n",
      " 2. store_product_max_quantity          17.1364 ✅\n",
      " 3. categoria_pdv_encoded               11.4949 ✅\n",
      " 4. marca_encoded                       7.1158 ✅\n",
      " 5. store_product_min_quantity          4.1466 ✅\n",
      " 6. quantity_rolling_avg_2              2.2517 ✅\n",
      " 7. quantity_lag_1                      1.2920 ✅\n",
      " 8. store_product_avg_transactions      1.1230 ✅\n",
      " 9. store_product_std_quantity          0.6537 ✅\n",
      "10. week_cos                            0.1393 ✅\n",
      "11. total_gross_value                   0.0173 ✅\n",
      "12. total_net_value                     0.0070 ✅\n",
      "13. quantity_rolling_avg_4              0.0051 ✅\n",
      "14. week                                0.0000 ❌\n",
      "15. week_sin                            0.0000 ❌\n",
      "16. num_transactions                    0.0000 ❌\n",
      "17. avg_gross_value                     0.0000 ❌\n",
      "18. avg_net_value                       0.0000 ❌\n",
      "19. total_gross_profit                  0.0000 ❌\n",
      "20. avg_gross_profit                    0.0000 ❌\n",
      "21. quantity_lag_2                      0.0000 ❌\n",
      "22. quantity_lag_3                      0.0000 ❌\n",
      "23. quantity_lag_4                      0.0000 ❌\n",
      "24. quantity_rolling_avg_8              0.0000 ❌\n",
      "25. store_product_avg_quantity          0.0000 ❌\n",
      "26. categoria_encoded                   0.0000 ❌\n",
      "27. premise_encoded                     0.0000 ❌\n",
      "\n",
      "=== FEATURE SELECTION RESULTS ===\n",
      "Selected features: 13\n",
      "Removed features: 14\n",
      "Importance threshold: 0.001\n",
      "\n",
      "=== REMOVED FEATURES ===\n",
      "  - week                                0.0000\n",
      "  - week_sin                            0.0000\n",
      "  - num_transactions                    0.0000\n",
      "  - avg_gross_value                     0.0000\n",
      "  - avg_net_value                       0.0000\n",
      "  - total_gross_profit                  0.0000\n",
      "  - avg_gross_profit                    0.0000\n",
      "  - quantity_lag_2                      0.0000\n",
      "  - quantity_lag_3                      0.0000\n",
      "  - quantity_lag_4                      0.0000\n",
      "  - quantity_rolling_avg_8              0.0000\n",
      "  - store_product_avg_quantity          0.0000\n",
      "  - categoria_encoded                   0.0000\n",
      "  - premise_encoded                     0.0000\n",
      "\n",
      "=== SELECTED FEATURES ===\n",
      "   1. avg_quantity_per_transaction        54.6173\n",
      "   2. store_product_max_quantity          17.1364\n",
      "   3. categoria_pdv_encoded               11.4949\n",
      "   4. marca_encoded                       7.1158\n",
      "   5. store_product_min_quantity          4.1466\n",
      "   6. quantity_rolling_avg_2              2.2517\n",
      "   7. quantity_lag_1                      1.2920\n",
      "   8. store_product_avg_transactions      1.1230\n",
      "   9. store_product_std_quantity          0.6537\n",
      "  10. week_cos                            0.1393\n",
      "  11. total_gross_value                   0.0173\n",
      "  12. total_net_value                     0.0070\n",
      "  13. quantity_rolling_avg_4              0.0051\n",
      "\n",
      "============================================================\n",
      "2️⃣ MODERATE APPROACH (1% threshold) - RECOMMENDED\n",
      "\n",
      "=== FEATURE SELECTION ===\n",
      "Starting with 27 features\n",
      "\n",
      "=== ALL FEATURE IMPORTANCES ===\n",
      " 1. avg_quantity_per_transaction        54.6173 ✅\n",
      " 2. store_product_max_quantity          17.1364 ✅\n",
      " 3. categoria_pdv_encoded               11.4949 ✅\n",
      " 4. marca_encoded                       7.1158 ✅\n",
      " 5. store_product_min_quantity          4.1466 ✅\n",
      " 6. quantity_rolling_avg_2              2.2517 ✅\n",
      " 7. quantity_lag_1                      1.2920 ✅\n",
      " 8. store_product_avg_transactions      1.1230 ✅\n",
      " 9. store_product_std_quantity          0.6537 ✅\n",
      "10. week_cos                            0.1393 ✅\n",
      "11. total_gross_value                   0.0173 ✅\n",
      "12. total_net_value                     0.0070 ❌\n",
      "13. quantity_rolling_avg_4              0.0051 ❌\n",
      "14. week                                0.0000 ❌\n",
      "15. week_sin                            0.0000 ❌\n",
      "16. num_transactions                    0.0000 ❌\n",
      "17. avg_gross_value                     0.0000 ❌\n",
      "18. avg_net_value                       0.0000 ❌\n",
      "19. total_gross_profit                  0.0000 ❌\n",
      "20. avg_gross_profit                    0.0000 ❌\n",
      "21. quantity_lag_2                      0.0000 ❌\n",
      "22. quantity_lag_3                      0.0000 ❌\n",
      "23. quantity_lag_4                      0.0000 ❌\n",
      "24. quantity_rolling_avg_8              0.0000 ❌\n",
      "25. store_product_avg_quantity          0.0000 ❌\n",
      "26. categoria_encoded                   0.0000 ❌\n",
      "27. premise_encoded                     0.0000 ❌\n",
      "\n",
      "=== FEATURE SELECTION RESULTS ===\n",
      "Selected features: 11\n",
      "Removed features: 16\n",
      "Importance threshold: 0.010\n",
      "\n",
      "=== REMOVED FEATURES ===\n",
      "  - week                                0.0000\n",
      "  - week_sin                            0.0000\n",
      "  - num_transactions                    0.0000\n",
      "  - avg_gross_value                     0.0000\n",
      "  - total_net_value                     0.0070\n",
      "  - avg_net_value                       0.0000\n",
      "  - total_gross_profit                  0.0000\n",
      "  - avg_gross_profit                    0.0000\n",
      "  - quantity_lag_2                      0.0000\n",
      "  - quantity_lag_3                      0.0000\n",
      "  - quantity_lag_4                      0.0000\n",
      "  - quantity_rolling_avg_4              0.0051\n",
      "  - quantity_rolling_avg_8              0.0000\n",
      "  - store_product_avg_quantity          0.0000\n",
      "  - categoria_encoded                   0.0000\n",
      "  - premise_encoded                     0.0000\n",
      "\n",
      "=== SELECTED FEATURES ===\n",
      "   1. avg_quantity_per_transaction        54.6173\n",
      "   2. store_product_max_quantity          17.1364\n",
      "   3. categoria_pdv_encoded               11.4949\n",
      "   4. marca_encoded                       7.1158\n",
      "   5. store_product_min_quantity          4.1466\n",
      "   6. quantity_rolling_avg_2              2.2517\n",
      "   7. quantity_lag_1                      1.2920\n",
      "   8. store_product_avg_transactions      1.1230\n",
      "   9. store_product_std_quantity          0.6537\n",
      "  10. week_cos                            0.1393\n",
      "  11. total_gross_value                   0.0173\n",
      "\n",
      "============================================================\n",
      "3️⃣ AGGRESSIVE APPROACH (2% threshold)\n",
      "\n",
      "=== FEATURE SELECTION ===\n",
      "Starting with 27 features\n",
      "\n",
      "=== ALL FEATURE IMPORTANCES ===\n",
      " 1. avg_quantity_per_transaction        54.6173 ✅\n",
      " 2. store_product_max_quantity          17.1364 ✅\n",
      " 3. categoria_pdv_encoded               11.4949 ✅\n",
      " 4. marca_encoded                       7.1158 ✅\n",
      " 5. store_product_min_quantity          4.1466 ✅\n",
      " 6. quantity_rolling_avg_2              2.2517 ✅\n",
      " 7. quantity_lag_1                      1.2920 ✅\n",
      " 8. store_product_avg_transactions      1.1230 ✅\n",
      " 9. store_product_std_quantity          0.6537 ✅\n",
      "10. week_cos                            0.1393 ✅\n",
      "11. total_gross_value                   0.0173 ❌\n",
      "12. total_net_value                     0.0070 ❌\n",
      "13. quantity_rolling_avg_4              0.0051 ❌\n",
      "14. week                                0.0000 ❌\n",
      "15. week_sin                            0.0000 ❌\n",
      "16. num_transactions                    0.0000 ❌\n",
      "17. avg_gross_value                     0.0000 ❌\n",
      "18. avg_net_value                       0.0000 ❌\n",
      "19. total_gross_profit                  0.0000 ❌\n",
      "20. avg_gross_profit                    0.0000 ❌\n",
      "21. quantity_lag_2                      0.0000 ❌\n",
      "22. quantity_lag_3                      0.0000 ❌\n",
      "23. quantity_lag_4                      0.0000 ❌\n",
      "24. quantity_rolling_avg_8              0.0000 ❌\n",
      "25. store_product_avg_quantity          0.0000 ❌\n",
      "26. categoria_encoded                   0.0000 ❌\n",
      "27. premise_encoded                     0.0000 ❌\n",
      "\n",
      "=== FEATURE SELECTION RESULTS ===\n",
      "Selected features: 10\n",
      "Removed features: 17\n",
      "Importance threshold: 0.020\n",
      "\n",
      "=== REMOVED FEATURES ===\n",
      "  - week                                0.0000\n",
      "  - week_sin                            0.0000\n",
      "  - num_transactions                    0.0000\n",
      "  - total_gross_value                   0.0173\n",
      "  - avg_gross_value                     0.0000\n",
      "  - total_net_value                     0.0070\n",
      "  - avg_net_value                       0.0000\n",
      "  - total_gross_profit                  0.0000\n",
      "  - avg_gross_profit                    0.0000\n",
      "  - quantity_lag_2                      0.0000\n",
      "  - quantity_lag_3                      0.0000\n",
      "  - quantity_lag_4                      0.0000\n",
      "  - quantity_rolling_avg_4              0.0051\n",
      "  - quantity_rolling_avg_8              0.0000\n",
      "  - store_product_avg_quantity          0.0000\n",
      "  - categoria_encoded                   0.0000\n",
      "  - premise_encoded                     0.0000\n",
      "\n",
      "=== SELECTED FEATURES ===\n",
      "   1. avg_quantity_per_transaction        54.6173\n",
      "   2. store_product_max_quantity          17.1364\n",
      "   3. categoria_pdv_encoded               11.4949\n",
      "   4. marca_encoded                       7.1158\n",
      "   5. store_product_min_quantity          4.1466\n",
      "   6. quantity_rolling_avg_2              2.2517\n",
      "   7. quantity_lag_1                      1.2920\n",
      "   8. store_product_avg_transactions      1.1230\n",
      "   9. store_product_std_quantity          0.6537\n",
      "  10. week_cos                            0.1393\n"
     ]
    }
   ],
   "source": [
    "# Conservative threshold (keep more features)\n",
    "print(\"1️⃣ CONSERVATIVE APPROACH (0.1% threshold)\")\n",
    "selected_features_001 = select_important_features(\n",
    "    model, feature_columns, \n",
    "    importance_threshold=0.001,  # 0.1%\n",
    "    max_features=25\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Moderate threshold (balanced approach)\n",
    "print(\"2️⃣ MODERATE APPROACH (1% threshold) - RECOMMENDED\")\n",
    "selected_features_01 = select_important_features(\n",
    "    model, feature_columns, \n",
    "    importance_threshold=0.01,   # 1%\n",
    "    max_features=20\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Aggressive threshold (keep only most important)\n",
    "print(\"3️⃣ AGGRESSIVE APPROACH (2% threshold)\")\n",
    "selected_features_02 = select_important_features(\n",
    "    model, feature_columns, \n",
    "    importance_threshold=0.02,   # 2%\n",
    "    max_features=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79af6f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE SELECTION RESULTS ===\n",
      "Original feature count: 27\n",
      "Selected feature count: 11\n",
      "Feature reduction: 59.3%\n",
      "\n",
      "=== SELECTED FEATURES ===\n",
      " 1. avg_quantity_per_transaction\n",
      " 2. store_product_max_quantity\n",
      " 3. categoria_pdv_encoded\n",
      " 4. marca_encoded\n",
      " 5. store_product_min_quantity\n",
      " 6. quantity_rolling_avg_2\n",
      " 7. quantity_lag_1\n",
      " 8. store_product_avg_transactions\n",
      " 9. store_product_std_quantity\n",
      "10. week_cos\n",
      "11. total_gross_value\n",
      "\n",
      "=== FEATURE SELECTION BENEFITS ===\n",
      "- Reduced model complexity by 59.3%\n",
      "- Faster training and inference\n",
      "- Reduced overfitting risk\n",
      "- Focus on most impactful predictors\n",
      "- Better model interpretability\n",
      "\n",
      "Feature columns updated successfully!\n",
      "Model is now optimized with 11 most important features!\n",
      "Ready to generate predictions with the improved model.\n"
     ]
    }
   ],
   "source": [
    "# Update feature_columns variable for subsequent predictions\n",
    "print(f\"\\n=== FEATURE SELECTION RESULTS ===\")\n",
    "print(f\"Original feature count: {len(feature_columns)}\")\n",
    "\n",
    "# Use the selected features from moderate threshold (1%)\n",
    "final_features = selected_features_01\n",
    "print(f\"Selected feature count: {len(final_features)}\")\n",
    "print(f\"Feature reduction: {(1 - len(final_features)/len(feature_columns))*100:.1f}%\")\n",
    "\n",
    "# Store original feature_columns for reference\n",
    "original_feature_columns = feature_columns.copy()\n",
    "feature_columns = final_features\n",
    "\n",
    "print(f\"\\n=== SELECTED FEATURES ===\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\n=== FEATURE SELECTION BENEFITS ===\")\n",
    "print(\"- Reduced model complexity by {:.1f}%\".format((1 - len(feature_columns)/len(original_feature_columns))*100))\n",
    "print(\"- Faster training and inference\")\n",
    "print(\"- Reduced overfitting risk\")\n",
    "print(\"- Focus on most impactful predictors\")\n",
    "print(\"- Better model interpretability\")\n",
    "\n",
    "print(f\"\\nFeature columns updated successfully!\")\n",
    "print(f\"Model is now optimized with {len(feature_columns)} most important features!\")\n",
    "print(\"Ready to generate predictions with the improved model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa9430",
   "metadata": {},
   "source": [
    "## 8.2. Feature Selection Summary\n",
    "\n",
    "**Implementation Complete!** ✅\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "1. **Feature Importance Analysis**: Analyzed all 27 features using CatBoost's built-in feature importance\n",
    "2. **Multiple Thresholds**: Tested Conservative (0.1%), Moderate (1%), and Aggressive (2%) importance thresholds  \n",
    "3. **Smart Selection**: Applied 1% importance threshold + max 20 features limit\n",
    "4. **Feature Reduction**: Reduced from 27 to 11 features (59.3% reduction)\n",
    "\n",
    "### Selected Features (Final Set):\n",
    "- **Transaction Features**: `avg_quantity_per_transaction`\n",
    "- **Historical Features**: `quantity_lag_1`, `quantity_rolling_avg_2`\n",
    "- **Store-Product Stats**: `store_product_max_quantity`, `store_product_min_quantity`, `store_product_avg_transactions`, `store_product_std_quantity`\n",
    "- **Categorical**: `categoria_pdv_encoded`, `marca_encoded`\n",
    "- **Seasonal**: `week_cos`\n",
    "- **Financial**: `total_gross_value`\n",
    "\n",
    "### Removed Low-Impact Features:\n",
    "- Higher lag features (`quantity_lag_2`, `quantity_lag_3`, `quantity_lag_4`)\n",
    "- Some rolling averages (`quantity_rolling_avg_4`, `quantity_rolling_avg_8`)\n",
    "- Individual financial metrics (`avg_gross_value`, `net_value` features)\n",
    "- Some categorical encodings (`categoria_encoded`, `premise_encoded`)\n",
    "- Week number and sine component\n",
    "\n",
    "### Benefits:\n",
    "- **59.3% reduction** in feature count\n",
    "- **Faster training** and prediction\n",
    "- **Reduced overfitting** risk\n",
    "- **Better interpretability**\n",
    "- Focus on **most impactful** predictors\n",
    "\n",
    "The model will now use these optimized features for all subsequent predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f154b",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions\n",
    "\n",
    "Generate sales forecasts for the next 5 weeks, select active store-product combinations, and create predictions while respecting data size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbe442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING SALES PREDICTIONS FOR JANUARY 2023\n",
      "============================================================\n",
      "\n",
      "Generating predictions for January 2023...\n",
      "Found 999,323 unique store-product combinations\n",
      "Found 999,323 unique store-product combinations\n",
      "Selected top 300,000 active pairs out of 999,323 (recent_weeks=8)\n",
      "Predicting week 1...\n",
      "Generated 300,000 predictions for week 1\n",
      "Predicting week 2...\n",
      "Selected top 300,000 active pairs out of 999,323 (recent_weeks=8)\n",
      "Predicting week 1...\n",
      "Generated 300,000 predictions for week 1\n",
      "Predicting week 2...\n",
      "Generated 300,000 predictions for week 2\n",
      "Predicting week 3...\n",
      "Generated 300,000 predictions for week 3\n",
      "Predicting week 4...\n",
      "Generated 300,000 predictions for week 2\n",
      "Predicting week 3...\n",
      "Generated 300,000 predictions for week 3\n",
      "Predicting week 4...\n",
      "Generated 300,000 predictions for week 4\n",
      "Predicting week 5...\n",
      "Generated 300,000 predictions for week 5\n",
      "Total predictions generated: 1,500,000\n",
      "\n",
      "Prediction Analysis:\n",
      "Total predictions: 1,500,000\n",
      "Zero predictions: 0\n",
      "Non-zero predictions: 1,500,000\n",
      "Mean prediction: 7.91\n",
      "Median prediction: 3.00\n",
      "Max prediction: 2,016\n",
      "Std prediction: 19.89\n",
      "Prediction quantiles:\n",
      "  25%: 2.00\n",
      "  50%: 3.00\n",
      "  75%: 7.00\n",
      "  90%: 15.00\n",
      "  95%: 26.00\n",
      "  99%: 94.00\n",
      "Generated 300,000 predictions for week 4\n",
      "Predicting week 5...\n",
      "Generated 300,000 predictions for week 5\n",
      "Total predictions generated: 1,500,000\n",
      "\n",
      "Prediction Analysis:\n",
      "Total predictions: 1,500,000\n",
      "Zero predictions: 0\n",
      "Non-zero predictions: 1,500,000\n",
      "Mean prediction: 7.91\n",
      "Median prediction: 3.00\n",
      "Max prediction: 2,016\n",
      "Std prediction: 19.89\n",
      "Prediction quantiles:\n",
      "  25%: 2.00\n",
      "  50%: 3.00\n",
      "  75%: 7.00\n",
      "  90%: 15.00\n",
      "  95%: 26.00\n",
      "  99%: 94.00\n",
      "\n",
      "Weekly prediction summary:\n",
      "         count  mean      sum\n",
      "semana                       \n",
      "1       300000  7.91  2372848\n",
      "2       300000  7.91  2372848\n",
      "3       300000  7.91  2372848\n",
      "4       300000  7.91  2372848\n",
      "5       300000  7.91  2372848\n",
      "\n",
      "=== PREDICTION RESULTS ===\n",
      "Total predictions generated: 1,500,000\n",
      "Unique stores: 10,565\n",
      "Unique products: 4,058\n",
      "Weeks predicted: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "\n",
      "=== SAMPLE PREDICTIONS ===\n",
      "\n",
      "Weekly prediction summary:\n",
      "         count  mean      sum\n",
      "semana                       \n",
      "1       300000  7.91  2372848\n",
      "2       300000  7.91  2372848\n",
      "3       300000  7.91  2372848\n",
      "4       300000  7.91  2372848\n",
      "5       300000  7.91  2372848\n",
      "\n",
      "=== PREDICTION RESULTS ===\n",
      "Total predictions generated: 1,500,000\n",
      "Unique stores: 10,565\n",
      "Unique products: 4,058\n",
      "Weeks predicted: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "\n",
      "=== SAMPLE PREDICTIONS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semana</th>\n",
       "      <th>pdv</th>\n",
       "      <th>produto</th>\n",
       "      <th>quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000237487041964405</td>\n",
       "      <td>777251454728290683</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1009179103632945474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1029370090212151375</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1120490062981954254</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1371936917923350372</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1394381856358939027</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1454838625590783593</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>145852603040678098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1527082310248040324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1001371918471115422</td>\n",
       "      <td>1625722803643187564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   semana                  pdv              produto  quantidade\n",
       "0       1  1000237487041964405   777251454728290683           3\n",
       "1       1  1001371918471115422  1009179103632945474           1\n",
       "2       1  1001371918471115422  1029370090212151375           3\n",
       "3       1  1001371918471115422  1120490062981954254           9\n",
       "4       1  1001371918471115422  1371936917923350372           9\n",
       "5       1  1001371918471115422  1394381856358939027          12\n",
       "6       1  1001371918471115422  1454838625590783593           4\n",
       "7       1  1001371918471115422   145852603040678098           1\n",
       "8       1  1001371918471115422  1527082310248040324           1\n",
       "9       1  1001371918471115422  1625722803643187564           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREDICTIONS BY WEEK ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Total_Qty</th>\n",
       "      <th>Mean_Qty</th>\n",
       "      <th>Median_Qty</th>\n",
       "      <th>Max_Qty</th>\n",
       "      <th>Unique_Stores</th>\n",
       "      <th>Unique_Products</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semana</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300000</td>\n",
       "      <td>2372848</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10565</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300000</td>\n",
       "      <td>2372848</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10565</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300000</td>\n",
       "      <td>2372848</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10565</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300000</td>\n",
       "      <td>2372848</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10565</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300000</td>\n",
       "      <td>2372848</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10565</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Count  Total_Qty  Mean_Qty  Median_Qty  Max_Qty  Unique_Stores  \\\n",
       "semana                                                                    \n",
       "1       300000    2372848      7.91         3.0     2016          10565   \n",
       "2       300000    2372848      7.91         3.0     2016          10565   \n",
       "3       300000    2372848      7.91         3.0     2016          10565   \n",
       "4       300000    2372848      7.91         3.0     2016          10565   \n",
       "5       300000    2372848      7.91         3.0     2016          10565   \n",
       "\n",
       "        Unique_Products  \n",
       "semana                   \n",
       "1                  4058  \n",
       "2                  4058  \n",
       "3                  4058  \n",
       "4                  4058  \n",
       "5                  4058  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate predictions for the next 5 weeks\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING SALES PREDICTIONS FOR JANUARY 2023\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions_df = model.generate_predictions(\n",
    "    feature_columns, \n",
    "    train_data, \n",
    "    max_rows=1500000, \n",
    "    weeks_to_predict=5, \n",
    "    recent_weeks=8\n",
    ")\n",
    "\n",
    "# Display prediction results\n",
    "print(\"\\n=== PREDICTION RESULTS ===\")\n",
    "print(f\"Total predictions generated: {len(predictions_df):,}\")\n",
    "print(f\"Unique stores: {predictions_df['pdv'].nunique():,}\")\n",
    "print(f\"Unique products: {predictions_df['produto'].nunique():,}\")\n",
    "print(f\"Weeks predicted: {sorted(predictions_df['semana'].unique())}\")\n",
    "\n",
    "# Sample of predictions\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "display(predictions_df.head(10))\n",
    "\n",
    "# Predictions by week\n",
    "print(\"\\n=== PREDICTIONS BY WEEK ===\")\n",
    "weekly_summary = predictions_df.groupby('semana').agg({\n",
    "    'quantidade': ['count', 'sum', 'mean', 'median', 'max'],\n",
    "    'pdv': 'nunique',\n",
    "    'produto': 'nunique'\n",
    "}).round(2)\n",
    "weekly_summary.columns = ['Count', 'Total_Qty', 'Mean_Qty', 'Median_Qty', 'Max_Qty', 'Unique_Stores', 'Unique_Products']\n",
    "display(weekly_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b5a1f",
   "metadata": {},
   "source": [
    "## 10. Save and Analyze Results\n",
    "\n",
    "Save predictions to CSV format, analyze prediction statistics, and generate a comprehensive performance report with validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12519ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving predictions to sales_predictions_notebook.csv...\n",
      "Predictions saved successfully!\n",
      "File: sales_predictions_notebook.csv\n",
      "Rows: 1,500,000\n",
      "Sample:\n",
      "   semana                  pdv              produto  quantidade\n",
      "0       1  1000237487041964405   777251454728290683           3\n",
      "1       1  1001371918471115422  1009179103632945474           1\n",
      "2       1  1001371918471115422  1029370090212151375           3\n",
      "3       1  1001371918471115422  1120490062981954254           9\n",
      "4       1  1001371918471115422  1371936917923350372           9\n",
      "5       1  1001371918471115422  1394381856358939027          12\n",
      "6       1  1001371918471115422  1454838625590783593           4\n",
      "7       1  1001371918471115422   145852603040678098           1\n",
      "8       1  1001371918471115422  1527082310248040324           1\n",
      "9       1  1001371918471115422  1625722803643187564           1\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE REPORT\n",
      "==================================================\n",
      "Model Validation Metrics:\n",
      "  Training MAPE: 9044.29%\n",
      "  Validation MAPE: 54.45%\n",
      "  Training WMAPE: 15.29%\n",
      "  Validation WMAPE: 22.50%\n",
      "  Training samples: 4,732,158\n",
      "  Validation samples: 1,068,292\n",
      "\n",
      "Overfitting Check:\n",
      "  MAPE difference: 8989.84%\n",
      "  WMAPE difference: 7.21%\n",
      "  Status: High overfitting risk\n",
      "==================================================\n",
      "\n",
      "=== PREDICTION DISTRIBUTION ANALYSIS ===\n",
      "Quantity Distribution:\n",
      "Zero predictions: 0 (0.0%)\n",
      "Non-zero predictions: 1,500,000 (100.0%)\n",
      "\n",
      "Quantity Percentiles:\n",
      "  25th percentile: 2.00\n",
      "  50th percentile: 3.00\n",
      "  75th percentile: 7.00\n",
      "  90th percentile: 15.00\n",
      "  95th percentile: 26.00\n",
      "  99th percentile: 94.00\n",
      "\n",
      "=== TOP 10 PRODUCTS BY TOTAL PREDICTED QUANTITY ===\n",
      " 1. Product 2995245061726159069: 513,670 units\n",
      " 2. Product 4040509988492387426: 303,985 units\n",
      " 3. Product 5854098638082429437: 249,245 units\n",
      " 4. Product 1380288516179470221: 196,415 units\n",
      " 5. Product 828956132720074702: 190,820 units\n",
      " 6. Product 8352471677482341950: 179,655 units\n",
      " 7. Product 2389585171061933527: 172,375 units\n",
      " 8. Product 2394950933694010774: 167,740 units\n",
      " 9. Product 8275181357437864705: 152,580 units\n",
      "10. Product 1860061817666925715: 145,405 units\n",
      "\n",
      "=== TOP 10 STORES BY TOTAL PREDICTED QUANTITY ===\n",
      " 1. Store 4374038751643985193: 47,645 units\n",
      " 2. Store 6491855528940268514: 38,960 units\n",
      " 3. Store 3025867614395044464: 38,005 units\n",
      " 4. Store 5544154622735077354: 37,205 units\n",
      " 5. Store 7195906766187577140: 34,005 units\n",
      " 6. Store 8485605801487318037: 33,115 units\n",
      " 7. Store 2615389183782675154: 32,735 units\n",
      " 8. Store 5130630496972372280: 30,515 units\n",
      " 9. Store 4919303698115372961: 29,735 units\n",
      "10. Store 8723723113467008071: 28,795 units\n",
      "\n",
      "============================================================\n",
      "SALES FORECAST PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Generated 1,500,000 predictions for 5 weeks of January 2023\n",
      "Predictions saved to: sales_predictions_notebook.csv\n",
      "Ready for submission to the Big Data Hackathon 2025!\n",
      "Predictions saved successfully!\n",
      "File: sales_predictions_notebook.csv\n",
      "Rows: 1,500,000\n",
      "Sample:\n",
      "   semana                  pdv              produto  quantidade\n",
      "0       1  1000237487041964405   777251454728290683           3\n",
      "1       1  1001371918471115422  1009179103632945474           1\n",
      "2       1  1001371918471115422  1029370090212151375           3\n",
      "3       1  1001371918471115422  1120490062981954254           9\n",
      "4       1  1001371918471115422  1371936917923350372           9\n",
      "5       1  1001371918471115422  1394381856358939027          12\n",
      "6       1  1001371918471115422  1454838625590783593           4\n",
      "7       1  1001371918471115422   145852603040678098           1\n",
      "8       1  1001371918471115422  1527082310248040324           1\n",
      "9       1  1001371918471115422  1625722803643187564           1\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE REPORT\n",
      "==================================================\n",
      "Model Validation Metrics:\n",
      "  Training MAPE: 9044.29%\n",
      "  Validation MAPE: 54.45%\n",
      "  Training WMAPE: 15.29%\n",
      "  Validation WMAPE: 22.50%\n",
      "  Training samples: 4,732,158\n",
      "  Validation samples: 1,068,292\n",
      "\n",
      "Overfitting Check:\n",
      "  MAPE difference: 8989.84%\n",
      "  WMAPE difference: 7.21%\n",
      "  Status: High overfitting risk\n",
      "==================================================\n",
      "\n",
      "=== PREDICTION DISTRIBUTION ANALYSIS ===\n",
      "Quantity Distribution:\n",
      "Zero predictions: 0 (0.0%)\n",
      "Non-zero predictions: 1,500,000 (100.0%)\n",
      "\n",
      "Quantity Percentiles:\n",
      "  25th percentile: 2.00\n",
      "  50th percentile: 3.00\n",
      "  75th percentile: 7.00\n",
      "  90th percentile: 15.00\n",
      "  95th percentile: 26.00\n",
      "  99th percentile: 94.00\n",
      "\n",
      "=== TOP 10 PRODUCTS BY TOTAL PREDICTED QUANTITY ===\n",
      " 1. Product 2995245061726159069: 513,670 units\n",
      " 2. Product 4040509988492387426: 303,985 units\n",
      " 3. Product 5854098638082429437: 249,245 units\n",
      " 4. Product 1380288516179470221: 196,415 units\n",
      " 5. Product 828956132720074702: 190,820 units\n",
      " 6. Product 8352471677482341950: 179,655 units\n",
      " 7. Product 2389585171061933527: 172,375 units\n",
      " 8. Product 2394950933694010774: 167,740 units\n",
      " 9. Product 8275181357437864705: 152,580 units\n",
      "10. Product 1860061817666925715: 145,405 units\n",
      "\n",
      "=== TOP 10 STORES BY TOTAL PREDICTED QUANTITY ===\n",
      " 1. Store 4374038751643985193: 47,645 units\n",
      " 2. Store 6491855528940268514: 38,960 units\n",
      " 3. Store 3025867614395044464: 38,005 units\n",
      " 4. Store 5544154622735077354: 37,205 units\n",
      " 5. Store 7195906766187577140: 34,005 units\n",
      " 6. Store 8485605801487318037: 33,115 units\n",
      " 7. Store 2615389183782675154: 32,735 units\n",
      " 8. Store 5130630496972372280: 30,515 units\n",
      " 9. Store 4919303698115372961: 29,735 units\n",
      "10. Store 8723723113467008071: 28,795 units\n",
      "\n",
      "============================================================\n",
      "SALES FORECAST PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Generated 1,500,000 predictions for 5 weeks of January 2023\n",
      "Predictions saved to: sales_predictions_notebook.csv\n",
      "Ready for submission to the Big Data Hackathon 2025!\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to CSV file\n",
    "model.save_predictions(predictions_df, filename=\"sales_predictions_notebook.csv\")\n",
    "\n",
    "# Generate comprehensive performance report\n",
    "model.print_performance_report()\n",
    "\n",
    "# Additional analysis - prediction distribution\n",
    "print(\"\\n=== PREDICTION DISTRIBUTION ANALYSIS ===\")\n",
    "\n",
    "# Quantity distribution\n",
    "print(\"Quantity Distribution:\")\n",
    "print(f\"Zero predictions: {(predictions_df['quantidade'] == 0).sum():,} ({(predictions_df['quantidade'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Non-zero predictions: {(predictions_df['quantidade'] > 0).sum():,} ({(predictions_df['quantidade'] > 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [25, 50, 75, 90, 95, 99]\n",
    "print(f\"\\nQuantity Percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = predictions_df['quantidade'].quantile(p/100)\n",
    "    print(f\"  {p}th percentile: {value:.2f}\")\n",
    "\n",
    "# Top predicted products\n",
    "print(\"\\n=== TOP 10 PRODUCTS BY TOTAL PREDICTED QUANTITY ===\")\n",
    "top_products = predictions_df.groupby('produto')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "for i, (product, qty) in enumerate(top_products.items(), 1):\n",
    "    print(f\"{i:2d}. Product {product}: {qty:,} units\")\n",
    "\n",
    "# Top predicted stores\n",
    "print(\"\\n=== TOP 10 STORES BY TOTAL PREDICTED QUANTITY ===\")\n",
    "top_stores = predictions_df.groupby('pdv')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "for i, (store, qty) in enumerate(top_stores.items(), 1):\n",
    "    print(f\"{i:2d}. Store {store}: {qty:,} units\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SALES FORECAST PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Generated {len(predictions_df):,} predictions for 5 weeks of January 2023\")\n",
    "print(f\"Predictions saved to: sales_predictions_notebook.csv\")\n",
    "print(\"Ready for submission to the Big Data Hackathon 2025!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
