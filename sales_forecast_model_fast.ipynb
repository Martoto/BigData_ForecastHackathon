{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecast Model V2 - Daily Predictions for January 2023 with Weekly Aggregation\n",
        "\n",
        "This enhanced sales forecasting model generates **daily predictions for January 2-31, 2023** and then aggregates them to weekly predictions. This approach provides:\n",
        "\n",
        "- **Higher temporal granularity**: Daily predictions capture day-of-week patterns\n",
        "- **January-focused**: Predictions for exactly 30 days (Jan 2-31, 2023)\n",
        "- **Better accuracy**: More nuanced modeling of sales behavior \n",
        "- **Weekly aggregation**: Final output matches required weekly format\n",
        "- **Day-of-week patterns**: Accounts for different sales volumes across weekdays vs weekends\n",
        "\n",
        "The model trains on weekly data but applies day-of-week adjustment factors during prediction to generate realistic daily forecasts for the entire month of January 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries and setup environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Optional: Optuna for hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Warning: Optuna not available. Install with: pip install optuna\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the main SalesForecastModelV2 class with initialization\n",
        "class SalesForecastModelV2:\n",
        "    def __init__(self, data_path=\"data/\"):\n",
        "        self.data_path = data_path\n",
        "        self.transactions = None\n",
        "        self.products = None\n",
        "        self.stores = None\n",
        "        self.model = None\n",
        "        self.label_encoders = {}\n",
        "        self.validation_metrics = {}\n",
        "        self.best_params = None\n",
        "        self.target_log_transformed = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from parquet files and identify data types\n",
        "def load_data(self):\n",
        "    print(\"Loading data...\")\n",
        "    parquet_files = [f for f in os.listdir(self.data_path) if f.endswith('.parquet')]\n",
        "    print(f\"Found {len(parquet_files)} parquet files\")\n",
        "    \n",
        "    for file in parquet_files:\n",
        "        df = pd.read_parquet(os.path.join(self.data_path, file))\n",
        "        print(f\"{file}: Shape {df.shape}\")\n",
        "        \n",
        "        if 'internal_store_id' in df.columns and 'quantity' in df.columns:\n",
        "            self.transactions = df\n",
        "            print(\"-> Identified as TRANSACTIONS data\")\n",
        "        elif 'produto' in df.columns and 'categoria' in df.columns:\n",
        "            self.products = df\n",
        "            print(\"-> Identified as PRODUCTS data\")\n",
        "        elif 'pdv' in df.columns and 'premise' in df.columns:\n",
        "            self.stores = df\n",
        "            print(\"-> Identified as STORES data\")\n",
        "    \n",
        "    print(f\"Data loaded successfully:\")\n",
        "    print(f\"- Transactions: {self.transactions.shape[0]:,} rows\")\n",
        "    print(f\"- Products: {self.products.shape[0]:,} rows\")\n",
        "    print(f\"- Stores: {self.stores.shape[0]:,} rows\")\n",
        "\n",
        "SalesForecastModelV2.load_data = load_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data by removing nulls, outliers, and filtering to 2022 data\n",
        "def cleanse_data(self):\n",
        "    print(\"\\nCleansing data...\")\n",
        "    \n",
        "    initial_rows = len(self.transactions)\n",
        "    self.transactions = self.transactions.dropna(subset=['internal_store_id', 'internal_product_id', 'quantity', 'transaction_date'])\n",
        "    print(f\"Removed {initial_rows - len(self.transactions):,} rows with null values\")\n",
        "    \n",
        "    self.transactions = self.transactions[self.transactions['quantity'] > 0]\n",
        "    print(f\"Kept {len(self.transactions):,} rows with positive quantities\")\n",
        "    \n",
        "    # Remove extreme value outliers (likely data errors)\n",
        "    print(\"Removing extreme value outliers...\")\n",
        "    initial_rows = len(self.transactions)\n",
        "    \n",
        "    # Calculate value per unit to detect unrealistic transactions\n",
        "    value_per_unit = self.transactions['gross_value'] / self.transactions['quantity']\n",
        "    q01 = value_per_unit.quantile(0.005)\n",
        "    q99 = value_per_unit.quantile(0.995)\n",
        "    \n",
        "    # Remove transactions with extreme value per unit\n",
        "    valid_value_mask = (value_per_unit >= q01) & (value_per_unit <= q99)\n",
        "    self.transactions = self.transactions[valid_value_mask]\n",
        "    \n",
        "    # Also cap extreme quantities (likely bulk orders or errors)\n",
        "    quantity_q99 = self.transactions['quantity'].quantile(0.995)\n",
        "    extreme_qty_mask = self.transactions['quantity'] <= quantity_q99\n",
        "    self.transactions = self.transactions[extreme_qty_mask]\n",
        "    \n",
        "    print(f\"Removed {initial_rows - len(self.transactions):,} outlier transactions ({((initial_rows - len(self.transactions))/initial_rows)*100:.2f}%)\")\n",
        "    \n",
        "    self.transactions[['transaction_date', 'reference_date']] = self.transactions[['transaction_date', 'reference_date']].apply(pd.to_datetime)\n",
        "    \n",
        "    self.transactions = self.transactions[\n",
        "        (self.transactions['transaction_date'].dt.year == 2022)\n",
        "    ]\n",
        "    print(f\"Filtered to 2022 data: {len(self.transactions):,} rows\")\n",
        "    \n",
        "    print(\"Cleaning products and stores...\")\n",
        "    self.products['descricao'] = self.products['descricao'].fillna('Unknown')\n",
        "    self.products['categoria'] = self.products['categoria'].fillna('Other')\n",
        "    self.products['marca'] = self.products['marca'].fillna('Unknown')\n",
        "    \n",
        "    self.stores['categoria_pdv'] = self.stores['categoria_pdv'].fillna('Other')\n",
        "    self.stores['premise'] = self.stores['premise'].fillna('Unknown')\n",
        "\n",
        "SalesForecastModelV2.cleanse_data = cleanse_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge transactions with product and store data\n",
        "def merge_data(self):\n",
        "    print(\"\\nMerging data...\")\n",
        "    \n",
        "    merged_data = self.transactions.merge(\n",
        "        self.products, \n",
        "        left_on='internal_product_id', \n",
        "        right_on='produto', \n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"After product merge: {len(merged_data):,} rows\")\n",
        "    \n",
        "    merged_data = merged_data.merge(\n",
        "        self.stores,\n",
        "        left_on='internal_store_id',\n",
        "        right_on='pdv',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\"After store merge: {len(merged_data):,} rows\")\n",
        "    \n",
        "    self.merged_data = merged_data\n",
        "    print(\"Data merge completed\")\n",
        "\n",
        "SalesForecastModelV2.merge_data = merge_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create weekly aggregations and temporal features\n",
        "def create_weekly_aggregations(self):\n",
        "    print(\"\\nCreating weekly aggregations...\")\n",
        "    \n",
        "    dt_info = self.merged_data['transaction_date'].dt\n",
        "    self.merged_data['year'] = dt_info.year\n",
        "    self.merged_data['week'] = dt_info.isocalendar().week\n",
        "    self.merged_data['month'] = dt_info.month\n",
        "    self.merged_data['quarter'] = dt_info.quarter\n",
        "    self.merged_data['year_week'] = self.merged_data['year'].astype(str) + '_' + self.merged_data['week'].astype(str).str.zfill(2)\n",
        "    \n",
        "    weekly_data = self.merged_data.groupby([\n",
        "        'year_week', 'week', 'month', 'quarter', 'internal_store_id', 'internal_product_id',\n",
        "        'categoria', 'marca', 'premise', 'categoria_pdv'\n",
        "    ]).agg({\n",
        "        'quantity': ['sum', 'mean', 'count'],\n",
        "        'gross_value': ['sum', 'mean'],\n",
        "        'net_value': ['sum', 'mean'],\n",
        "        'gross_profit': ['sum', 'mean']\n",
        "    }).reset_index()\n",
        "    \n",
        "    weekly_data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in weekly_data.columns.values]\n",
        "    \n",
        "    column_mapping = {\n",
        "        'quantity_sum': 'total_quantity',\n",
        "        'quantity_mean': 'avg_quantity_per_transaction',\n",
        "        'quantity_count': 'num_transactions',\n",
        "        'gross_value_sum': 'total_gross_value',\n",
        "        'gross_value_mean': 'avg_gross_value',\n",
        "        'net_value_sum': 'total_net_value',\n",
        "        'net_value_mean': 'avg_net_value',\n",
        "        'gross_profit_sum': 'total_gross_profit',\n",
        "        'gross_profit_mean': 'avg_gross_profit'\n",
        "    }\n",
        "    weekly_data.rename(columns=column_mapping, inplace=True)\n",
        "    \n",
        "    self.weekly_data = weekly_data\n",
        "    print(f\"Created weekly aggregations: {len(self.weekly_data):,} rows\")\n",
        "    \n",
        "    # Cap weekly quantity outliers per store-product pair\n",
        "    print(\"Capping weekly quantity outliers...\")\n",
        "    self.cap_weekly_outliers()\n",
        "\n",
        "SalesForecastModelV2.create_weekly_aggregations = create_weekly_aggregations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cap extreme weekly quantities using robust per-group statistics\n",
        "def cap_weekly_outliers(self):\n",
        "    # Calculate caps per store-product pair (99.5th percentile)\n",
        "    store_product_caps = self.weekly_data.groupby(['internal_store_id', 'internal_product_id'])['total_quantity'].quantile(0.995).reset_index()\n",
        "    store_product_caps.rename(columns={'total_quantity': 'sp_quantity_cap'}, inplace=True)\n",
        "    \n",
        "    # Calculate global cap as backup (99.8th percentile)\n",
        "    global_cap = self.weekly_data['total_quantity'].quantile(0.998)\n",
        "    \n",
        "    # Merge caps\n",
        "    self.weekly_data = self.weekly_data.merge(\n",
        "        store_product_caps, \n",
        "        on=['internal_store_id', 'internal_product_id'], \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Apply caps (use store-product cap, fallback to global cap)\n",
        "    before_sum = self.weekly_data['total_quantity'].sum()\n",
        "    before_max = self.weekly_data['total_quantity'].max()\n",
        "    \n",
        "    self.weekly_data['sp_quantity_cap'] = self.weekly_data['sp_quantity_cap'].fillna(global_cap)\n",
        "    self.weekly_data['total_quantity'] = np.minimum(\n",
        "        self.weekly_data['total_quantity'], \n",
        "        self.weekly_data['sp_quantity_cap']\n",
        "    )\n",
        "    \n",
        "    after_sum = self.weekly_data['total_quantity'].sum()\n",
        "    after_max = self.weekly_data['total_quantity'].max()\n",
        "    \n",
        "    # Clean up temporary column\n",
        "    self.weekly_data.drop('sp_quantity_cap', axis=1, inplace=True)\n",
        "    \n",
        "    print(f\"  Before capping: max={before_max:,.0f}, total={before_sum:,.0f}\")\n",
        "    print(f\"  After capping:  max={after_max:,.0f}, total={after_sum:,.0f}\")\n",
        "    print(f\"  Reduction: {((before_sum - after_sum)/before_sum)*100:.2f}% of total volume capped\")\n",
        "\n",
        "SalesForecastModelV2.cap_weekly_outliers = cap_weekly_outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build enhanced features including temporal, lag, trend, and interaction features\n",
        "def build_features_v2(self):\n",
        "    print(\"\\nBuilding enhanced features...\")\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.sort_values(['internal_store_id', 'internal_product_id', 'week'])\n",
        "    \n",
        "    print(\"Creating temporal features...\")\n",
        "    self.weekly_data['week_sin'] = np.sin(2 * np.pi * self.weekly_data['week'] / 52)\n",
        "    self.weekly_data['week_cos'] = np.cos(2 * np.pi * self.weekly_data['week'] / 52)\n",
        "    self.weekly_data['month_sin'] = np.sin(2 * np.pi * self.weekly_data['month'] / 12)\n",
        "    self.weekly_data['month_cos'] = np.cos(2 * np.pi * self.weekly_data['month'] / 12)\n",
        "    \n",
        "    print(\"Creating trend features...\")\n",
        "    grouped = self.weekly_data.groupby(['internal_store_id', 'internal_product_id'])\n",
        "    \n",
        "    self.weekly_data['quantity_lag_1'] = grouped['total_quantity'].shift(1)\n",
        "    self.weekly_data['quantity_lag_2'] = grouped['total_quantity'].shift(2)\n",
        "    \n",
        "    for window in [2, 4]:\n",
        "        past_total = grouped['total_quantity'].shift(1)\n",
        "        self.weekly_data[f'quantity_rolling_avg_{window}'] = past_total.rolling(window=window, min_periods=1).mean()\n",
        "    \n",
        "    self.weekly_data['quantity_trend_2w'] = (self.weekly_data['quantity_lag_1'] - self.weekly_data['quantity_lag_2']) / (self.weekly_data['quantity_lag_2'] + 1)\n",
        "    self.weekly_data['quantity_lag_3'] = grouped['total_quantity'].shift(3)\n",
        "    self.weekly_data['quantity_growth_rate'] = (self.weekly_data['quantity_lag_1'] - self.weekly_data['quantity_lag_3']) / (self.weekly_data['quantity_lag_3'] + 1)\n",
        "    \n",
        "    print(\"Creating lifecycle features...\")\n",
        "    first_sale = grouped['week'].transform('min')\n",
        "    last_sale = grouped['week'].transform('max')\n",
        "    self.weekly_data['weeks_since_first_sale'] = self.weekly_data['week'] - first_sale\n",
        "    self.weekly_data['weeks_since_last_sale'] = self.weekly_data['week'] - last_sale\n",
        "    \n",
        "    print(\"Creating store-product interaction features...\")\n",
        "    store_product_stats = self.weekly_data.groupby(['internal_store_id', 'internal_product_id']).agg({\n",
        "        'total_quantity': ['mean', 'std', 'count'],\n",
        "        'num_transactions': 'mean'\n",
        "    })\n",
        "    \n",
        "    store_product_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in store_product_stats.columns.values]\n",
        "    store_product_stats.rename(columns={\n",
        "        'total_quantity_mean': 'store_product_avg_quantity',\n",
        "        'total_quantity_std': 'store_product_std_quantity',\n",
        "        'total_quantity_count': 'store_product_weeks_active',\n",
        "        'num_transactions_mean': 'store_product_avg_transactions'\n",
        "    }, inplace=True)\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.merge(\n",
        "        store_product_stats,\n",
        "        left_on=['internal_store_id', 'internal_product_id'],\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    print(\"Creating category performance features...\")\n",
        "    self.weekly_data = self.weekly_data.sort_values(['categoria', 'week'])\n",
        "    self.weekly_data['category_weekly_avg'] = (\n",
        "        self.weekly_data.groupby('categoria')['total_quantity']\n",
        "        .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.sort_values(['internal_store_id', 'week'])\n",
        "    self.weekly_data['store_weekly_avg'] = (\n",
        "        self.weekly_data.groupby('internal_store_id')['total_quantity']\n",
        "        .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "    \n",
        "    self.weekly_data = self.weekly_data.fillna(0)\n",
        "    print(\"Enhanced feature engineering completed\")\n",
        "\n",
        "SalesForecastModelV2.build_features_v2 = build_features_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with feature selection and target transformation\n",
        "def prepare_training_data_v2(self):\n",
        "    print(\"\\nPreparing training data...\")\n",
        "    \n",
        "    categorical_features = ['categoria', 'marca', 'premise', 'categoria_pdv']\n",
        "    \n",
        "    # Keep original categorical columns for CatBoost native handling\n",
        "    for feature in categorical_features:\n",
        "        self.weekly_data[feature] = self.weekly_data[feature].astype(str)\n",
        "    \n",
        "    feature_columns = [\n",
        "        'week', 'month', 'quarter',\n",
        "        'week_sin', 'week_cos', 'month_sin', 'month_cos',\n",
        "        'num_transactions', 'avg_quantity_per_transaction',\n",
        "        'total_gross_value', 'avg_gross_value',\n",
        "        'total_net_value', 'avg_net_value',\n",
        "        'total_gross_profit', 'avg_gross_profit',\n",
        "        'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3',\n",
        "        'quantity_rolling_avg_2', 'quantity_rolling_avg_4',\n",
        "        'quantity_trend_2w', 'quantity_growth_rate',\n",
        "        'weeks_since_first_sale', 'weeks_since_last_sale',\n",
        "        'store_product_avg_quantity', 'store_product_std_quantity',\n",
        "        'store_product_weeks_active', 'store_product_avg_transactions',\n",
        "        'category_weekly_avg', 'store_weekly_avg'\n",
        "    ] + categorical_features\n",
        "    \n",
        "    self.categorical_features = categorical_features\n",
        "    \n",
        "    train_data = self.weekly_data[self.weekly_data['week'] >= 5].copy()\n",
        "    \n",
        "    X = train_data[feature_columns]\n",
        "    y = train_data['total_quantity']\n",
        "    \n",
        "    if self.target_log_transformed:\n",
        "        y = np.log1p(y)\n",
        "        print(\"Applied log1p transformation to target\")\n",
        "    \n",
        "    print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
        "    \n",
        "    return X, y, feature_columns, train_data\n",
        "\n",
        "SalesForecastModelV2.prepare_training_data_v2 = prepare_training_data_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Weighted Mean Absolute Percentage Error (WMAPE) and create time-series split\n",
        "def calculate_wmape(self, y_true, y_pred):\n",
        "    if self.target_log_transformed:\n",
        "        y_true = np.expm1(y_true)\n",
        "        y_pred = np.expm1(y_pred)\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "    denom = np.sum(y_true)\n",
        "    return 0.0 if denom == 0 else np.sum(np.abs(y_true - y_pred)) / denom * 100\n",
        "\n",
        "def time_series_split(self, X, y, test_weeks=6):\n",
        "    train_data = self.weekly_data[self.weekly_data['week'] >= 5].copy()\n",
        "    max_week = train_data['week'].max()\n",
        "    split_week = max_week - test_weeks + 1\n",
        "    \n",
        "    train_mask = train_data['week'] < split_week\n",
        "    val_mask = train_data['week'] >= split_week\n",
        "    \n",
        "    X_train = X[train_mask]\n",
        "    X_val = X[val_mask]\n",
        "    y_train = y[train_mask]\n",
        "    y_val = y[val_mask]\n",
        "    \n",
        "    print(f\"Time-based split: Train weeks 5-{split_week-1}, Validation weeks {split_week}-{max_week}\")\n",
        "    print(f\"Train samples: {len(X_train):,}, Validation samples: {len(X_val):,}\")\n",
        "    \n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "SalesForecastModelV2.calculate_wmape = calculate_wmape\n",
        "SalesForecastModelV2.time_series_split = time_series_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost model with GPU/multicore acceleration and optimized hyperparameters\n",
        "def train_model_v2(self, X, y, optimize_hyperparams=False, n_trials=150, use_gpu=True):\n",
        "    print(\"\\nTraining enhanced CatBoost model...\")\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = self.time_series_split(X, y)\n",
        "    \n",
        "    # Check GPU availability\n",
        "    gpu_available = False\n",
        "    if use_gpu:\n",
        "        try:\n",
        "            # Try creating a small GPU model to test\n",
        "            test_model = CatBoostRegressor(iterations=1, task_type=\"GPU\", verbose=False)\n",
        "            test_model.fit(X_train.head(100), y_train.head(100), verbose=False)\n",
        "            gpu_available = True\n",
        "            print(\"GPU acceleration available and enabled\")\n",
        "        except Exception as e:\n",
        "            print(f\"GPU not available: {str(e)}\")\n",
        "            gpu_available = False\n",
        "    \n",
        "    # Get optimal thread count for CPU\n",
        "    import os\n",
        "    n_threads = os.cpu_count()\n",
        "    print(f\"🔧 Using {n_threads} CPU threads\")\n",
        "    \n",
        "    model_params = {\n",
        "        'iterations': 1500,\n",
        "        'learning_rate': 0.03,\n",
        "        'depth': 8,\n",
        "        'l2_leaf_reg': 15,\n",
        "        'bagging_temperature': 0.8,\n",
        "        'random_strength': 1.5,\n",
        "        'border_count': 200,\n",
        "        'loss_function': 'MAE',\n",
        "        'eval_metric': 'MAE',\n",
        "        'random_seed': 42,\n",
        "        'verbose': 100,\n",
        "        'early_stopping_rounds': 200,\n",
        "        'thread_count': n_threads,  # Use all available CPU cores\n",
        "    }\n",
        "    \n",
        "    # Add GPU-specific parameters if available\n",
        "    if gpu_available and use_gpu:\n",
        "        model_params.update({\n",
        "            'task_type': 'GPU',\n",
        "            'devices': '0',  # Use first GPU\n",
        "            'gpu_ram_part': 0.5,  # Use 50% of GPU memory\n",
        "        })\n",
        "        print(\"GPU acceleration enabled\")\n",
        "    else:\n",
        "        print(\"Using CPU training with multicore support\")\n",
        "    \n",
        "    print(f\"Using parameters: {model_params}\")\n",
        "    self.model = CatBoostRegressor(**model_params)\n",
        "    \n",
        "    cat_feature_indices = [X_train.columns.get_loc(col) for col in self.categorical_features if col in X_train.columns]\n",
        "    \n",
        "    self.model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        cat_features=cat_feature_indices,\n",
        "        early_stopping_rounds=150,\n",
        "        verbose=100,\n",
        "        use_best_model=True\n",
        "    )\n",
        "    \n",
        "    self.evaluate_model_v2(X_train, X_val, y_train, y_val)\n",
        "\n",
        "SalesForecastModelV2.train_model_v2 = train_model_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance on training and validation sets\n",
        "def evaluate_model_v2(self, X_train, X_val, y_train, y_val):\n",
        "    train_pred = self.model.predict(X_train)\n",
        "    val_pred = self.model.predict(X_val)\n",
        "    \n",
        "    train_wmape = self.calculate_wmape(y_train, train_pred)\n",
        "    val_wmape = self.calculate_wmape(y_val, val_pred)\n",
        "    \n",
        "    if self.target_log_transformed:\n",
        "        y_train_orig = np.expm1(y_train)\n",
        "        y_val_orig = np.expm1(y_val)\n",
        "        train_pred_orig = np.expm1(train_pred)\n",
        "        val_pred_orig = np.expm1(val_pred)\n",
        "    else:\n",
        "        y_train_orig = y_train\n",
        "        y_val_orig = y_val\n",
        "        train_pred_orig = train_pred\n",
        "        val_pred_orig = val_pred\n",
        "    \n",
        "    train_pred_orig = np.maximum(0, train_pred_orig)\n",
        "    val_pred_orig = np.maximum(0, val_pred_orig)\n",
        "    \n",
        "    train_mape = mean_absolute_percentage_error(y_train_orig, train_pred_orig) * 100\n",
        "    val_mape = mean_absolute_percentage_error(y_val_orig, val_pred_orig) * 100\n",
        "    \n",
        "    self.validation_metrics = {\n",
        "        'train_mape': train_mape,\n",
        "        'val_mape': val_mape,\n",
        "        'train_wmape': train_wmape,\n",
        "        'val_wmape': val_wmape,\n",
        "        'train_samples': len(y_train),\n",
        "        'val_samples': len(y_val)\n",
        "    }\n",
        "    \n",
        "    print(f\"Training MAPE: {train_mape:.2f}%\")\n",
        "    print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
        "    print(f\"Training WMAPE: {train_wmape:.2f}%\")\n",
        "    print(f\"Validation WMAPE: {val_wmape:.2f}%\")\n",
        "    \n",
        "    return self.validation_metrics\n",
        "\n",
        "SalesForecastModelV2.evaluate_model_v2 = evaluate_model_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost model optimized for Apple M4 with maximum CPU performance\n",
        "def train_model_v2(self, X, y, optimize_hyperparams=False, n_trials=150, use_gpu=True):\n",
        "    print(\"\\nTraining enhanced CatBoost model...\")\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = self.time_series_split(X, y)\n",
        "    \n",
        "    # System detection and optimization\n",
        "    import platform\n",
        "    import os\n",
        "    system_info = platform.system()\n",
        "    machine_info = platform.machine()\n",
        "    print(f\"System: {system_info} {machine_info}\")\n",
        "    \n",
        "    # GPU availability check (CatBoost supports NVIDIA CUDA only)\n",
        "    gpu_available = False\n",
        "    if use_gpu:\n",
        "        try:\n",
        "            test_model = CatBoostRegressor(iterations=1, task_type=\"GPU\", verbose=False)\n",
        "            test_model.fit(X_train.head(100), y_train.head(100), verbose=False)\n",
        "            gpu_available = True\n",
        "            print(\"NVIDIA GPU acceleration enabled\")\n",
        "        except Exception:\n",
        "            print(\"CUDA GPU not available - using optimized CPU training\")\n",
        "            print(\"Note: Apple Silicon GPU cores not supported by CatBoost\")\n",
        "            gpu_available = False\n",
        "    \n",
        "    # Thread optimization for Apple Silicon\n",
        "    n_threads = os.cpu_count()\n",
        "    if system_info == \"Darwin\" and (\"arm64\" in machine_info or \"arm\" in machine_info):\n",
        "        print(\"Apple Silicon detected - optimizing thread allocation\")\n",
        "        n_threads = max(1, int(n_threads * 0.9))  # Reserve some cores for system\n",
        "    \n",
        "    print(f\"Using {n_threads} CPU threads\")\n",
        "    \n",
        "    # Optimized model parameters for Apple Silicon\n",
        "    model_params = {\n",
        "        'iterations': 200,\n",
        "        'learning_rate': 0.03,\n",
        "        'depth': 8,\n",
        "        'l2_leaf_reg': 15,\n",
        "        'bootstrap_type': 'Bayesian',  # Fixed: Compatible with bagging_temperature\n",
        "        'bagging_temperature': 0.8,\n",
        "        'random_strength': 1.5,\n",
        "        'border_count': 200,\n",
        "        'loss_function': 'MAE',\n",
        "        'eval_metric': 'MAE',\n",
        "        'random_seed': 42,\n",
        "        'verbose': 100,\n",
        "        'early_stopping_rounds': 200,\n",
        "        'thread_count': n_threads,\n",
        "        'used_ram_limit': '64GB',  # Increased for M4 systems\n",
        "    }\n",
        "    \n",
        "    # GPU parameters if available\n",
        "    if gpu_available and use_gpu:\n",
        "        model_params.update({\n",
        "            'task_type': 'GPU',\n",
        "            'devices': '0',\n",
        "            'gpu_ram_part': 0.5,\n",
        "        })\n",
        "        print(\"GPU acceleration enabled\")\n",
        "    else:\n",
        "        print(\"CPU training optimized for Apple Silicon\")\n",
        "    \n",
        "    print(f\"Model parameters: {model_params}\")\n",
        "    self.model = CatBoostRegressor(**model_params)\n",
        "    \n",
        "    cat_feature_indices = [X_train.columns.get_loc(col) for col in self.categorical_features if col in X_train.columns]\n",
        "    \n",
        "    print(\"Starting model training...\")\n",
        "    self.model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        cat_features=cat_feature_indices,\n",
        "        early_stopping_rounds=150,\n",
        "        verbose=100,\n",
        "        use_best_model=True\n",
        "    )\n",
        "    \n",
        "    self.evaluate_model_v2(X_train, X_val, y_train, y_val)\n",
        "\n",
        "SalesForecastModelV2.train_model_v2 = train_model_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze prediction distribution, save predictions, and print performance report\n",
        "def analyze_predictions(self, predictions_df):\n",
        "    print(\"\\nPrediction Analysis:\")\n",
        "    print(f\"Total predictions: {len(predictions_df):,}\")\n",
        "    print(f\"Zero predictions: {(predictions_df['quantidade'] == 0).sum():,}\")\n",
        "    print(f\"Non-zero predictions: {(predictions_df['quantidade'] > 0).sum():,}\")\n",
        "    print(f\"Mean prediction: {predictions_df['quantidade'].mean():.2f}\")\n",
        "    print(f\"Median prediction: {predictions_df['quantidade'].median():.2f}\")\n",
        "    print(f\"Max prediction: {predictions_df['quantidade'].max():,}\")\n",
        "    print(f\"Std prediction: {predictions_df['quantidade'].std():.2f}\")\n",
        "    \n",
        "    quantiles = predictions_df['quantidade'].quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
        "    print(\"Prediction quantiles:\")\n",
        "    for q, val in quantiles.items():\n",
        "        print(f\"  {q*100:.0f}%: {val:.2f}\")\n",
        "    \n",
        "    weekly_stats = predictions_df.groupby('semana')['quantidade'].agg(['count', 'mean', 'sum']).round(2)\n",
        "    print(\"\\nWeekly prediction summary:\")\n",
        "    print(weekly_stats)\n",
        "\n",
        "def analyze_daily_predictions(self, daily_predictions_df):\n",
        "    \"\"\"Analyze daily prediction patterns\"\"\"\n",
        "    print(\"\\nDaily Prediction Analysis:\")\n",
        "    print(f\"Total daily predictions: {len(daily_predictions_df):,}\")\n",
        "    print(f\"Date range: {daily_predictions_df['date'].min()} to {daily_predictions_df['date'].max()}\")\n",
        "    \n",
        "    # Day-of-week analysis\n",
        "    dow_stats = daily_predictions_df.groupby('day_of_week')['quantidade_daily'].agg(['count', 'mean', 'sum']).round(2)\n",
        "    dow_stats.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    print(\"\\nDay-of-week patterns:\")\n",
        "    print(dow_stats)\n",
        "    \n",
        "    # Weekly aggregation check\n",
        "    weekly_check = daily_predictions_df.groupby(['week', 'pdv', 'produto'])['quantidade_daily'].sum().reset_index()\n",
        "    print(f\"\\nWeekly aggregation check:\")\n",
        "    print(f\"Average weekly quantity per store-product: {weekly_check['quantidade_daily'].mean():.2f}\")\n",
        "    print(f\"Max weekly quantity: {weekly_check['quantidade_daily'].max():.2f}\")\n",
        "\n",
        "def save_predictions(self, predictions_df, filename=\"sales_predictions_v2.csv\"):\n",
        "    print(f\"\\nSaving predictions to {filename}...\")\n",
        "    predictions_df.to_csv(filename, sep=';', index=False, encoding='utf-8')\n",
        "    \n",
        "    parquet_filename = filename.replace('.csv', '.parquet')\n",
        "    predictions_df.to_parquet(parquet_filename, index=False)\n",
        "    \n",
        "    print(f\"Predictions saved successfully!\")\n",
        "    print(f\"CSV File: {filename}\")\n",
        "    print(f\"Parquet File: {parquet_filename}\")\n",
        "    print(f\"Rows: {len(predictions_df):,}\")\n",
        "    print(f\"Sample:\")\n",
        "    print(predictions_df.head(10))\n",
        "    \n",
        "    # Save daily predictions if available\n",
        "    if hasattr(self, 'daily_predictions') and self.daily_predictions is not None:\n",
        "        daily_filename = filename.replace('.csv', '_daily.csv')\n",
        "        daily_parquet = filename.replace('.csv', '_daily.parquet')\n",
        "        \n",
        "        print(f\"\\nSaving daily predictions to {daily_filename}...\")\n",
        "        self.daily_predictions.to_csv(daily_filename, sep=';', index=False, encoding='utf-8')\n",
        "        self.daily_predictions.to_parquet(daily_parquet, index=False)\n",
        "        \n",
        "        print(f\"Daily predictions saved!\")\n",
        "        print(f\"Daily CSV: {daily_filename}\")\n",
        "        print(f\"Daily Parquet: {daily_parquet}\")\n",
        "        print(f\"Daily rows: {len(self.daily_predictions):,}\")\n",
        "        \n",
        "        # Analyze daily predictions\n",
        "        self.analyze_daily_predictions(self.daily_predictions)\n",
        "\n",
        "def print_performance_report(self):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMANCE REPORT V2 - DAILY AGGREGATION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    if self.validation_metrics:\n",
        "        print(\"Model Validation Metrics:\")\n",
        "        print(f\"  Training MAPE: {self.validation_metrics['train_mape']:.2f}%\")\n",
        "        print(f\"  Validation MAPE: {self.validation_metrics['val_mape']:.2f}%\")\n",
        "        print(f\"  Training WMAPE: {self.validation_metrics['train_wmape']:.2f}%\")\n",
        "        print(f\"  Validation WMAPE: {self.validation_metrics['val_wmape']:.2f}%\")\n",
        "        print(f\"  Training samples: {self.validation_metrics['train_samples']:,}\")\n",
        "        print(f\"  Validation samples: {self.validation_metrics['val_samples']:,}\")\n",
        "        \n",
        "        wmape_diff = abs(self.validation_metrics['val_wmape'] - self.validation_metrics['train_wmape'])\n",
        "        mape_diff = abs(self.validation_metrics['val_mape'] - self.validation_metrics['train_mape'])\n",
        "        \n",
        "        print(f\"\\nOverfitting Check:\")\n",
        "        print(f\"  MAPE difference: {mape_diff:.2f}%\")\n",
        "        print(f\"  WMAPE difference: {wmape_diff:.2f}%\")\n",
        "        \n",
        "        if wmape_diff < 2 and mape_diff < 10:\n",
        "            print(\"  Status: Good generalization\")\n",
        "        elif wmape_diff < 5 and mape_diff < 20:\n",
        "            print(\"  Status: Moderate overfitting\")\n",
        "        else:\n",
        "            print(\"  Status: High overfitting risk\")\n",
        "    \n",
        "    print(\"\\nPrediction Approach:\")\n",
        "    print(\"  ✓ Daily predictions generated with day-of-week patterns\")\n",
        "    print(\"  ✓ Aggregated to weekly predictions for final output\")\n",
        "    print(\"  ✓ Enhanced temporal granularity for better accuracy\")\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "\n",
        "SalesForecastModelV2.analyze_predictions = analyze_predictions\n",
        "SalesForecastModelV2.analyze_daily_predictions = analyze_daily_predictions\n",
        "SalesForecastModelV2.save_predictions = save_predictions\n",
        "SalesForecastModelV2.print_performance_report = print_performance_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate daily predictions for January 2023 (up to day 31), then aggregate to weekly\n",
        "def generate_daily_predictions_v2(self, feature_columns, train_data, max_rows=1500000, weeks_to_predict=5, recent_weeks=8):\n",
        "    print(\"\\nGenerating daily predictions for January 2023 (up to day 31)...\")\n",
        "    \n",
        "    # Calculate total days for January 2023 (31 days)\n",
        "    days_to_predict = 30  # January 2-31 (30 days, starting from Jan 2)\n",
        "    \n",
        "    # Discover unique store-product combinations\n",
        "    store_product_combinations = train_data[['internal_store_id', 'internal_product_id']].drop_duplicates()\n",
        "    print(f\"Found {len(store_product_combinations):,} unique store-product combinations\")\n",
        "    \n",
        "    # Build latest snapshot per pair with all features\n",
        "    latest_records = train_data.loc[train_data.groupby(['internal_store_id', 'internal_product_id'])['week'].idxmax()].copy()\n",
        "    \n",
        "    # Select top active pairs to respect the portal limit (accounting for 30 days in January)\n",
        "    try:\n",
        "        pairs_limit = max_rows // days_to_predict\n",
        "        max_week = int(self.weekly_data['week'].max())\n",
        "        start_week = max(1, max_week - int(recent_weeks) + 1)\n",
        "        recent_slice = self.weekly_data[self.weekly_data['week'] >= start_week]\n",
        "        activity = recent_slice.groupby(['internal_store_id', 'internal_product_id']).agg(\n",
        "            recent_total_qty=('total_quantity', 'sum'),\n",
        "            weeks_with_sales=('total_quantity', lambda s: int((s > 0).sum())),\n",
        "            last_week_seen=('week', 'max')\n",
        "        ).reset_index()\n",
        "        activity = activity.sort_values(\n",
        "            by=['recent_total_qty', 'weeks_with_sales', 'last_week_seen'],\n",
        "            ascending=[False, False, False]\n",
        "        )\n",
        "        selected_pairs = activity.head(pairs_limit)[['internal_store_id', 'internal_product_id']]\n",
        "        before = len(latest_records)\n",
        "        latest_records = latest_records.merge(selected_pairs, on=['internal_store_id', 'internal_product_id'], how='inner')\n",
        "        after = len(latest_records)\n",
        "        print(f\"Selected top {after:,} active pairs out of {before:,} (recent_weeks={recent_weeks})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Pair selection step skipped due to error: {e}\")\n",
        "    \n",
        "    all_daily_predictions = []\n",
        "    \n",
        "    # Generate predictions for each day in January 2023 (Jan 2-31)\n",
        "    import datetime\n",
        "    start_date = datetime.date(2023, 1, 2)  # Start from Monday, January 2, 2023\n",
        "    end_date = datetime.date(2023, 1, 31)   # End on January 31, 2023\n",
        "    \n",
        "    current_date = start_date\n",
        "    day_count = 0\n",
        "    \n",
        "    while current_date <= end_date:\n",
        "        day_count += 1\n",
        "        week_num = current_date.isocalendar()[1]  # ISO week number\n",
        "        day_of_week = current_date.weekday() + 1  # Monday=1, Sunday=7\n",
        "        \n",
        "        print(f\"Predicting day {day_count} ({current_date}, Week {week_num}, Day {day_of_week})...\")\n",
        "        \n",
        "        day_data = latest_records.copy()\n",
        "        \n",
        "        # Update temporal features for the prediction day\n",
        "        day_data['week'] = week_num\n",
        "        day_data['month'] = current_date.month\n",
        "        day_data['quarter'] = (current_date.month - 1) // 3 + 1\n",
        "        day_data['day_of_week'] = day_of_week\n",
        "        \n",
        "        # Update seasonal features\n",
        "        day_data['week_sin'] = np.sin(2 * np.pi * week_num / 52)\n",
        "        day_data['week_cos'] = np.cos(2 * np.pi * week_num / 52)\n",
        "        day_data['month_sin'] = np.sin(2 * np.pi * current_date.month / 12)\n",
        "        day_data['month_cos'] = np.cos(2 * np.pi * current_date.month / 12)\n",
        "        \n",
        "        # Add day-of-week cyclical features\n",
        "        day_data['day_sin'] = np.sin(2 * np.pi * day_of_week / 7)\n",
        "        day_data['day_cos'] = np.cos(2 * np.pi * day_of_week / 7)\n",
        "        \n",
        "        # Update lifecycle features\n",
        "        day_data['weeks_since_last_sale'] = 0  # Assume continuing sales\n",
        "        \n",
        "        # Ensure all required features are present\n",
        "        missing_features = [f for f in feature_columns if f not in day_data.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features for prediction: {missing_features}\")\n",
        "            # Fill missing features with zeros or appropriate defaults\n",
        "            for feature in missing_features:\n",
        "                day_data[feature] = 0\n",
        "        \n",
        "        # Add day-of-week features to feature columns if not present\n",
        "        extended_features = feature_columns.copy()\n",
        "        if 'day_sin' not in extended_features:\n",
        "            extended_features.extend(['day_sin', 'day_cos', 'day_of_week'])\n",
        "        \n",
        "        # Prepare feature matrix\n",
        "        try:\n",
        "            # Use only available features\n",
        "            available_features = [f for f in extended_features if f in day_data.columns]\n",
        "            features_matrix = day_data[available_features]\n",
        "            \n",
        "            # Make predictions (weekly model predicts weekly quantity)\n",
        "            weekly_predictions = self.model.predict(features_matrix)\n",
        "            \n",
        "            # Convert weekly predictions to daily by dividing by 7 and adjusting for day-of-week patterns\n",
        "            # Apply day-of-week adjustment factors (weekdays typically higher than weekends)\n",
        "            day_adjustment = {1: 1.2, 2: 1.2, 3: 1.2, 4: 1.2, 5: 1.1, 6: 0.8, 7: 0.6}  # Mon-Sun\n",
        "            daily_factor = day_adjustment[day_of_week] / 7.0\n",
        "            \n",
        "            daily_predictions = weekly_predictions * daily_factor\n",
        "            \n",
        "            # Handle log transformation if used\n",
        "            if self.target_log_transformed:\n",
        "                daily_predictions = np.expm1(daily_predictions)\n",
        "            \n",
        "            # Ensure non-negative predictions\n",
        "            daily_predictions = np.maximum(0, daily_predictions)\n",
        "            \n",
        "            # Create prediction dataframe\n",
        "            day_predictions = pd.DataFrame({\n",
        "                'date': current_date,\n",
        "                'week': week_num,\n",
        "                'day_of_week': day_of_week,\n",
        "                'pdv': day_data['internal_store_id'].astype(int),\n",
        "                'produto': day_data['internal_product_id'].astype(int),\n",
        "                'quantidade_daily': daily_predictions\n",
        "            })\n",
        "            \n",
        "            all_daily_predictions.append(day_predictions)\n",
        "            print(f\"Generated {len(day_predictions):,} daily predictions for {current_date}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error generating predictions for day {day_count} ({current_date}): {e}\")\n",
        "            # Create fallback predictions with zeros\n",
        "            day_predictions = pd.DataFrame({\n",
        "                'date': current_date,\n",
        "                'week': week_num,\n",
        "                'day_of_week': day_of_week,\n",
        "                'pdv': day_data['internal_store_id'].astype(int),\n",
        "                'produto': day_data['internal_product_id'].astype(int),\n",
        "                'quantidade_daily': 0.0\n",
        "            })\n",
        "            all_daily_predictions.append(day_predictions)\n",
        "        \n",
        "        # Move to next day\n",
        "        current_date += datetime.timedelta(days=1)\n",
        "    \n",
        "    # Combine all daily predictions\n",
        "    daily_predictions_df = pd.concat(all_daily_predictions, ignore_index=True)\n",
        "    \n",
        "    # Aggregate daily predictions to weekly (only for January weeks)\n",
        "    print(f\"\\nAggregating daily predictions to weekly (January 2023 only)...\")\n",
        "    weekly_predictions_df = daily_predictions_df.groupby(['week', 'pdv', 'produto']).agg({\n",
        "        'quantidade_daily': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Rename for consistency with original format\n",
        "    weekly_predictions_df = weekly_predictions_df.rename(columns={\n",
        "        'week': 'semana',\n",
        "        'quantidade_daily': 'quantidade'\n",
        "    })\n",
        "    \n",
        "    # Round to integers\n",
        "    weekly_predictions_df['quantidade'] = weekly_predictions_df['quantidade'].round().astype(int)\n",
        "    \n",
        "    # Final safeguard for row limit\n",
        "    if len(weekly_predictions_df) > max_rows:\n",
        "        print(f\"Limiting predictions to {max_rows:,} rows (was {len(weekly_predictions_df):,})\")\n",
        "        weekly_predictions_df = weekly_predictions_df.head(max_rows)\n",
        "    \n",
        "    print(f\"Total daily predictions generated: {len(daily_predictions_df):,} (Jan 2-31, 2023)\")\n",
        "    print(f\"Total weekly predictions after aggregation: {len(weekly_predictions_df):,}\")\n",
        "    print(f\"Date range: {daily_predictions_df['date'].min()} to {daily_predictions_df['date'].max()}\")\n",
        "    \n",
        "    # Get unique weeks covered\n",
        "    unique_weeks = sorted(daily_predictions_df['week'].unique())\n",
        "    print(f\"Weeks covered: {unique_weeks}\")\n",
        "    \n",
        "    # Store daily predictions for analysis\n",
        "    self.daily_predictions = daily_predictions_df\n",
        "    \n",
        "    self.analyze_predictions(weekly_predictions_df)\n",
        "    return weekly_predictions_df\n",
        "\n",
        "# Updated generate_predictions_v2 to use daily approach (January only)\n",
        "def generate_predictions_v2(self, feature_columns, train_data, max_rows=1500000, weeks_to_predict=5, recent_weeks=8):\n",
        "    \"\"\"Generate predictions using daily approach for January 2023 only, then aggregate to weekly\"\"\"\n",
        "    return self.generate_daily_predictions_v2(feature_columns, train_data, max_rows, weeks_to_predict, recent_weeks)\n",
        "\n",
        "SalesForecastModelV2.generate_daily_predictions_v2 = generate_daily_predictions_v2\n",
        "SalesForecastModelV2.generate_predictions_v2 = generate_predictions_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the complete pipeline: data loading, processing, training, and prediction\n",
        "def run_complete_pipeline(self):\n",
        "    print(\"=== Sales Forecast Model V2 - Enhanced with Daily Predictions ===\")\n",
        "    \n",
        "    self.load_data()\n",
        "    self.cleanse_data()\n",
        "    self.merge_data()\n",
        "    self.create_weekly_aggregations()\n",
        "    self.build_features_v2()\n",
        "    \n",
        "    X, y, feature_columns, train_data = self.prepare_training_data_v2()\n",
        "    self.train_model_v2(X, y, optimize_hyperparams=False, n_trials=150)\n",
        "    \n",
        "    # Generate daily predictions and aggregate to weekly\n",
        "    predictions_df = self.generate_predictions_v2(feature_columns, train_data)\n",
        "    \n",
        "    # Store weekly predictions for comprehensive export\n",
        "    self.weekly_predictions = predictions_df\n",
        "    \n",
        "    # Analyze daily patterns if available\n",
        "    self.analyze_daily_patterns()\n",
        "    \n",
        "    # Save predictions (standard format)\n",
        "    self.save_predictions(predictions_df)\n",
        "    \n",
        "    # Export comprehensive predictions (enhanced format)\n",
        "    self.export_all_predictions()\n",
        "    \n",
        "    self.print_performance_report()\n",
        "    \n",
        "    return predictions_df\n",
        "\n",
        "SalesForecastModelV2.run_complete_pipeline = run_complete_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze daily predictions if available\n",
        "def analyze_daily_patterns(self):\n",
        "    \"\"\"Analyze daily prediction patterns and day-of-week effects for January 2023\"\"\"\n",
        "    if hasattr(self, 'daily_predictions') and self.daily_predictions is not None:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"DAILY PREDICTION ANALYSIS - JANUARY 2023\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        df = self.daily_predictions\n",
        "        \n",
        "        # Overall daily statistics\n",
        "        print(f\"Total daily predictions: {len(df):,}\")\n",
        "        print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        print(f\"Days covered: {len(df['date'].unique())} days in January 2023\")\n",
        "        print(f\"Average daily quantity per store-product: {df['quantidade_daily'].mean():.2f}\")\n",
        "        \n",
        "        # Day-of-week analysis\n",
        "        dow_names = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', \n",
        "                     5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
        "        \n",
        "        dow_stats = df.groupby('day_of_week')['quantidade_daily'].agg([\n",
        "            'count', 'mean', 'sum', 'std'\n",
        "        ]).round(2)\n",
        "        dow_stats.index = [dow_names[i] for i in dow_stats.index]\n",
        "        \n",
        "        print(\"\\nDay-of-week patterns in January 2023:\")\n",
        "        print(dow_stats)\n",
        "        \n",
        "        # Calculate relative patterns (Monday = 1.0 baseline)\n",
        "        monday_avg = dow_stats.loc['Monday', 'mean']\n",
        "        if monday_avg > 0:\n",
        "            print(\"\\nRelative to Monday (1.0 = same as Monday):\")\n",
        "            for day in dow_stats.index:\n",
        "                relative = dow_stats.loc[day, 'mean'] / monday_avg\n",
        "                print(f\"  {day}: {relative:.2f}x\")\n",
        "        \n",
        "        # Weekly totals check for January weeks\n",
        "        weekly_totals = df.groupby(['week', 'pdv', 'produto'])['quantidade_daily'].sum()\n",
        "        unique_weeks = sorted(df['week'].unique())\n",
        "        \n",
        "        print(f\"\\nJanuary 2023 weekly aggregation verification:\")\n",
        "        print(f\"  Weeks covered: {unique_weeks}\")\n",
        "        print(f\"  Average weekly total per store-product: {weekly_totals.mean():.2f}\")\n",
        "        print(f\"  Max weekly total: {weekly_totals.max():.2f}\")\n",
        "        print(f\"  Min weekly total: {weekly_totals.min():.2f}\")\n",
        "        \n",
        "        # Show distribution across January weeks\n",
        "        week_summary = df.groupby('week').agg({\n",
        "            'date': ['min', 'max'],\n",
        "            'quantidade_daily': ['count', 'sum', 'mean']\n",
        "        }).round(2)\n",
        "        week_summary.columns = ['Week_Start', 'Week_End', 'Predictions_Count', 'Total_Quantity', 'Avg_Daily_Quantity']\n",
        "        print(f\"\\nWeek-by-week summary for January 2023:\")\n",
        "        print(week_summary)\n",
        "        \n",
        "        return df\n",
        "    else:\n",
        "        print(\"No daily predictions available for analysis\")\n",
        "        return None\n",
        "\n",
        "SalesForecastModelV2.analyze_daily_patterns = analyze_daily_patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sales Forecast Model V2 - Enhanced ===\n",
            "Loading data...\n",
            "Found 3 parquet files\n",
            "part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet: Shape (6560698, 11)\n",
            "-> Identified as TRANSACTIONS data\n",
            "part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet: Shape (7092, 8)\n",
            "-> Identified as PRODUCTS data\n",
            "part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet: Shape (14419, 4)\n",
            "-> Identified as STORES data\n",
            "Data loaded successfully:\n",
            "- Transactions: 6,560,698 rows\n",
            "- Products: 7,092 rows\n",
            "- Stores: 14,419 rows\n",
            "\n",
            "Cleansing data...\n",
            "Removed 0 rows with null values\n",
            "Kept 6,430,161 rows with positive quantities\n",
            "Removing extreme value outliers...\n",
            "Removed 94,398 outlier transactions (1.47%)\n",
            "Filtered to 2022 data: 6,335,763 rows\n",
            "Cleaning products and stores...\n",
            "\n",
            "Merging data...\n",
            "After product merge: 6,335,763 rows\n",
            "After store merge: 6,335,763 rows\n",
            "Data merge completed\n",
            "\n",
            "Creating weekly aggregations...\n",
            "Created weekly aggregations: 6,067,972 rows\n",
            "Capping weekly quantity outliers...\n",
            "  Before capping: max=360, total=31,721,805\n",
            "  After capping:  max=355, total=31,656,366\n",
            "  Reduction: 0.21% of total volume capped\n",
            "\n",
            "Building enhanced features...\n",
            "Creating temporal features...\n",
            "Creating trend features...\n",
            "Creating lifecycle features...\n",
            "Creating store-product interaction features...\n",
            "Creating category performance features...\n",
            "Enhanced feature engineering completed\n",
            "\n",
            "Preparing training data...\n",
            "Applied log1p transformation to target\n",
            "Training data shape: X=(5735913, 34), y=(5735913,)\n",
            "\n",
            "Training enhanced CatBoost model...\n",
            "Time-based split: Train weeks 5-46, Validation weeks 47-52\n",
            "Train samples: 5,065,293, Validation samples: 670,620\n",
            "System: Darwin arm64\n",
            "CUDA GPU not available - using optimized CPU training\n",
            "Note: Apple Silicon GPU cores not supported by CatBoost\n",
            "Apple Silicon detected - optimizing thread allocation\n",
            "Using 12 CPU threads\n",
            "CPU training optimized for Apple Silicon\n",
            "Model parameters: {'iterations': 1500, 'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 15, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.8, 'random_strength': 1.5, 'border_count': 200, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': 42, 'verbose': 100, 'early_stopping_rounds': 200, 'thread_count': 12, 'used_ram_limit': '12GB'}\n",
            "Starting model training...\n",
            "0:\tlearn: 0.5467547\ttest: 0.5375071\tbest: 0.5375071 (0)\ttotal: 898ms\tremaining: 22m 26s\n",
            "100:\tlearn: 0.0488974\ttest: 0.0440138\tbest: 0.0440138 (100)\ttotal: 1m 27s\tremaining: 20m 5s\n",
            "200:\tlearn: 0.0152278\ttest: 0.0145785\tbest: 0.0145785 (200)\ttotal: 2m 46s\tremaining: 17m 56s\n",
            "300:\tlearn: 0.0102263\ttest: 0.0101216\tbest: 0.0101216 (300)\ttotal: 3m 56s\tremaining: 15m 41s\n",
            "400:\tlearn: 0.0083957\ttest: 0.0085491\tbest: 0.0085491 (400)\ttotal: 5m 3s\tremaining: 13m 52s\n",
            "500:\tlearn: 0.0064862\ttest: 0.0068516\tbest: 0.0068516 (500)\ttotal: 6m 17s\tremaining: 12m 33s\n",
            "600:\tlearn: 0.0053854\ttest: 0.0058834\tbest: 0.0058834 (600)\ttotal: 7m 32s\tremaining: 11m 16s\n",
            "700:\tlearn: 0.0047492\ttest: 0.0052348\tbest: 0.0052348 (700)\ttotal: 8m 48s\tremaining: 10m 2s\n",
            "800:\tlearn: 0.0042891\ttest: 0.0047220\tbest: 0.0047220 (800)\ttotal: 10m 2s\tremaining: 8m 45s\n",
            "900:\tlearn: 0.0039492\ttest: 0.0043460\tbest: 0.0043460 (900)\ttotal: 11m 14s\tremaining: 7m 28s\n",
            "1000:\tlearn: 0.0036971\ttest: 0.0041133\tbest: 0.0041133 (1000)\ttotal: 12m 28s\tremaining: 6m 13s\n",
            "1100:\tlearn: 0.0034931\ttest: 0.0038956\tbest: 0.0038956 (1100)\ttotal: 13m 42s\tremaining: 4m 58s\n",
            "1200:\tlearn: 0.0033234\ttest: 0.0037270\tbest: 0.0037270 (1200)\ttotal: 14m 57s\tremaining: 3m 43s\n",
            "1300:\tlearn: 0.0031845\ttest: 0.0035843\tbest: 0.0035843 (1300)\ttotal: 16m 10s\tremaining: 2m 28s\n",
            "1400:\tlearn: 0.0030813\ttest: 0.0034822\tbest: 0.0034822 (1400)\ttotal: 17m 21s\tremaining: 1m 13s\n",
            "1499:\tlearn: 0.0029615\ttest: 0.0033572\tbest: 0.0033572 (1499)\ttotal: 18m 33s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.003357198766\n",
            "bestIteration = 1499\n",
            "\n",
            "Training MAPE: 55.73%\n",
            "Validation MAPE: 0.40%\n",
            "Training WMAPE: 0.98%\n",
            "Validation WMAPE: 1.11%\n",
            "\n",
            "Generating predictions for January 2023...\n",
            "Found 958,984 unique store-product combinations\n",
            "Selected top 300,000 active pairs out of 958,984 (recent_weeks=8)\n",
            "Predicting week 1...\n",
            "Generated 300,000 predictions for week 1\n",
            "Predicting week 2...\n",
            "Generated 300,000 predictions for week 2\n",
            "Predicting week 3...\n",
            "Generated 300,000 predictions for week 3\n",
            "Predicting week 4...\n",
            "Generated 300,000 predictions for week 4\n",
            "Predicting week 5...\n",
            "Generated 300,000 predictions for week 5\n",
            "Total predictions generated: 1,500,000\n",
            "\n",
            "Prediction Analysis:\n",
            "Total predictions: 1,500,000\n",
            "Zero predictions: 35\n",
            "Non-zero predictions: 1,499,965\n",
            "Mean prediction: 7.09\n",
            "Median prediction: 3.00\n",
            "Max prediction: 273\n",
            "Std prediction: 14.02\n",
            "Prediction quantiles:\n",
            "  25%: 1.00\n",
            "  50%: 3.00\n",
            "  75%: 7.00\n",
            "  90%: 13.00\n",
            "  95%: 24.00\n",
            "  99%: 72.00\n",
            "\n",
            "Weekly prediction summary:\n",
            "         count  mean      sum\n",
            "semana                       \n",
            "1       300000  7.10  2128821\n",
            "2       300000  7.11  2133236\n",
            "3       300000  7.08  2123604\n",
            "4       300000  7.08  2123524\n",
            "5       300000  7.08  2124843\n",
            "\n",
            "Saving predictions to sales_predictions_v2.csv...\n",
            "Predictions saved successfully!\n",
            "CSV File: sales_predictions_v2.csv\n",
            "Parquet File: sales_predictions_v2.parquet\n",
            "Rows: 1,500,000\n",
            "Sample:\n",
            "   semana                  pdv              produto  quantidade\n",
            "0       1  1000237487041964405   777251454728290683           4\n",
            "1       1  1001371918471115422  1009179103632945474           2\n",
            "2       1  1001371918471115422  1029370090212151375           4\n",
            "3       1  1001371918471115422  1120490062981954254          10\n",
            "4       1  1001371918471115422  1371936917923350372          10\n",
            "5       1  1001371918471115422  1394381856358939027          13\n",
            "6       1  1001371918471115422  1454838625590783593           5\n",
            "7       1  1001371918471115422   145852603040678098           1\n",
            "8       1  1001371918471115422  1527082310248040324           1\n",
            "9       1  1001371918471115422  1625722803643187564           1\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE REPORT V2\n",
            "==================================================\n",
            "Model Validation Metrics:\n",
            "  Training MAPE: 55.73%\n",
            "  Validation MAPE: 0.40%\n",
            "  Training WMAPE: 0.98%\n",
            "  Validation WMAPE: 1.11%\n",
            "  Training samples: 5,065,293\n",
            "  Validation samples: 670,620\n",
            "\n",
            "Overfitting Check:\n",
            "  MAPE difference: 55.34%\n",
            "  WMAPE difference: 0.12%\n",
            "  Status: High overfitting risk\n",
            "==================================================\n",
            "\n",
            "=== V2 Pipeline completed successfully! ===\n",
            "Generated 1,500,000 predictions for 5 weeks of January 2023\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the complete sales forecasting pipeline with comprehensive exports\n",
        "try:\n",
        "    model = SalesForecastModelV2()\n",
        "    predictions = model.run_complete_pipeline()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🎉 V2 PIPELINE WITH DAILY PREDICTIONS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"📊 Generated {len(predictions):,} weekly predictions from daily forecasts\")\n",
        "    \n",
        "    # Show detailed output summary\n",
        "    if hasattr(model, 'daily_predictions') and model.daily_predictions is not None:\n",
        "        daily_count = len(model.daily_predictions)\n",
        "        date_range = f\"{model.daily_predictions['date'].min()} to {model.daily_predictions['date'].max()}\"\n",
        "        unique_weeks = sorted(model.daily_predictions['week'].unique())\n",
        "        \n",
        "        print(f\"\\n📅 DAILY PREDICTIONS:\")\n",
        "        print(f\"   • {daily_count:,} daily predictions for January 2023 ({date_range})\")\n",
        "        print(f\"   • Covered weeks: {unique_weeks}\")\n",
        "        print(f\"   • Day-of-week patterns applied\")\n",
        "        \n",
        "        print(f\"\\n📈 WEEKLY AGGREGATIONS:\")\n",
        "        print(f\"   • {len(predictions):,} weekly predictions generated\")\n",
        "        print(f\"   • Aggregated from daily forecasts\")\n",
        "        print(f\"   • Ready for submission\")\n",
        "        \n",
        "        print(f\"\\n💾 EXPORTED FILES:\")\n",
        "        print(f\"   • sales_predictions_january_2023_weekly.csv (main submission)\")\n",
        "        print(f\"   • daily_predictions_january_2023.csv (detailed daily data)\")\n",
        "        print(f\"   • sales_predictions_january_2023_verification.csv (quality check)\")\n",
        "        print(f\"   • sales_predictions_january_2023_summary.txt (documentation)\")\n",
        "        \n",
        "        print(f\"\\n✅ FEATURES:\")\n",
        "        print(f\"   ✓ Daily predictions with day-of-week patterns for January 2-31, 2023\")\n",
        "        print(f\"   ✓ Aggregated to weekly format for final submission\")\n",
        "        print(f\"   ✓ Enhanced temporal granularity for better accuracy\")\n",
        "        print(f\"   ✓ Comprehensive export with verification and analysis\")\n",
        "        print(f\"   ✓ Multiple file formats (CSV, Parquet, Excel)\")\n",
        "    \n",
        "    print(f\"\\n🚀 Ready for submission and analysis!\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced function to export daily predictions with detailed formatting\n",
        "def export_daily_predictions(self, filename=\"daily_predictions_january_2023.csv\"):\n",
        "    \"\"\"Export daily predictions in a detailed format suitable for analysis\"\"\"\n",
        "    if not hasattr(self, 'daily_predictions') or self.daily_predictions is None:\n",
        "        print(\"No daily predictions available to export.\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EXPORTING DAILY PREDICTIONS FOR JANUARY 2023\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df = self.daily_predictions.copy()\n",
        "    \n",
        "    # Add more descriptive columns\n",
        "    df['year'] = 2023\n",
        "    df['month'] = 1\n",
        "    df['month_name'] = 'January'\n",
        "    \n",
        "    # Add day-of-week names\n",
        "    dow_names = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', \n",
        "                 5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
        "    df['day_name'] = df['day_of_week'].map(dow_names)\n",
        "    \n",
        "    # Round daily quantities for better readability\n",
        "    df['quantidade_daily_rounded'] = df['quantidade_daily'].round(2)\n",
        "    \n",
        "    # Reorder columns for better presentation\n",
        "    export_columns = [\n",
        "        'date', 'year', 'month', 'month_name', 'week', \n",
        "        'day_of_week', 'day_name', 'pdv', 'produto', \n",
        "        'quantidade_daily', 'quantidade_daily_rounded'\n",
        "    ]\n",
        "    \n",
        "    export_df = df[export_columns].copy()\n",
        "    \n",
        "    # Sort by date, store, and product for organized output\n",
        "    export_df = export_df.sort_values(['date', 'pdv', 'produto'])\n",
        "    \n",
        "    # Export to CSV\n",
        "    csv_filename = filename\n",
        "    parquet_filename = filename.replace('.csv', '.parquet')\n",
        "    excel_filename = filename.replace('.csv', '.xlsx')\n",
        "    \n",
        "    print(f\"Exporting daily predictions...\")\n",
        "    print(f\"📅 Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    print(f\"📊 Total records: {len(export_df):,}\")\n",
        "    print(f\"🏪 Unique stores: {df['pdv'].nunique():,}\")\n",
        "    print(f\"📦 Unique products: {df['produto'].nunique():,}\")\n",
        "    print(f\"📆 Days covered: {df['date'].nunique()} days in January 2023\")\n",
        "    \n",
        "    # Save CSV\n",
        "    export_df.to_csv(csv_filename, sep=';', index=False, encoding='utf-8')\n",
        "    print(f\"✅ CSV exported: {csv_filename}\")\n",
        "    \n",
        "    # Save Parquet\n",
        "    export_df.to_parquet(parquet_filename, index=False)\n",
        "    print(f\"✅ Parquet exported: {parquet_filename}\")\n",
        "    \n",
        "    # Save Excel (with error handling)\n",
        "    try:\n",
        "        export_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "        print(f\"✅ Excel exported: {excel_filename}\")\n",
        "    except ImportError:\n",
        "        print(\"⚠️  Excel export skipped (openpyxl not available)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Excel export failed: {str(e)}\")\n",
        "    \n",
        "    # Show sample data\n",
        "    print(f\"\\n📋 Sample of exported data (first 10 rows):\")\n",
        "    print(export_df.head(10).to_string(index=False))\n",
        "    \n",
        "    # Daily summary statistics\n",
        "    print(f\"\\n📈 Daily prediction summary:\")\n",
        "    daily_summary = export_df.groupby('date').agg({\n",
        "        'quantidade_daily': ['count', 'sum', 'mean', 'std']\n",
        "    }).round(2)\n",
        "    daily_summary.columns = ['Store_Product_Pairs', 'Total_Quantity', 'Avg_Quantity', 'Std_Quantity']\n",
        "    print(daily_summary.head())\n",
        "    \n",
        "    # Day-of-week summary\n",
        "    print(f\"\\n📊 Day-of-week summary:\")\n",
        "    dow_summary = export_df.groupby('day_name').agg({\n",
        "        'quantidade_daily': ['count', 'sum', 'mean']\n",
        "    }).round(2)\n",
        "    dow_summary.columns = ['Total_Predictions', 'Total_Quantity', 'Avg_Quantity']\n",
        "    # Reorder by weekday\n",
        "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    dow_summary = dow_summary.reindex([day for day in day_order if day in dow_summary.index])\n",
        "    print(dow_summary)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"✅ DAILY PREDICTIONS EXPORT COMPLETED\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return export_df\n",
        "\n",
        "# Add method to class\n",
        "SalesForecastModelV2.export_daily_predictions = export_daily_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to export both weekly and daily predictions with comprehensive formatting\n",
        "def export_all_predictions(self, base_filename=\"sales_predictions_january_2023\"):\n",
        "    \"\"\"Export both weekly aggregated and daily predictions with comprehensive analysis\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"COMPREHENSIVE PREDICTIONS EXPORT - JANUARY 2023\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Export weekly predictions (main submission format)\n",
        "    weekly_filename = f\"{base_filename}_weekly.csv\"\n",
        "    if hasattr(self, 'weekly_predictions') and self.weekly_predictions is not None:\n",
        "        weekly_df = self.weekly_predictions\n",
        "    else:\n",
        "        print(\"⚠️  No weekly predictions stored. Using last generated predictions.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n📊 WEEKLY PREDICTIONS EXPORT:\")\n",
        "    weekly_df.to_csv(weekly_filename, sep=';', index=False, encoding='utf-8')\n",
        "    weekly_df.to_parquet(weekly_filename.replace('.csv', '.parquet'), index=False)\n",
        "    \n",
        "    print(f\"✅ Weekly CSV: {weekly_filename}\")\n",
        "    print(f\"✅ Weekly Parquet: {weekly_filename.replace('.csv', '.parquet')}\")\n",
        "    print(f\"📋 Weekly records: {len(weekly_df):,}\")\n",
        "    print(f\"📅 Weeks covered: {sorted(weekly_df['semana'].unique())}\")\n",
        "    \n",
        "    # Export daily predictions with enhanced details\n",
        "    daily_filename = f\"{base_filename}_daily.csv\"\n",
        "    if hasattr(self, 'daily_predictions') and self.daily_predictions is not None:\n",
        "        print(f\"\\n📅 DAILY PREDICTIONS EXPORT:\")\n",
        "        daily_export = self.export_daily_predictions(daily_filename)\n",
        "        \n",
        "        # Create aggregation verification file\n",
        "        verification_filename = f\"{base_filename}_verification.csv\"\n",
        "        \n",
        "        if daily_export is not None:\n",
        "            # Verify aggregation by comparing daily sum to weekly totals\n",
        "            daily_to_weekly = daily_export.groupby(['week', 'pdv', 'produto']).agg({\n",
        "                'quantidade_daily': 'sum'\n",
        "            }).reset_index()\n",
        "            daily_to_weekly.rename(columns={'quantidade_daily': 'daily_sum'}, inplace=True)\n",
        "            \n",
        "            # Create verification report\n",
        "            verification_df = daily_to_weekly.merge(\n",
        "                weekly_df.rename(columns={'semana': 'week', 'quantidade': 'weekly_total'}),\n",
        "                on=['week', 'pdv', 'produto'],\n",
        "                how='outer',\n",
        "                suffixes=('_daily', '_weekly')\n",
        "            )\n",
        "            \n",
        "            verification_df['difference'] = verification_df['weekly_total'] - verification_df['daily_sum']\n",
        "            verification_df['match'] = abs(verification_df['difference']) < 0.01\n",
        "            \n",
        "            verification_df.to_csv(verification_filename, sep=';', index=False, encoding='utf-8')\n",
        "            \n",
        "            print(f\"\\n🔍 VERIFICATION REPORT:\")\n",
        "            print(f\"✅ Verification file: {verification_filename}\")\n",
        "            print(f\"📊 Records compared: {len(verification_df):,}\")\n",
        "            print(f\"✅ Perfect matches: {verification_df['match'].sum():,} ({verification_df['match'].mean()*100:.1f}%)\")\n",
        "            \n",
        "            if verification_df['match'].mean() < 1.0:\n",
        "                print(f\"⚠️  Mismatches found: {(~verification_df['match']).sum():,}\")\n",
        "                print(f\"📈 Max difference: {verification_df['difference'].abs().max():.2f}\")\n",
        "    \n",
        "    # Create summary report\n",
        "    summary_filename = f\"{base_filename}_summary.txt\"\n",
        "    with open(summary_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"SALES FORECAST PREDICTIONS SUMMARY - JANUARY 2023\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(\"PREDICTION SCOPE:\\n\")\n",
        "        f.write(f\"- Period: January 2-31, 2023 (30 days)\\n\")\n",
        "        f.write(f\"- Approach: Daily predictions aggregated to weekly\\n\")\n",
        "        f.write(f\"- Model: CatBoost with day-of-week patterns\\n\\n\")\n",
        "        \n",
        "        if hasattr(self, 'daily_predictions') and self.daily_predictions is not None:\n",
        "            df = self.daily_predictions\n",
        "            f.write(\"DAILY PREDICTIONS:\\n\")\n",
        "            f.write(f\"- Total daily records: {len(df):,}\\n\")\n",
        "            f.write(f\"- Date range: {df['date'].min()} to {df['date'].max()}\\n\")\n",
        "            f.write(f\"- Unique stores: {df['pdv'].nunique():,}\\n\")\n",
        "            f.write(f\"- Unique products: {df['produto'].nunique():,}\\n\")\n",
        "            f.write(f\"- Total predicted quantity: {df['quantidade_daily'].sum():,.0f}\\n\\n\")\n",
        "        \n",
        "        f.write(\"WEEKLY AGGREGATIONS:\\n\")\n",
        "        f.write(f\"- Total weekly records: {len(weekly_df):,}\\n\")\n",
        "        f.write(f\"- Weeks covered: {sorted(weekly_df['semana'].unique())}\\n\")\n",
        "        f.write(f\"- Total predicted quantity: {weekly_df['quantidade'].sum():,.0f}\\n\\n\")\n",
        "        \n",
        "        f.write(\"FILES GENERATED:\\n\")\n",
        "        f.write(f\"- {weekly_filename} (main submission format)\\n\")\n",
        "        f.write(f\"- {daily_filename} (detailed daily breakdown)\\n\")\n",
        "        f.write(f\"- {verification_filename} (aggregation verification)\\n\")\n",
        "        f.write(f\"- {summary_filename} (this summary)\\n\")\n",
        "    \n",
        "    print(f\"\\n📄 Summary report: {summary_filename}\")\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"✅ ALL PREDICTIONS EXPORTED SUCCESSFULLY\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "# Add method to class\n",
        "SalesForecastModelV2.export_all_predictions = export_all_predictions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
